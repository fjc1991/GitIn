{
  "project_name": "003random",
  "repository_url": "https://github.com/003random/003Recon",
  "repository_name": "003Recon",
  "ecosystem": "github",
  "repo_category": "all",
  "analysis_period": {
    "start_date": "2017-11-20",
    "end_date": "2018-06-05",
    "full_history": false
  },
  "commits": [
    {"hash": "0be7fe9e8ad6ac7f53e1b4476ae147b582eb2047", "msg": "Initial commit", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 16:53:17+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 16:53:17+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": [], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 1, "lines": 1, "files": 1, "modified_files": [{"old_path": null, "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1 @@\n+# Recon\n\\ No newline at end of file\n", "diff_parsed": {"added": [[1, "# Recon"]], "deleted": []}, "added_lines": 1, "deleted_lines": 0, "source_code": "# Recon", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "f7f69d66189aa9a73cc89eaf0061258d735ffb9e", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 16:55:47+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 16:55:47+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["0be7fe9e8ad6ac7f53e1b4476ae147b582eb2047"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 5, "lines": 6, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1 +1,5 @@\n-# Recon\n\\ No newline at end of file\n+## recon\n+\n+This repository contains all my scripts that i made to automate alot of my recon and scanning.\n+These files are free to take, edit and share.\n+\n", "diff_parsed": {"added": [[1, "## recon"], [2, ""], [3, "This repository contains all my scripts that i made to automate alot of my recon and scanning."], [4, "These files are free to take, edit and share."], [5, ""]], "deleted": [[1, "# Recon"]]}, "added_lines": 5, "deleted_lines": 1, "source_code": "## recon\n\nThis repository contains all my scripts that i made to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n", "source_code_before": "# Recon", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "ecca419a484877fc45454372e5262d1b2d68d68a", "msg": "Added main files", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 16:58:17+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 16:58:17+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["f7f69d66189aa9a73cc89eaf0061258d735ffb9e"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 578, "lines": 578, "files": 14, "modified_files": [{"old_path": null, "new_path": "cors_misconfiguration_scan.sh", "filename": "cors_misconfiguration_scan.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,25 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n+\n+if [ ! -f $1 ]; then\n+    echo \"[-]File not found!\"\n+else\n+    while read domain; do\n+        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n+            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n+                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n+                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n+\n+                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n+                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n+                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n+                fi\n+            else\n+                echo \"[-]$domain [$origin]\"\n+            fi\n+        done\n+    done < $1 \n+fi\n+\n+printf \"\\n -- Done -- \\n\"\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    echo \"[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do"], [10, "            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then"], [11, "                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\""], [12, "                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2"], [13, ""], [14, "                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then"], [15, "                    echo \"[+](Allow-Credentials) $domain [$origin]\""], [16, "                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2"], [17, "                fi"], [18, "            else"], [19, "                echo \"[-]$domain [$origin]\""], [20, "            fi"], [21, "        done"], [22, "    done < $1"], [23, "fi"], [24, ""], [25, "printf \"\\n -- Done -- \\n\""]], "deleted": []}, "added_lines": 25, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do\n        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n\n                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n                fi\n            else\n                echo \"[-]$domain [$origin]\"\n            fi\n        done\n    done < $1 \nfi\n\nprintf \"\\n -- Done -- \\n\"\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "crlf.sh", "filename": "crlf.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,21 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\"\n+\n+green=\"tput setaf 2\"\n+reset=\"tput sgr0\"\n+\n+#First loop trough the payloads to prevent 429 (rate limit)\n+while read payload; do \n+    while read domain; do \n+        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then \n+            echo \"${green}[+]${reset}$domain/$payload\"\n+            echo \"$domain/$payload\" >> $2\n+        else\n+            echo \"[-]$domain/$payload\"\n+        fi\n+    done < $1\n+done < ~/Documents/Wordlists/crlf.txt \n+\n+printf \"\\n-- Done --\"\n+\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\""], [4, ""], [5, "green=\"tput setaf 2\""], [6, "reset=\"tput sgr0\""], [7, ""], [8, "#First loop trough the payloads to prevent 429 (rate limit)"], [9, "while read payload; do"], [10, "    while read domain; do"], [11, "        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then"], [12, "            echo \"${green}[+]${reset}$domain/$payload\""], [13, "            echo \"$domain/$payload\" >> $2"], [14, "        else"], [15, "            echo \"[-]$domain/$payload\""], [16, "        fi"], [17, "    done < $1"], [18, "done < ~/Documents/Wordlists/crlf.txt"], [19, ""], [20, "printf \"\\n-- Done --\""], [21, ""]], "deleted": []}, "added_lines": 21, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\"\n\ngreen=\"tput setaf 2\"\nreset=\"tput sgr0\"\n\n#First loop trough the payloads to prevent 429 (rate limit)\nwhile read payload; do \n    while read domain; do \n        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then \n            echo \"${green}[+]${reset}$domain/$payload\"\n            echo \"$domain/$payload\" >> $2\n        else\n            echo \"[-]$domain/$payload\"\n        fi\n    done < $1\ndone < ~/Documents/Wordlists/crlf.txt \n\nprintf \"\\n-- Done --\"\n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "error_page_info_check.py", "filename": "error_page_info_check.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,50 @@\n+#!/usr/bin/python\n+\n+import requests, sys\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+is_closed = True\n+\n+domains = open(input_file,'r').read().split('\\n')\n+info = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/error_pages_info.txt')]\n+\n+print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n+\n+payloads = [\"/\",\n+            \"/NotFound123\",\n+            \"/.htaccess\",\n+            \"/<>\"]\n+\n+for payload in payloads:\n+    print \"\\n - Trying payload \"+payload+\" - \"\n+    for domain in domains:\n+        info_found = \"\"\n+        if domain != \"\":\n+            found = False\n+            try:\n+                response = requests.get(\"http://\"+domain+payload)\n+            except:\n+                print(\"[-]Error on http://\"+domain+payload)\n+            for i in info:\n+                if i.lower() in response.content.lower():\n+                    found = True\n+                    #Search and get the line in the response that contains i.lower()\n+                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n+\n+            if found:\n+                if is_closed:\n+                    file = open(output_file,\"w+\")\n+                is_closed = False\n+                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n+                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n+            else:\n+               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n+                \n+        else:\n+            print(\"[-]Domain is invalid\")\n+        \n+if is_closed == False:\n+    file.close()\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, "info = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/error_pages_info.txt')]"], [11, ""], [12, "print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")"], [13, ""], [14, "payloads = [\"/\","], [15, "            \"/NotFound123\","], [16, "            \"/.htaccess\","], [17, "            \"/<>\"]"], [18, ""], [19, "for payload in payloads:"], [20, "    print \"\\n - Trying payload \"+payload+\" - \""], [21, "    for domain in domains:"], [22, "        info_found = \"\""], [23, "        if domain != \"\":"], [24, "            found = False"], [25, "            try:"], [26, "                response = requests.get(\"http://\"+domain+payload)"], [27, "            except:"], [28, "                print(\"[-]Error on http://\"+domain+payload)"], [29, "            for i in info:"], [30, "                if i.lower() in response.content.lower():"], [31, "                    found = True"], [32, "                    #Search and get the line in the response that contains i.lower()"], [33, "                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]"], [34, ""], [35, "            if found:"], [36, "                if is_closed:"], [37, "                    file = open(output_file,\"w+\")"], [38, "                is_closed = False"], [39, "                print(\"[+]\"+domain+payload+\" - \"+str(info_found))"], [40, "                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")"], [41, "            else:"], [42, "               print(\"[-]\"+domain+payload+\" - \"+str(info_found))"], [43, ""], [44, "        else:"], [45, "            print(\"[-]Domain is invalid\")"], [46, ""], [47, "if is_closed == False:"], [48, "    file.close()"], [49, ""], [50, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 50, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\ninfo = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/error_pages_info.txt')]\n\nprint(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n\npayloads = [\"/\",\n            \"/NotFound123\",\n            \"/.htaccess\",\n            \"/<>\"]\n\nfor payload in payloads:\n    print \"\\n - Trying payload \"+payload+\" - \"\n    for domain in domains:\n        info_found = \"\"\n        if domain != \"\":\n            found = False\n            try:\n                response = requests.get(\"http://\"+domain+payload)\n            except:\n                print(\"[-]Error on http://\"+domain+payload)\n            for i in info:\n                if i.lower() in response.content.lower():\n                    found = True\n                    #Search and get the line in the response that contains i.lower()\n                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n\n            if found:\n                if is_closed:\n                    file = open(output_file,\"w+\")\n                is_closed = False\n                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n            else:\n               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n                \n        else:\n            print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 38, "complexity": 0, "token_count": 278}, {"old_path": null, "new_path": "header_scan.py", "filename": "header_scan.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,38 @@\n+import requests, sys\r\n+\r\n+input_file = sys.argv[1]\r\n+output_file = sys.argv[2]\r\n+\r\n+print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n+\r\n+is_closed = True\r\n+domains = open(input_file,'r').read().split('\\n')\r\n+headers = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/headers.txt')]\r\n+\r\n+for domain in domains:\r\n+\tif domain != \"\":\r\n+\t\ttry:\r\n+\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n+\t\texcept:\r\n+\t\t\tprint(\"[-]Error on https://\"+domain)\r\n+\t\theaders_found = []\r\n+\r\n+\t\tfor header in headers:\r\n+\t\t\tcurrent_header = r.headers.get(header.lower())\r\n+\t\t\tif current_header != None:\r\n+\t\t\t\theaders_found.append(str(current_header))\r\n+\t\tif headers_found != []:\r\n+\t\t\tif is_closed:\r\n+        \t\t\tfile = open(output_file,\"w+\")\r\n+\t\t\t\tis_closed = False\r\n+\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n+\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n+\t\telse:\r\n+\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n+\telse:\r\n+            print \"[-]Domain is invalid\"\r\n+\r\n+if is_closed == False:\r\n+\tfile.close()\r\n+\r\n+print(\"\\n-- Done --\")\r\n", "diff_parsed": {"added": [[1, "import requests, sys"], [2, ""], [3, "input_file = sys.argv[1]"], [4, "output_file = sys.argv[2]"], [5, ""], [6, "print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [7, ""], [8, "is_closed = True"], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, "headers = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/headers.txt')]"], [11, ""], [12, "for domain in domains:"], [13, "\tif domain != \"\":"], [14, "\t\ttry:"], [15, "\t\t\tr = requests.head(\"https://\"+domain, timeout=5)"], [16, "\t\texcept:"], [17, "\t\t\tprint(\"[-]Error on https://\"+domain)"], [18, "\t\theaders_found = []"], [19, ""], [20, "\t\tfor header in headers:"], [21, "\t\t\tcurrent_header = r.headers.get(header.lower())"], [22, "\t\t\tif current_header != None:"], [23, "\t\t\t\theaders_found.append(str(current_header))"], [24, "\t\tif headers_found != []:"], [25, "\t\t\tif is_closed:"], [26, "        \t\t\tfile = open(output_file,\"w+\")"], [27, "\t\t\t\tis_closed = False"], [28, "\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))"], [29, "\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")"], [30, "\t\telse:"], [31, "\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))"], [32, "\telse:"], [33, "            print \"[-]Domain is invalid\""], [34, ""], [35, "if is_closed == False:"], [36, "\tfile.close()"], [37, ""], [38, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 38, "deleted_lines": 0, "source_code": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/headers.txt')]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None:\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 31, "complexity": 0, "token_count": 225}, {"old_path": null, "new_path": "javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,72 @@\n+#!/usr/bin/python\n+\n+import re, requests, sys\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+\n+print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n+\n+domains = open(input_file,'r').read().split('\\n')\n+\n+file = open(output_file,\"w+\")\n+\n+black_listed_domains = [\"ajax.googleapis.com\",\n+                        \"cdn.optimizely.com\",\n+                        \"googletagmanager.com\",\n+                        \"fontawesome.com\"]\n+\n+for domain in domains:\n+\tdomain_written = False\n+\ti = 0\n+\tb_amount = 0\n+\tfull_domain = \"\"\n+\tif domain != \"\":\n+\t\tmatches = \"\"\n+\t\tr = \"\"\n+\t\tregex = r'script src=\"(.*?)\"'\n+\t\ttry:\n+\t\t\tr = requests.get(\"http://\"+domain).content\n+\t\texcept:\n+\t\t\tprint \"[-]Error in http://\"+domain\n+\n+\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\t\tif matches == []:\n+\t\t\tregex = r\"script src='(.*?)'\"\n+\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\n+\t\tfor m in matches:\n+\t\t\tif domain_written != True:\n+\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n+\t\t\t\tdomain_written = True\n+\t\n+\t\t\tblack_listed = False\n+\t\t\tfor b in black_listed_domains:\n+\t\t\t\tif b in m:\n+\t\t\t\t\tblack_listed = True\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\tif m.startswith(\"/\"):\n+\t\t\t\t\tif m.startswith(\"//\"):\n+\t\t\t\t\t\tfull_domain = \"https:\"+m\n+\t\t\t\t\telse:\n+\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n+\t\t\t\telif m.startswith(\"http\"):\n+\t\t\t\t\tfull_domain = m\n+\t\t\t\telse:\n+\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n+\t\t\telse:\n+\t\t\t\tb_amount += 1\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\ti += 1\n+\t\t\t\tfile.write(full_domain+\"\\n\")\n+\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n+\telse:\n+\t\tprint \"[-]Domain is invalid \" + domain\n+\n+\n+\n+file.close()\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import re, requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, ""], [8, "print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [9, ""], [10, "domains = open(input_file,'r').read().split('\\n')"], [11, ""], [12, "file = open(output_file,\"w+\")"], [13, ""], [14, "black_listed_domains = [\"ajax.googleapis.com\","], [15, "                        \"cdn.optimizely.com\","], [16, "                        \"googletagmanager.com\","], [17, "                        \"fontawesome.com\"]"], [18, ""], [19, "for domain in domains:"], [20, "\tdomain_written = False"], [21, "\ti = 0"], [22, "\tb_amount = 0"], [23, "\tfull_domain = \"\""], [24, "\tif domain != \"\":"], [25, "\t\tmatches = \"\""], [26, "\t\tr = \"\""], [27, "\t\tregex = r'script src=\"(.*?)\"'"], [28, "\t\ttry:"], [29, "\t\t\tr = requests.get(\"http://\"+domain).content"], [30, "\t\texcept:"], [31, "\t\t\tprint \"[-]Error in http://\"+domain"], [32, ""], [33, "\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [34, "\t\tif matches == []:"], [35, "\t\t\tregex = r\"script src='(.*?)'\""], [36, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [37, ""], [38, "\t\tfor m in matches:"], [39, "\t\t\tif domain_written != True:"], [40, "\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")"], [41, "\t\t\t\tdomain_written = True"], [42, ""], [43, "\t\t\tblack_listed = False"], [44, "\t\t\tfor b in black_listed_domains:"], [45, "\t\t\t\tif b in m:"], [46, "\t\t\t\t\tblack_listed = True"], [47, ""], [48, "\t\t\tif black_listed != True:"], [49, "\t\t\t\tif m.startswith(\"/\"):"], [50, "\t\t\t\t\tif m.startswith(\"//\"):"], [51, "\t\t\t\t\t\tfull_domain = \"https:\"+m"], [52, "\t\t\t\t\telse:"], [53, "\t\t\t\t\t\tfull_domain = \"https://\"+domain+m"], [54, "\t\t\t\telif m.startswith(\"http\"):"], [55, "\t\t\t\t\tfull_domain = m"], [56, "\t\t\t\telse:"], [57, "\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m"], [58, "\t\t\telse:"], [59, "\t\t\t\tb_amount += 1"], [60, ""], [61, "\t\t\tif black_listed != True:"], [62, "\t\t\t\ti += 1"], [63, "\t\t\t\tfile.write(full_domain+\"\\n\")"], [64, "\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain"], [65, "\telse:"], [66, "\t\tprint \"[-]Domain is invalid \" + domain"], [67, ""], [68, ""], [69, ""], [70, "file.close()"], [71, ""], [72, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 72, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains = open(input_file,'r').read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 55, "complexity": 0, "token_count": 310}, {"old_path": null, "new_path": "javascript_files_link_extractor.sh", "filename": "javascript_files_link_extractor.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,23 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n+\n+if [ ! -f $1 ]; then\n+    printf  \"\\n[-]File not found!\"\n+else\n+    while read domain; do \n+        if [[ $domain == \"-\"* ]]; then\n+            printf  \"\\n\\n\\n[+]-$domain--\"\n+        else\n+            if [ -z \"$domain\" ]; then\n+                printf \"\\n[-]Invalid domain $domain\"\n+            else\n+                printf \"\\n[+]$domain \\n\"\n+\t\techo \"----------------------\"\n+                command=\"ruby ~/Documents/Tools/relative-url-extractor/extract.rb $domain\"\n+                eval $command\n+            fi\n+        fi\n+    done < $1 >> $2\n+printf \"\\n -- Done -- \\n\"\n+fi\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    printf  \"\\n[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        if [[ $domain == \"-\"* ]]; then"], [10, "            printf  \"\\n\\n\\n[+]-$domain--\""], [11, "        else"], [12, "            if [ -z \"$domain\" ]; then"], [13, "                printf \"\\n[-]Invalid domain $domain\""], [14, "            else"], [15, "                printf \"\\n[+]$domain \\n\""], [16, "\t\techo \"----------------------\""], [17, "                command=\"ruby ~/Documents/Tools/relative-url-extractor/extract.rb $domain\""], [18, "                eval $command"], [19, "            fi"], [20, "        fi"], [21, "    done < $1 >> $2"], [22, "printf \"\\n -- Done -- \\n\""], [23, "fi"]], "deleted": []}, "added_lines": 23, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n\nif [ ! -f $1 ]; then\n    printf  \"\\n[-]File not found!\"\nelse\n    while read domain; do \n        if [[ $domain == \"-\"* ]]; then\n            printf  \"\\n\\n\\n[+]-$domain--\"\n        else\n            if [ -z \"$domain\" ]; then\n                printf \"\\n[-]Invalid domain $domain\"\n            else\n                printf \"\\n[+]$domain \\n\"\n\t\techo \"----------------------\"\n                command=\"ruby ~/Documents/Tools/relative-url-extractor/extract.rb $domain\"\n                eval $command\n            fi\n        fi\n    done < $1 >> $2\nprintf \"\\n -- Done -- \\n\"\nfi\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "nmap_scan.sh", "filename": "nmap_scan.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,13 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\n+echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n+\n+while read domain; do \n+    echo \"-- $domain --\"\n+    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\n+done < $1  >> $2\n+\n+echo \"-- Done --\"\n+\n+\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\""], [4, "echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\""], [5, ""], [6, "while read domain; do"], [7, "    echo \"-- $domain --\""], [8, "    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'"], [9, "done < $1  >> $2"], [10, ""], [11, "echo \"-- Done --\""], [12, ""], [13, ""]], "deleted": []}, "added_lines": 13, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\necho \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n\nwhile read domain; do \n    echo \"-- $domain --\"\n    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\ndone < $1  >> $2\n\necho \"-- Done --\"\n\n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "online.py", "filename": "online.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,45 @@\n+#!/usr/bin/python\n+\n+import httplib\n+import socket\n+import re\n+import sys\n+\n+def online(host):\n+    try:\n+        socket.gethostbyname(host)\n+    except socket.gaierror:\n+        return False\n+    else:\n+        return True\n+\n+def available(host, path=\"/\"):\n+    try:\n+        conn = httplib.HTTPConnection(host, timeout=5)\n+        conn.request(\"HEAD\", path)\n+        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n+            return True\n+    except StandardError:\n+        return False\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+\n+input_file_open = open(input_file, 'r')\n+output_file_open = open(output_file, 'w+')\n+\n+domains = input_file_open.readlines()\n+\n+print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n+\n+for domain in domains:\n+    domain = domain.strip()\n+    if online(domain) == True and available(domain) == True:\n+        print(\"[+]\"+domain.strip())\n+        output_file_open.write(domain+\"\\n\")\n+    else:\n+        print(\"[-]\"+domain)\n+\n+input_file_open.close()\n+output_file_open.close()\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import httplib"], [4, "import socket"], [5, "import re"], [6, "import sys"], [7, ""], [8, "def online(host):"], [9, "    try:"], [10, "        socket.gethostbyname(host)"], [11, "    except socket.gaierror:"], [12, "        return False"], [13, "    else:"], [14, "        return True"], [15, ""], [16, "def available(host, path=\"/\"):"], [17, "    try:"], [18, "        conn = httplib.HTTPConnection(host, timeout=5)"], [19, "        conn.request(\"HEAD\", path)"], [20, "        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):"], [21, "            return True"], [22, "    except StandardError:"], [23, "        return False"], [24, ""], [25, "input_file = sys.argv[1]"], [26, "output_file = sys.argv[2]"], [27, ""], [28, "input_file_open = open(input_file, 'r')"], [29, "output_file_open = open(output_file, 'w+')"], [30, ""], [31, "domains = input_file_open.readlines()"], [32, ""], [33, "print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")"], [34, ""], [35, "for domain in domains:"], [36, "    domain = domain.strip()"], [37, "    if online(domain) == True and available(domain) == True:"], [38, "        print(\"[+]\"+domain.strip())"], [39, "        output_file_open.write(domain+\"\\n\")"], [40, "    else:"], [41, "        print(\"[-]\"+domain)"], [42, ""], [43, "input_file_open.close()"], [44, "output_file_open.close()"], [45, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 45, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport httplib\nimport socket\nimport re\nimport sys\n\ndef online(host):\n    try:\n        socket.gethostbyname(host)\n    except socket.gaierror:\n        return False\n    else:\n        return True\n\ndef available(host, path=\"/\"):\n    try:\n        conn = httplib.HTTPConnection(host, timeout=5)\n        conn.request(\"HEAD\", path)\n        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n            return True\n    except StandardError:\n        return False\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\ninput_file_open = open(input_file, 'r')\noutput_file_open = open(output_file, 'w+')\n\ndomains = input_file_open.readlines()\n\nprint(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    domain = domain.strip()\n    if online(domain) == True and available(domain) == True:\n        print(\"[+]\"+domain.strip())\n        output_file_open.write(domain+\"\\n\")\n    else:\n        print(\"[-]\"+domain)\n\ninput_file_open.close()\noutput_file_open.close()\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "methods_before": [], "changed_methods": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "nloc": 35, "complexity": 5, "token_count": 209}, {"old_path": null, "new_path": "open_redirect.py", "filename": "open_redirect.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,48 @@\n+#!/usr/bin/python\r\n+import requests,sys\r\n+\r\n+def start():\r\n+    input_file = sys.argv[1]\r\n+    output_file = sys.argv[2]\r\n+\r\n+    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n+\r\n+    is_closed = True\r\n+\r\n+    payloads = open(\"/home/rjp/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n+\r\n+    #First loop trough the payloads to prevent 429 (rate limit)\r\n+    for payload in payloads: \r\n+        domains = open(input_file,'r').read().split('\\n')   \r\n+        print \"\\n - Trying payload \"+payload+\" - \"\r\n+        for domain in domains:\r\n+            if domain != \"\":\r\n+\r\n+                url = \"https://\" + domain + payload\r\n+                url = url.strip()\r\n+            \r\n+                try:\r\n+                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n+                except:\r\n+                    print \"[-]Error on \" + url\r\n+\r\n+                if r.history:  \r\n+                    if r.url == \"https://example.com\":\r\n+                        print \"[+]\"+url\r\n+                        if is_closed:\r\n+                            file = open(output_file,\"w+\")\r\n+                        is_closed = False\r\n+                        file.write(url + \"\\n\")\r\n+                    else:\r\n+                        print \"[-]\"+url\r\n+                else:\r\n+                    print \"[-]\"+url\r\n+        else:\r\n+            print \"[-]Domain is invalid\"\r\n+\r\n+    if is_closed == False:\r\n+        file.close()\r\n+    print(\"\\n-- Done --\")\r\n+\r\n+start()\r\n+\r\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, "import requests,sys"], [3, ""], [4, "def start():"], [5, "    input_file = sys.argv[1]"], [6, "    output_file = sys.argv[2]"], [7, ""], [8, "    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")"], [9, ""], [10, "    is_closed = True"], [11, ""], [12, "    payloads = open(\"/home/rjp/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')"], [13, ""], [14, "    #First loop trough the payloads to prevent 429 (rate limit)"], [15, "    for payload in payloads:"], [16, "        domains = open(input_file,'r').read().split('\\n')"], [17, "        print \"\\n - Trying payload \"+payload+\" - \""], [18, "        for domain in domains:"], [19, "            if domain != \"\":"], [20, ""], [21, "                url = \"https://\" + domain + payload"], [22, "                url = url.strip()"], [23, ""], [24, "                try:"], [25, "                    r = requests.head(url, allow_redirects=True, timeout=5)"], [26, "                except:"], [27, "                    print \"[-]Error on \" + url"], [28, ""], [29, "                if r.history:"], [30, "                    if r.url == \"https://example.com\":"], [31, "                        print \"[+]\"+url"], [32, "                        if is_closed:"], [33, "                            file = open(output_file,\"w+\")"], [34, "                        is_closed = False"], [35, "                        file.write(url + \"\\n\")"], [36, "                    else:"], [37, "                        print \"[-]\"+url"], [38, "                else:"], [39, "                    print \"[-]\"+url"], [40, "        else:"], [41, "            print \"[-]Domain is invalid\""], [42, ""], [43, "    if is_closed == False:"], [44, "        file.close()"], [45, "    print(\"\\n-- Done --\")"], [46, ""], [47, "start()"], [48, ""]], "deleted": []}, "added_lines": 48, "deleted_lines": 0, "source_code": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(\"/home/rjp/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n\r\n                if r.history:  \r\n                    if r.url == \"https://example.com\":\r\n                        print \"[+]\"+url\r\n                        if is_closed:\r\n                            file = open(output_file,\"w+\")\r\n                        is_closed = False\r\n                        file.write(url + \"\\n\")\r\n                    else:\r\n                        print \"[-]\"+url\r\n                else:\r\n                    print \"[-]\"+url\r\n        else:\r\n            print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "source_code_before": null, "methods": [{"name": "start", "start_line": 4, "end_line": 45}], "methods_before": [], "changed_methods": [{"name": "start", "start_line": 4, "end_line": 45}], "nloc": 35, "complexity": 9, "token_count": 204}, {"old_path": null, "new_path": "start_recon.sh", "filename": "start_recon.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,34 @@\n+#!/bin/bash\n+    \n+all_domains_file=\"domains-all\"\n+domains_file=\"domains\"\n+crlf_file=\"crlf\"\n+open_redirects_file=\"redirects\"\n+nmap_scan_file=\"nmap_scans\"\n+screenshots_folder=\"screenshots\"\n+wordpress_file=\"wordpress_sites\"\n+headers_file=\"sensitive_headers\"\n+subdomain_take_over_file=\"sub_take_over\"\n+javascript_files_file=\"javascript_files\"\n+javascript_extracted_urls=\"extracted_urls\"\n+error_page_info_file=\"error_page_info\"\n+cors_file=\"misconfigured_cors\"\n+\n+rm -rf ~/Desktop/$@; \n+mkdir ~/Desktop/$@; \n+cd ~/Desktop/$@; \n+python ~/Documents/Tools/Sublist3r/sublist3r.py -o $all_domains_file -d $@;\n+python ~/Documents/Tools/online.py $all_domains_file $domains_file;\n+~/Documents/Tools/crlf.sh $domains_file $crlf_file;\n+~/Documents/Tools/cors_misconfiguration_scan.sh $domains_file $cors_file;\n+python ~/Documents/Tools/open_redirect.py $domains_file $open_redirects_file;\n+python ~/Documents/Tools/header_scan.py $domains_file $headers_file\n+python ~/Documents/Tools/error_page_info_check.py $domains_file $error_page_info_file;\n+python ~/Documents/Tools/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n+python ~/Documents/Tools/javascript_files_extractor.py $domains_file $javascript_files_file\n+~/Documents/Tools/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls\n+python ~/Documents/Tools/webscreenshot/webscreenshot.py -i $domains_file -o $screenshots_folder\n+python ~/Documents/Tools/wordpress_check.py $domains_file $wordpress_file\n+~/Documents/Tools/wpscan/wpscan.rb --update\n+~/Documents/Tools/wpscan_domains.sh $wordpress_file\n+~/Documents/Tools/nmap_scan.sh $domains_file $nmap_scan_file;\n\\ No newline at end of file\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "all_domains_file=\"domains-all\""], [4, "domains_file=\"domains\""], [5, "crlf_file=\"crlf\""], [6, "open_redirects_file=\"redirects\""], [7, "nmap_scan_file=\"nmap_scans\""], [8, "screenshots_folder=\"screenshots\""], [9, "wordpress_file=\"wordpress_sites\""], [10, "headers_file=\"sensitive_headers\""], [11, "subdomain_take_over_file=\"sub_take_over\""], [12, "javascript_files_file=\"javascript_files\""], [13, "javascript_extracted_urls=\"extracted_urls\""], [14, "error_page_info_file=\"error_page_info\""], [15, "cors_file=\"misconfigured_cors\""], [16, ""], [17, "rm -rf ~/Desktop/$@;"], [18, "mkdir ~/Desktop/$@;"], [19, "cd ~/Desktop/$@;"], [20, "python ~/Documents/Tools/Sublist3r/sublist3r.py -o $all_domains_file -d $@;"], [21, "python ~/Documents/Tools/online.py $all_domains_file $domains_file;"], [22, "~/Documents/Tools/crlf.sh $domains_file $crlf_file;"], [23, "~/Documents/Tools/cors_misconfiguration_scan.sh $domains_file $cors_file;"], [24, "python ~/Documents/Tools/open_redirect.py $domains_file $open_redirects_file;"], [25, "python ~/Documents/Tools/header_scan.py $domains_file $headers_file"], [26, "python ~/Documents/Tools/error_page_info_check.py $domains_file $error_page_info_file;"], [27, "python ~/Documents/Tools/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;"], [28, "python ~/Documents/Tools/javascript_files_extractor.py $domains_file $javascript_files_file"], [29, "~/Documents/Tools/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls"], [30, "python ~/Documents/Tools/webscreenshot/webscreenshot.py -i $domains_file -o $screenshots_folder"], [31, "python ~/Documents/Tools/wordpress_check.py $domains_file $wordpress_file"], [32, "~/Documents/Tools/wpscan/wpscan.rb --update"], [33, "~/Documents/Tools/wpscan_domains.sh $wordpress_file"], [34, "~/Documents/Tools/nmap_scan.sh $domains_file $nmap_scan_file;"]], "deleted": []}, "added_lines": 34, "deleted_lines": 0, "source_code": "#!/bin/bash\n    \nall_domains_file=\"domains-all\"\ndomains_file=\"domains\"\ncrlf_file=\"crlf\"\nopen_redirects_file=\"redirects\"\nnmap_scan_file=\"nmap_scans\"\nscreenshots_folder=\"screenshots\"\nwordpress_file=\"wordpress_sites\"\nheaders_file=\"sensitive_headers\"\nsubdomain_take_over_file=\"sub_take_over\"\njavascript_files_file=\"javascript_files\"\njavascript_extracted_urls=\"extracted_urls\"\nerror_page_info_file=\"error_page_info\"\ncors_file=\"misconfigured_cors\"\n\nrm -rf ~/Desktop/$@; \nmkdir ~/Desktop/$@; \ncd ~/Desktop/$@; \npython ~/Documents/Tools/Sublist3r/sublist3r.py -o $all_domains_file -d $@;\npython ~/Documents/Tools/online.py $all_domains_file $domains_file;\n~/Documents/Tools/crlf.sh $domains_file $crlf_file;\n~/Documents/Tools/cors_misconfiguration_scan.sh $domains_file $cors_file;\npython ~/Documents/Tools/open_redirect.py $domains_file $open_redirects_file;\npython ~/Documents/Tools/header_scan.py $domains_file $headers_file\npython ~/Documents/Tools/error_page_info_check.py $domains_file $error_page_info_file;\npython ~/Documents/Tools/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\npython ~/Documents/Tools/javascript_files_extractor.py $domains_file $javascript_files_file\n~/Documents/Tools/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls\npython ~/Documents/Tools/webscreenshot/webscreenshot.py -i $domains_file -o $screenshots_folder\npython ~/Documents/Tools/wordpress_check.py $domains_file $wordpress_file\n~/Documents/Tools/wpscan/wpscan.rb --update\n~/Documents/Tools/wpscan_domains.sh $wordpress_file\n~/Documents/Tools/nmap_scan.sh $domains_file $nmap_scan_file;", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "start_recons.py", "filename": "start_recons.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,45 @@\n+#!/usr/bin/python\n+\n+import sys, os, time, threading\n+\n+domains = [\"lyst.com\",\n+\"bimeanalytics.com\",\n+\"toytalk.com\",\n+\"goodhire.com\",\n+\"identity.com\",\n+\"glasswire.com\",\n+\"keybase.io\",\n+\"quora.com\",\n+\"nextcloud.com\",\n+\"boards.greenhouse.io\",\n+\"trello.com\",\n+\"trello.services\",\n+\"badoo.com\",\n+\"pinion.gg\",\n+\"unikrn.com\",\n+\"spotify.com\",\n+\"mapbox.com\",\n+\"semrush.com\",\n+\"ok.ru\",\n+\"booztx.com\",\n+\"irccloud.com\",\n+\"irccloud-cdn.com\",\n+\"udemy.com\",\n+\"legalrobot.com\",\n+\"harvestapp.com\",\n+\"forecastapp.com\",\n+\"spectacles.com\",\n+\"bitstrips.com\",\n+\"bitmoji.com\",\n+\"scan.me\"]\n+\n+def run(d):\n+\tos.system(\"/home/rjp/Documents/Tools/start_recon.sh \"+d)\n+\n+for domain in domains:\n+\tif domain is not \"\":\n+\t\tthr = threading.Thread(target=run, args=(domain,))\n+\t\tthr.start()\n+\t\ttime.sleep(500)\n+\n+\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import sys, os, time, threading"], [4, ""], [5, "domains = [\"lyst.com\","], [6, "\"bimeanalytics.com\","], [7, "\"toytalk.com\","], [8, "\"goodhire.com\","], [9, "\"identity.com\","], [10, "\"glasswire.com\","], [11, "\"keybase.io\","], [12, "\"quora.com\","], [13, "\"nextcloud.com\","], [14, "\"boards.greenhouse.io\","], [15, "\"trello.com\","], [16, "\"trello.services\","], [17, "\"badoo.com\","], [18, "\"pinion.gg\","], [19, "\"unikrn.com\","], [20, "\"spotify.com\","], [21, "\"mapbox.com\","], [22, "\"semrush.com\","], [23, "\"ok.ru\","], [24, "\"booztx.com\","], [25, "\"irccloud.com\","], [26, "\"irccloud-cdn.com\","], [27, "\"udemy.com\","], [28, "\"legalrobot.com\","], [29, "\"harvestapp.com\","], [30, "\"forecastapp.com\","], [31, "\"spectacles.com\","], [32, "\"bitstrips.com\","], [33, "\"bitmoji.com\","], [34, "\"scan.me\"]"], [35, ""], [36, "def run(d):"], [37, "\tos.system(\"/home/rjp/Documents/Tools/start_recon.sh \"+d)"], [38, ""], [39, "for domain in domains:"], [40, "\tif domain is not \"\":"], [41, "\t\tthr = threading.Thread(target=run, args=(domain,))"], [42, "\t\tthr.start()"], [43, "\t\ttime.sleep(500)"], [44, ""], [45, ""]], "deleted": []}, "added_lines": 45, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport sys, os, time, threading\n\ndomains = [\"lyst.com\",\n\"bimeanalytics.com\",\n\"toytalk.com\",\n\"goodhire.com\",\n\"identity.com\",\n\"glasswire.com\",\n\"keybase.io\",\n\"quora.com\",\n\"nextcloud.com\",\n\"boards.greenhouse.io\",\n\"trello.com\",\n\"trello.services\",\n\"badoo.com\",\n\"pinion.gg\",\n\"unikrn.com\",\n\"spotify.com\",\n\"mapbox.com\",\n\"semrush.com\",\n\"ok.ru\",\n\"booztx.com\",\n\"irccloud.com\",\n\"irccloud-cdn.com\",\n\"udemy.com\",\n\"legalrobot.com\",\n\"harvestapp.com\",\n\"forecastapp.com\",\n\"spectacles.com\",\n\"bitstrips.com\",\n\"bitmoji.com\",\n\"scan.me\"]\n\ndef run(d):\n\tos.system(\"/home/rjp/Documents/Tools/start_recon.sh \"+d)\n\nfor domain in domains:\n\tif domain is not \"\":\n\t\tthr = threading.Thread(target=run, args=(domain,))\n\t\tthr.start()\n\t\ttime.sleep(500)\n\n\n", "source_code_before": null, "methods": [{"name": "run", "start_line": 36, "end_line": 37}], "methods_before": [], "changed_methods": [{"name": "run", "start_line": 36, "end_line": 37}], "nloc": 38, "complexity": 1, "token_count": 124}, {"old_path": null, "new_path": "subdomain_takeover_scan.py", "filename": "subdomain_takeover_scan.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,117 @@\n+#!/usr/bin/python\n+\n+import requests, sys, dns.resolver\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+is_closed = True\n+\n+domains = open(input_file,'r').read().split('\\n')\n+\n+take_over_cnames = [\"createsend\",\n+\"cargocollective\",\n+\"cloudfront\",\n+\"desk.com\",\n+\"fastly.net\",\n+\"feedpress.me\",\n+\"freshdesk.com\",\n+\"github.io\",\n+\"helpjuice.com\",\n+\"helpscoutdocs.com\",\n+\"herokudns.com\",\n+\"herokussl.com\",\n+\"herokuapp.com\",\n+\"pageserve.co\",\n+\"pingdom.com\",\n+\"amazonaws.com\",\n+\"myshopify.com\",\n+\"stspg-customer.com\",\n+\"sgizmo.com\",\n+\"surveygizmo.eu\",\n+\"sgizmoca.com\",\n+\"sgizmoca.com\",\n+\"tictail.com\",\n+\"domains.tumblr.com\",\n+\"uservoice.com\",\n+\"wpengine.com\",\n+\"squarespace.com\",\n+\"unbounce.com\",\n+\"zendesk.com\"]\n+\n+take_over_content = [\"<strong>Trying to access your account\",\n+\"Use a personal domain name\",\n+\"The request could not be satisfied\",\n+\"Sorry, We Couldn't Find That Page\",\n+\"Fastly error: unknown domain\",\n+\"The feed has not been found\",\n+\"You can claim it now at\",\n+\"Publishing platform\",                        \n+\"There isn't a GitHub Pages site here\",                       \n+\"No settings were found for this company\",\n+\"<title>No such app</title>\",                        \n+\"You've Discovered A Missing Link. Our Apologies!\",\n+\"Sorry, couldn&rsquo;t find the status page\",                        \n+\"NoSuchBucket\",\n+\"Sorry, this shop is currently unavailable\",\n+\"<title>Hosted Status Pages for Your Company</title>\",\n+\"data-html-name=\\\"Header Logo Link\\\"\",                        \n+\"<title>Oops - We didn't find your site.</title>\",\n+\"class=\\\"MarketplaceHeader__tictailLogo\\\"\",                        \n+\"Whatever you were looking for doesn't currently exist at this address\",\n+\"The requested URL was not found on this server\",\n+\"The page you have requested does not exist\",\n+\"This UserVoice subdomain is currently available!\",\n+\"but is not configured for an account on our platform\",\n+\"<title>Help Center Closed | Zendesk</title>\"]\n+\n+print(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n+\n+\n+for domain in domains:\n+\t#Skip first row, lol\n+\tif domain != domains[0]:\n+\t\tfound_content = False\n+\t\tfound_cname = False\n+\t\ttry:\n+\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text\n+\t\texcept:\n+\t\t\tprint(\"[-]Error in http://\"+domain)\n+\n+\t\tfor content in take_over_content:\n+\t\t\tif str(content) in r:\n+\t\t\t\tfound_content = True\n+\n+\t\ttry:\n+\t\t\tcnames = dns.resolver.query(domain, 'CNAME')\n+\t\t\tfor cname in cnames:\n+    \t\t\t\tfor cname_url in take_over_cnames:\n+\t\t\t\t\tif str(cname_url) in str(cname.target):\n+\t\t\t\t\t\tfound_cname = True\n+\n+\t\t\tif found_cname and found_content:\n+\t\t\t\tprint(\"[+]\"+domain)\n+\t\t\t\tif is_closed:\n+\t\t\t\t\tfile = open(output_file,\"w+\")\n+\t\t\t\t\tis_closed = False\n+\t\t\t\tfile.write(domain+\"\\n\")\n+\t\t\telse:\n+\t\t\t\tprint(\"[-]\"+domain)\n+\t\texcept:\n+\t\t\tprint \"[-]No cnames for \"+domain\n+\n+if is_closed == False:\n+\tfile.close()\n+\n+print(\"\\n-- Done --\")\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys, dns.resolver"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, ""], [11, "take_over_cnames = [\"createsend\","], [12, "\"cargocollective\","], [13, "\"cloudfront\","], [14, "\"desk.com\","], [15, "\"fastly.net\","], [16, "\"feedpress.me\","], [17, "\"freshdesk.com\","], [18, "\"github.io\","], [19, "\"helpjuice.com\","], [20, "\"helpscoutdocs.com\","], [21, "\"herokudns.com\","], [22, "\"herokussl.com\","], [23, "\"herokuapp.com\","], [24, "\"pageserve.co\","], [25, "\"pingdom.com\","], [26, "\"amazonaws.com\","], [27, "\"myshopify.com\","], [28, "\"stspg-customer.com\","], [29, "\"sgizmo.com\","], [30, "\"surveygizmo.eu\","], [31, "\"sgizmoca.com\","], [32, "\"sgizmoca.com\","], [33, "\"tictail.com\","], [34, "\"domains.tumblr.com\","], [35, "\"uservoice.com\","], [36, "\"wpengine.com\","], [37, "\"squarespace.com\","], [38, "\"unbounce.com\","], [39, "\"zendesk.com\"]"], [40, ""], [41, "take_over_content = [\"<strong>Trying to access your account\","], [42, "\"Use a personal domain name\","], [43, "\"The request could not be satisfied\","], [44, "\"Sorry, We Couldn't Find That Page\","], [45, "\"Fastly error: unknown domain\","], [46, "\"The feed has not been found\","], [47, "\"You can claim it now at\","], [48, "\"Publishing platform\","], [49, "\"There isn't a GitHub Pages site here\","], [50, "\"No settings were found for this company\","], [51, "\"<title>No such app</title>\","], [52, "\"You've Discovered A Missing Link. Our Apologies!\","], [53, "\"Sorry, couldn&rsquo;t find the status page\","], [54, "\"NoSuchBucket\","], [55, "\"Sorry, this shop is currently unavailable\","], [56, "\"<title>Hosted Status Pages for Your Company</title>\","], [57, "\"data-html-name=\\\"Header Logo Link\\\"\","], [58, "\"<title>Oops - We didn't find your site.</title>\","], [59, "\"class=\\\"MarketplaceHeader__tictailLogo\\\"\","], [60, "\"Whatever you were looking for doesn't currently exist at this address\","], [61, "\"The requested URL was not found on this server\","], [62, "\"The page you have requested does not exist\","], [63, "\"This UserVoice subdomain is currently available!\","], [64, "\"but is not configured for an account on our platform\","], [65, "\"<title>Help Center Closed | Zendesk</title>\"]"], [66, ""], [67, "print(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [68, ""], [69, ""], [70, "for domain in domains:"], [71, "\t#Skip first row, lol"], [72, "\tif domain != domains[0]:"], [73, "\t\tfound_content = False"], [74, "\t\tfound_cname = False"], [75, "\t\ttry:"], [76, "\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text"], [77, "\t\texcept:"], [78, "\t\t\tprint(\"[-]Error in http://\"+domain)"], [79, ""], [80, "\t\tfor content in take_over_content:"], [81, "\t\t\tif str(content) in r:"], [82, "\t\t\t\tfound_content = True"], [83, ""], [84, "\t\ttry:"], [85, "\t\t\tcnames = dns.resolver.query(domain, 'CNAME')"], [86, "\t\t\tfor cname in cnames:"], [87, "    \t\t\t\tfor cname_url in take_over_cnames:"], [88, "\t\t\t\t\tif str(cname_url) in str(cname.target):"], [89, "\t\t\t\t\t\tfound_cname = True"], [90, ""], [91, "\t\t\tif found_cname and found_content:"], [92, "\t\t\t\tprint(\"[+]\"+domain)"], [93, "\t\t\t\tif is_closed:"], [94, "\t\t\t\t\tfile = open(output_file,\"w+\")"], [95, "\t\t\t\t\tis_closed = False"], [96, "\t\t\t\tfile.write(domain+\"\\n\")"], [97, "\t\t\telse:"], [98, "\t\t\t\tprint(\"[-]\"+domain)"], [99, "\t\texcept:"], [100, "\t\t\tprint \"[-]No cnames for \"+domain"], [101, ""], [102, "if is_closed == False:"], [103, "\tfile.close()"], [104, ""], [105, "print(\"\\n-- Done --\")"], [106, ""], [107, ""], [108, ""], [109, ""], [110, ""], [111, ""], [112, ""], [113, ""], [114, ""], [115, ""], [116, ""], [117, ""]], "deleted": []}, "added_lines": 117, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport requests, sys, dns.resolver\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\n\ntake_over_cnames = [\"createsend\",\n\"cargocollective\",\n\"cloudfront\",\n\"desk.com\",\n\"fastly.net\",\n\"feedpress.me\",\n\"freshdesk.com\",\n\"github.io\",\n\"helpjuice.com\",\n\"helpscoutdocs.com\",\n\"herokudns.com\",\n\"herokussl.com\",\n\"herokuapp.com\",\n\"pageserve.co\",\n\"pingdom.com\",\n\"amazonaws.com\",\n\"myshopify.com\",\n\"stspg-customer.com\",\n\"sgizmo.com\",\n\"surveygizmo.eu\",\n\"sgizmoca.com\",\n\"sgizmoca.com\",\n\"tictail.com\",\n\"domains.tumblr.com\",\n\"uservoice.com\",\n\"wpengine.com\",\n\"squarespace.com\",\n\"unbounce.com\",\n\"zendesk.com\"]\n\ntake_over_content = [\"<strong>Trying to access your account\",\n\"Use a personal domain name\",\n\"The request could not be satisfied\",\n\"Sorry, We Couldn't Find That Page\",\n\"Fastly error: unknown domain\",\n\"The feed has not been found\",\n\"You can claim it now at\",\n\"Publishing platform\",                        \n\"There isn't a GitHub Pages site here\",                       \n\"No settings were found for this company\",\n\"<title>No such app</title>\",                        \n\"You've Discovered A Missing Link. Our Apologies!\",\n\"Sorry, couldn&rsquo;t find the status page\",                        \n\"NoSuchBucket\",\n\"Sorry, this shop is currently unavailable\",\n\"<title>Hosted Status Pages for Your Company</title>\",\n\"data-html-name=\\\"Header Logo Link\\\"\",                        \n\"<title>Oops - We didn't find your site.</title>\",\n\"class=\\\"MarketplaceHeader__tictailLogo\\\"\",                        \n\"Whatever you were looking for doesn't currently exist at this address\",\n\"The requested URL was not found on this server\",\n\"The page you have requested does not exist\",\n\"This UserVoice subdomain is currently available!\",\n\"but is not configured for an account on our platform\",\n\"<title>Help Center Closed | Zendesk</title>\"]\n\nprint(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\n\nfor domain in domains:\n\t#Skip first row, lol\n\tif domain != domains[0]:\n\t\tfound_content = False\n\t\tfound_cname = False\n\t\ttry:\n\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text\n\t\texcept:\n\t\t\tprint(\"[-]Error in http://\"+domain)\n\n\t\tfor content in take_over_content:\n\t\t\tif str(content) in r:\n\t\t\t\tfound_content = True\n\n\t\ttry:\n\t\t\tcnames = dns.resolver.query(domain, 'CNAME')\n\t\t\tfor cname in cnames:\n    \t\t\t\tfor cname_url in take_over_cnames:\n\t\t\t\t\tif str(cname_url) in str(cname.target):\n\t\t\t\t\t\tfound_cname = True\n\n\t\t\tif found_cname and found_content:\n\t\t\t\tprint(\"[+]\"+domain)\n\t\t\t\tif is_closed:\n\t\t\t\t\tfile = open(output_file,\"w+\")\n\t\t\t\t\tis_closed = False\n\t\t\t\tfile.write(domain+\"\\n\")\n\t\t\telse:\n\t\t\t\tprint(\"[-]\"+domain)\n\t\texcept:\n\t\t\tprint \"[-]No cnames for \"+domain\n\nif is_closed == False:\n\tfile.close()\n\nprint(\"\\n-- Done --\")\n\n\n\n\n\n\n\n\n\n\n\n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 90, "complexity": 0, "token_count": 332}, {"old_path": null, "new_path": "wordpress_check.py", "filename": "wordpress_check.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,34 @@\n+#!/usr/bin/python\n+\n+import requests, sys\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+is_closed = True\n+\n+domains = open(input_file,'r').read().split('\\n')\n+\n+print(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n+\n+for domain in domains:\n+    if domain != \"\":\n+        try:\n+            response = requests.get(\"https://\"+domain)\n+        except:\n+            print(\"[-]Error on https://\"+domain)\n+\n+        if \"/wp-content/\" in response.content:\n+            if is_closed:\n+                file = open(output_file,\"w+\")\n+            is_closed = False\n+            print(\"[+]\"+domain)\n+            file.write(domain + \"\\n\")\n+        else:\n+            print(\"[-]\"+domain)\n+    else:\n+        print(\"[-]Domain is invalid\")\n+        \n+if is_closed == False:\n+    file.close()\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, ""], [11, "print(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [12, ""], [13, "for domain in domains:"], [14, "    if domain != \"\":"], [15, "        try:"], [16, "            response = requests.get(\"https://\"+domain)"], [17, "        except:"], [18, "            print(\"[-]Error on https://\"+domain)"], [19, ""], [20, "        if \"/wp-content/\" in response.content:"], [21, "            if is_closed:"], [22, "                file = open(output_file,\"w+\")"], [23, "            is_closed = False"], [24, "            print(\"[+]\"+domain)"], [25, "            file.write(domain + \"\\n\")"], [26, "        else:"], [27, "            print(\"[-]\"+domain)"], [28, "    else:"], [29, "        print(\"[-]Domain is invalid\")"], [30, ""], [31, "if is_closed == False:"], [32, "    file.close()"], [33, ""], [34, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 34, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\n\nprint(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    if domain != \"\":\n        try:\n            response = requests.get(\"https://\"+domain)\n        except:\n            print(\"[-]Error on https://\"+domain)\n\n        if \"/wp-content/\" in response.content:\n            if is_closed:\n                file = open(output_file,\"w+\")\n            is_closed = False\n            print(\"[+]\"+domain)\n            file.write(domain + \"\\n\")\n        else:\n            print(\"[-]\"+domain)\n    else:\n        print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 25, "complexity": 0, "token_count": 145}, {"old_path": null, "new_path": "wpscan_domains.sh", "filename": "wpscan_domains.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,13 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n+\n+if [ ! -f $1 ]; then\n+    echo \"[-]File not found!\"\n+else\n+    while read domain; do \n+        echo \"[+]Opening $domain\"\n+        xterm -hold -e \"/home/rjp/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n+    done < $1\n+fi\n+printf \"\\n -- Done -- \\n\"\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    echo \"[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        echo \"[+]Opening $domain\""], [10, "        xterm -hold -e \"/home/rjp/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &"], [11, "    done < $1"], [12, "fi"], [13, "printf \"\\n -- Done -- \\n\""]], "deleted": []}, "added_lines": 13, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"/home/rjp/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": 0.34, "dmm_unit_complexity": 0.34, "dmm_unit_interfacing": 1.0},
    {"hash": "c15b430362a13998a6b8a023748f36e084bded3c", "msg": "removed absolute path", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:01:03+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:01:03+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["ecca419a484877fc45454372e5262d1b2d68d68a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "wpscan_domains.sh", "new_path": "wpscan_domains.sh", "filename": "wpscan_domains.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -7,7 +7,7 @@ if [ ! -f $1 ]; then\n else\n     while read domain; do \n         echo \"[+]Opening $domain\"\n-        xterm -hold -e \"/home/rjp/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n+        xterm -hold -e \"/home/YOUR_USER/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n     done < $1\n fi\n printf \"\\n -- Done -- \\n\"\n", "diff_parsed": {"added": [[10, "        xterm -hold -e \"/home/YOUR_USER/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &"]], "deleted": [[10, "        xterm -hold -e \"/home/rjp/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"/home/YOUR_USER/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"/home/rjp/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "643d3511bc2cafb0cb98f701209e4d103b4aedda", "msg": "Update start_recons.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:03:17+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:03:17+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["c15b430362a13998a6b8a023748f36e084bded3c"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "start_recons.py", "new_path": "start_recons.py", "filename": "start_recons.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -34,7 +34,7 @@ domains = [\"lyst.com\",\n \"scan.me\"]\n \n def run(d):\n-\tos.system(\"/home/rjp/Documents/Tools/start_recon.sh \"+d)\n+\tos.system(\"/home/YOUR_USER/Documents/Tools/start_recon.sh \"+d)\n \n for domain in domains:\n \tif domain is not \"\":\n", "diff_parsed": {"added": [[37, "\tos.system(\"/home/YOUR_USER/Documents/Tools/start_recon.sh \"+d)"]], "deleted": [[37, "\tos.system(\"/home/rjp/Documents/Tools/start_recon.sh \"+d)"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/usr/bin/python\n\nimport sys, os, time, threading\n\ndomains = [\"lyst.com\",\n\"bimeanalytics.com\",\n\"toytalk.com\",\n\"goodhire.com\",\n\"identity.com\",\n\"glasswire.com\",\n\"keybase.io\",\n\"quora.com\",\n\"nextcloud.com\",\n\"boards.greenhouse.io\",\n\"trello.com\",\n\"trello.services\",\n\"badoo.com\",\n\"pinion.gg\",\n\"unikrn.com\",\n\"spotify.com\",\n\"mapbox.com\",\n\"semrush.com\",\n\"ok.ru\",\n\"booztx.com\",\n\"irccloud.com\",\n\"irccloud-cdn.com\",\n\"udemy.com\",\n\"legalrobot.com\",\n\"harvestapp.com\",\n\"forecastapp.com\",\n\"spectacles.com\",\n\"bitstrips.com\",\n\"bitmoji.com\",\n\"scan.me\"]\n\ndef run(d):\n\tos.system(\"/home/YOUR_USER/Documents/Tools/start_recon.sh \"+d)\n\nfor domain in domains:\n\tif domain is not \"\":\n\t\tthr = threading.Thread(target=run, args=(domain,))\n\t\tthr.start()\n\t\ttime.sleep(500)\n\n\n", "source_code_before": "#!/usr/bin/python\n\nimport sys, os, time, threading\n\ndomains = [\"lyst.com\",\n\"bimeanalytics.com\",\n\"toytalk.com\",\n\"goodhire.com\",\n\"identity.com\",\n\"glasswire.com\",\n\"keybase.io\",\n\"quora.com\",\n\"nextcloud.com\",\n\"boards.greenhouse.io\",\n\"trello.com\",\n\"trello.services\",\n\"badoo.com\",\n\"pinion.gg\",\n\"unikrn.com\",\n\"spotify.com\",\n\"mapbox.com\",\n\"semrush.com\",\n\"ok.ru\",\n\"booztx.com\",\n\"irccloud.com\",\n\"irccloud-cdn.com\",\n\"udemy.com\",\n\"legalrobot.com\",\n\"harvestapp.com\",\n\"forecastapp.com\",\n\"spectacles.com\",\n\"bitstrips.com\",\n\"bitmoji.com\",\n\"scan.me\"]\n\ndef run(d):\n\tos.system(\"/home/rjp/Documents/Tools/start_recon.sh \"+d)\n\nfor domain in domains:\n\tif domain is not \"\":\n\t\tthr = threading.Thread(target=run, args=(domain,))\n\t\tthr.start()\n\t\ttime.sleep(500)\n\n\n", "methods": [{"name": "run", "start_line": 36, "end_line": 37}], "methods_before": [{"name": "run", "start_line": 36, "end_line": 37}], "changed_methods": [{"name": "run", "start_line": 36, "end_line": 37}], "nloc": 38, "complexity": 1, "token_count": 124}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "8f02934a57e7d19b215c2cafa61e84fdbd92edb3", "msg": "Delete start_recons.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:05:09+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:05:09+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["643d3511bc2cafb0cb98f701209e4d103b4aedda"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 45, "insertions": 0, "lines": 45, "files": 1, "modified_files": [{"old_path": "start_recons.py", "new_path": null, "filename": "start_recons.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,45 +0,0 @@\n-#!/usr/bin/python\n-\n-import sys, os, time, threading\n-\n-domains = [\"lyst.com\",\n-\"bimeanalytics.com\",\n-\"toytalk.com\",\n-\"goodhire.com\",\n-\"identity.com\",\n-\"glasswire.com\",\n-\"keybase.io\",\n-\"quora.com\",\n-\"nextcloud.com\",\n-\"boards.greenhouse.io\",\n-\"trello.com\",\n-\"trello.services\",\n-\"badoo.com\",\n-\"pinion.gg\",\n-\"unikrn.com\",\n-\"spotify.com\",\n-\"mapbox.com\",\n-\"semrush.com\",\n-\"ok.ru\",\n-\"booztx.com\",\n-\"irccloud.com\",\n-\"irccloud-cdn.com\",\n-\"udemy.com\",\n-\"legalrobot.com\",\n-\"harvestapp.com\",\n-\"forecastapp.com\",\n-\"spectacles.com\",\n-\"bitstrips.com\",\n-\"bitmoji.com\",\n-\"scan.me\"]\n-\n-def run(d):\n-\tos.system(\"/home/YOUR_USER/Documents/Tools/start_recon.sh \"+d)\n-\n-for domain in domains:\n-\tif domain is not \"\":\n-\t\tthr = threading.Thread(target=run, args=(domain,))\n-\t\tthr.start()\n-\t\ttime.sleep(500)\n-\n-\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, ""], [3, "import sys, os, time, threading"], [4, ""], [5, "domains = [\"lyst.com\","], [6, "\"bimeanalytics.com\","], [7, "\"toytalk.com\","], [8, "\"goodhire.com\","], [9, "\"identity.com\","], [10, "\"glasswire.com\","], [11, "\"keybase.io\","], [12, "\"quora.com\","], [13, "\"nextcloud.com\","], [14, "\"boards.greenhouse.io\","], [15, "\"trello.com\","], [16, "\"trello.services\","], [17, "\"badoo.com\","], [18, "\"pinion.gg\","], [19, "\"unikrn.com\","], [20, "\"spotify.com\","], [21, "\"mapbox.com\","], [22, "\"semrush.com\","], [23, "\"ok.ru\","], [24, "\"booztx.com\","], [25, "\"irccloud.com\","], [26, "\"irccloud-cdn.com\","], [27, "\"udemy.com\","], [28, "\"legalrobot.com\","], [29, "\"harvestapp.com\","], [30, "\"forecastapp.com\","], [31, "\"spectacles.com\","], [32, "\"bitstrips.com\","], [33, "\"bitmoji.com\","], [34, "\"scan.me\"]"], [35, ""], [36, "def run(d):"], [37, "\tos.system(\"/home/YOUR_USER/Documents/Tools/start_recon.sh \"+d)"], [38, ""], [39, "for domain in domains:"], [40, "\tif domain is not \"\":"], [41, "\t\tthr = threading.Thread(target=run, args=(domain,))"], [42, "\t\tthr.start()"], [43, "\t\ttime.sleep(500)"], [44, ""], [45, ""]]}, "added_lines": 0, "deleted_lines": 45, "source_code": null, "source_code_before": "#!/usr/bin/python\n\nimport sys, os, time, threading\n\ndomains = [\"lyst.com\",\n\"bimeanalytics.com\",\n\"toytalk.com\",\n\"goodhire.com\",\n\"identity.com\",\n\"glasswire.com\",\n\"keybase.io\",\n\"quora.com\",\n\"nextcloud.com\",\n\"boards.greenhouse.io\",\n\"trello.com\",\n\"trello.services\",\n\"badoo.com\",\n\"pinion.gg\",\n\"unikrn.com\",\n\"spotify.com\",\n\"mapbox.com\",\n\"semrush.com\",\n\"ok.ru\",\n\"booztx.com\",\n\"irccloud.com\",\n\"irccloud-cdn.com\",\n\"udemy.com\",\n\"legalrobot.com\",\n\"harvestapp.com\",\n\"forecastapp.com\",\n\"spectacles.com\",\n\"bitstrips.com\",\n\"bitmoji.com\",\n\"scan.me\"]\n\ndef run(d):\n\tos.system(\"/home/YOUR_USER/Documents/Tools/start_recon.sh \"+d)\n\nfor domain in domains:\n\tif domain is not \"\":\n\t\tthr = threading.Thread(target=run, args=(domain,))\n\t\tthr.start()\n\t\ttime.sleep(500)\n\n\n", "methods": [], "methods_before": [{"name": "run", "start_line": 36, "end_line": 37}], "changed_methods": [{"name": "run", "start_line": 36, "end_line": 37}], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": 0.0, "dmm_unit_complexity": 0.0, "dmm_unit_interfacing": 0.0},
    {"hash": "7a6a17ef6eb3163063111b97dc0fbd02f21ab7dc", "msg": "Removed absolute path", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:06:20+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:06:20+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["8f02934a57e7d19b215c2cafa61e84fdbd92edb3"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "open_redirect.py", "new_path": "open_redirect.py", "filename": "open_redirect.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -9,7 +9,7 @@ def start():\n \r\n     is_closed = True\r\n \r\n-    payloads = open(\"/home/rjp/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n+    payloads = open(\"/home/YOUR_USER/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n \r\n     #First loop trough the payloads to prevent 429 (rate limit)\r\n     for payload in payloads: \r\n", "diff_parsed": {"added": [[12, "    payloads = open(\"/home/YOUR_USER/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')"]], "deleted": [[12, "    payloads = open(\"/home/rjp/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(\"/home/YOUR_USER/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n\r\n                if r.history:  \r\n                    if r.url == \"https://example.com\":\r\n                        print \"[+]\"+url\r\n                        if is_closed:\r\n                            file = open(output_file,\"w+\")\r\n                        is_closed = False\r\n                        file.write(url + \"\\n\")\r\n                    else:\r\n                        print \"[-]\"+url\r\n                else:\r\n                    print \"[-]\"+url\r\n        else:\r\n            print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "source_code_before": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(\"/home/rjp/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n\r\n                if r.history:  \r\n                    if r.url == \"https://example.com\":\r\n                        print \"[+]\"+url\r\n                        if is_closed:\r\n                            file = open(output_file,\"w+\")\r\n                        is_closed = False\r\n                        file.write(url + \"\\n\")\r\n                    else:\r\n                        print \"[-]\"+url\r\n                else:\r\n                    print \"[-]\"+url\r\n        else:\r\n            print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "methods": [{"name": "start", "start_line": 4, "end_line": 45}], "methods_before": [{"name": "start", "start_line": 4, "end_line": 45}], "changed_methods": [{"name": "start", "start_line": 4, "end_line": 45}], "nloc": 35, "complexity": 9, "token_count": 204}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "ef91e7749b7f6a0e4c4f92712d3e5f9df5d1991a", "msg": "Removed absolute path", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:07:39+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:07:39+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["7a6a17ef6eb3163063111b97dc0fbd02f21ab7dc"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "header_scan.py", "new_path": "header_scan.py", "filename": "header_scan.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -7,7 +7,7 @@ print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" w\n \r\n is_closed = True\r\n domains = open(input_file,'r').read().split('\\n')\r\n-headers = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/headers.txt')]\r\n+headers = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/headers.txt')]\r\n \r\n for domain in domains:\r\n \tif domain != \"\":\r\n", "diff_parsed": {"added": [[10, "headers = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/headers.txt')]"]], "deleted": [[10, "headers = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/headers.txt')]"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/headers.txt')]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None:\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "source_code_before": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/headers.txt')]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None:\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 31, "complexity": 0, "token_count": 225}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "222672f0c08119887373b759c2956d6cca5434b1", "msg": "Removed absolute path", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:08:41+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:08:41+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["ef91e7749b7f6a0e4c4f92712d3e5f9df5d1991a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "error_page_info_check.py", "new_path": "error_page_info_check.py", "filename": "error_page_info_check.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -7,7 +7,7 @@ output_file = sys.argv[2]\n is_closed = True\n \n domains = open(input_file,'r').read().split('\\n')\n-info = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/error_pages_info.txt')]\n+info = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/error_pages_info.txt')]\n \n print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n \n", "diff_parsed": {"added": [[10, "info = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/error_pages_info.txt')]"]], "deleted": [[10, "info = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/error_pages_info.txt')]"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\ninfo = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/error_pages_info.txt')]\n\nprint(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n\npayloads = [\"/\",\n            \"/NotFound123\",\n            \"/.htaccess\",\n            \"/<>\"]\n\nfor payload in payloads:\n    print \"\\n - Trying payload \"+payload+\" - \"\n    for domain in domains:\n        info_found = \"\"\n        if domain != \"\":\n            found = False\n            try:\n                response = requests.get(\"http://\"+domain+payload)\n            except:\n                print(\"[-]Error on http://\"+domain+payload)\n            for i in info:\n                if i.lower() in response.content.lower():\n                    found = True\n                    #Search and get the line in the response that contains i.lower()\n                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n\n            if found:\n                if is_closed:\n                    file = open(output_file,\"w+\")\n                is_closed = False\n                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n            else:\n               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n                \n        else:\n            print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\ninfo = [line.rstrip('\\n').lower() for line in open('/home/rjp/Documents/Wordlists/error_pages_info.txt')]\n\nprint(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n\npayloads = [\"/\",\n            \"/NotFound123\",\n            \"/.htaccess\",\n            \"/<>\"]\n\nfor payload in payloads:\n    print \"\\n - Trying payload \"+payload+\" - \"\n    for domain in domains:\n        info_found = \"\"\n        if domain != \"\":\n            found = False\n            try:\n                response = requests.get(\"http://\"+domain+payload)\n            except:\n                print(\"[-]Error on http://\"+domain+payload)\n            for i in info:\n                if i.lower() in response.content.lower():\n                    found = True\n                    #Search and get the line in the response that contains i.lower()\n                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n\n            if found:\n                if is_closed:\n                    file = open(output_file,\"w+\")\n                is_closed = False\n                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n            else:\n               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n                \n        else:\n            print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 38, "complexity": 0, "token_count": 278}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "f41dc1fea9189dd0872843f6daea337cb0584956", "msg": "Added credit", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-20 17:16:29+01:00", "author_timezone": -3600, "committer_date": "2017-11-20 17:16:29+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["222672f0c08119887373b759c2956d6cca5434b1"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 1, "lines": 1, "files": 1, "modified_files": [{"old_path": "cors_misconfiguration_scan.sh", "new_path": "cors_misconfiguration_scan.sh", "filename": "cors_misconfiguration_scan.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -23,3 +23,4 @@ else\n fi\n \n printf \"\\n -- Done -- \\n\"\n+#Credits to Tomnomnom for the help :)\n", "diff_parsed": {"added": [[26, "#Credits to Tomnomnom for the help :)"]], "deleted": []}, "added_lines": 1, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do\n        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n\n                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n                fi\n            else\n                echo \"[-]$domain [$origin]\"\n            fi\n        done\n    done < $1 \nfi\n\nprintf \"\\n -- Done -- \\n\"\n#Credits to Tomnomnom for the help :)\n", "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do\n        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n\n                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n                fi\n            else\n                echo \"[-]$domain [$origin]\"\n            fi\n        done\n    done < $1 \nfi\n\nprintf \"\\n -- Done -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "1c598de4bb5d8c470220c1659bf4edf68b231d0a", "msg": "Delete cors_misconfiguration_scan.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 17:48:36+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 17:48:36+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["f41dc1fea9189dd0872843f6daea337cb0584956"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 26, "insertions": 0, "lines": 26, "files": 1, "modified_files": [{"old_path": "cors_misconfiguration_scan.sh", "new_path": null, "filename": "cors_misconfiguration_scan.sh", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,26 +0,0 @@\n-#!/bin/bash\n-\n-printf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n-\n-if [ ! -f $1 ]; then\n-    echo \"[-]File not found!\"\n-else\n-    while read domain; do\n-        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n-            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n-                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n-                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n-\n-                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n-                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n-                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n-                fi\n-            else\n-                echo \"[-]$domain [$origin]\"\n-            fi\n-        done\n-    done < $1 \n-fi\n-\n-printf \"\\n -- Done -- \\n\"\n-#Credits to Tomnomnom for the help :)\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    echo \"[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do"], [10, "            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then"], [11, "                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\""], [12, "                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2"], [13, ""], [14, "                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then"], [15, "                    echo \"[+](Allow-Credentials) $domain [$origin]\""], [16, "                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2"], [17, "                fi"], [18, "            else"], [19, "                echo \"[-]$domain [$origin]\""], [20, "            fi"], [21, "        done"], [22, "    done < $1"], [23, "fi"], [24, ""], [25, "printf \"\\n -- Done -- \\n\""], [26, "#Credits to Tomnomnom for the help :)"]]}, "added_lines": 0, "deleted_lines": 26, "source_code": null, "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do\n        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n\n                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n                fi\n            else\n                echo \"[-]$domain [$origin]\"\n            fi\n        done\n    done < $1 \nfi\n\nprintf \"\\n -- Done -- \\n\"\n#Credits to Tomnomnom for the help :)\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "f071eb2154384b95202476535c01a990ea5373c6", "msg": "Delete crlf.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 17:48:43+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 17:48:43+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["1c598de4bb5d8c470220c1659bf4edf68b231d0a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 21, "insertions": 0, "lines": 21, "files": 1, "modified_files": [{"old_path": "crlf.sh", "new_path": null, "filename": "crlf.sh", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,21 +0,0 @@\n-#!/bin/bash\n-\n-printf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\"\n-\n-green=\"tput setaf 2\"\n-reset=\"tput sgr0\"\n-\n-#First loop trough the payloads to prevent 429 (rate limit)\n-while read payload; do \n-    while read domain; do \n-        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then \n-            echo \"${green}[+]${reset}$domain/$payload\"\n-            echo \"$domain/$payload\" >> $2\n-        else\n-            echo \"[-]$domain/$payload\"\n-        fi\n-    done < $1\n-done < ~/Documents/Wordlists/crlf.txt \n-\n-printf \"\\n-- Done --\"\n-\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\""], [4, ""], [5, "green=\"tput setaf 2\""], [6, "reset=\"tput sgr0\""], [7, ""], [8, "#First loop trough the payloads to prevent 429 (rate limit)"], [9, "while read payload; do"], [10, "    while read domain; do"], [11, "        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then"], [12, "            echo \"${green}[+]${reset}$domain/$payload\""], [13, "            echo \"$domain/$payload\" >> $2"], [14, "        else"], [15, "            echo \"[-]$domain/$payload\""], [16, "        fi"], [17, "    done < $1"], [18, "done < ~/Documents/Wordlists/crlf.txt"], [19, ""], [20, "printf \"\\n-- Done --\""], [21, ""]]}, "added_lines": 0, "deleted_lines": 21, "source_code": null, "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\"\n\ngreen=\"tput setaf 2\"\nreset=\"tput sgr0\"\n\n#First loop trough the payloads to prevent 429 (rate limit)\nwhile read payload; do \n    while read domain; do \n        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then \n            echo \"${green}[+]${reset}$domain/$payload\"\n            echo \"$domain/$payload\" >> $2\n        else\n            echo \"[-]$domain/$payload\"\n        fi\n    done < $1\ndone < ~/Documents/Wordlists/crlf.txt \n\nprintf \"\\n-- Done --\"\n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "b9099eba0350d31455c72d8b6ec8bbac02875049", "msg": "Delete error_page_info_check.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 17:48:51+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 17:48:51+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["f071eb2154384b95202476535c01a990ea5373c6"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 50, "insertions": 0, "lines": 50, "files": 1, "modified_files": [{"old_path": "error_page_info_check.py", "new_path": null, "filename": "error_page_info_check.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,50 +0,0 @@\n-#!/usr/bin/python\n-\n-import requests, sys\n-\n-input_file = sys.argv[1]\n-output_file = sys.argv[2]\n-is_closed = True\n-\n-domains = open(input_file,'r').read().split('\\n')\n-info = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/error_pages_info.txt')]\n-\n-print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n-\n-payloads = [\"/\",\n-            \"/NotFound123\",\n-            \"/.htaccess\",\n-            \"/<>\"]\n-\n-for payload in payloads:\n-    print \"\\n - Trying payload \"+payload+\" - \"\n-    for domain in domains:\n-        info_found = \"\"\n-        if domain != \"\":\n-            found = False\n-            try:\n-                response = requests.get(\"http://\"+domain+payload)\n-            except:\n-                print(\"[-]Error on http://\"+domain+payload)\n-            for i in info:\n-                if i.lower() in response.content.lower():\n-                    found = True\n-                    #Search and get the line in the response that contains i.lower()\n-                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n-\n-            if found:\n-                if is_closed:\n-                    file = open(output_file,\"w+\")\n-                is_closed = False\n-                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n-                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n-            else:\n-               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n-                \n-        else:\n-            print(\"[-]Domain is invalid\")\n-        \n-if is_closed == False:\n-    file.close()\n-\n-print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, "info = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/error_pages_info.txt')]"], [11, ""], [12, "print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")"], [13, ""], [14, "payloads = [\"/\","], [15, "            \"/NotFound123\","], [16, "            \"/.htaccess\","], [17, "            \"/<>\"]"], [18, ""], [19, "for payload in payloads:"], [20, "    print \"\\n - Trying payload \"+payload+\" - \""], [21, "    for domain in domains:"], [22, "        info_found = \"\""], [23, "        if domain != \"\":"], [24, "            found = False"], [25, "            try:"], [26, "                response = requests.get(\"http://\"+domain+payload)"], [27, "            except:"], [28, "                print(\"[-]Error on http://\"+domain+payload)"], [29, "            for i in info:"], [30, "                if i.lower() in response.content.lower():"], [31, "                    found = True"], [32, "                    #Search and get the line in the response that contains i.lower()"], [33, "                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]"], [34, ""], [35, "            if found:"], [36, "                if is_closed:"], [37, "                    file = open(output_file,\"w+\")"], [38, "                is_closed = False"], [39, "                print(\"[+]\"+domain+payload+\" - \"+str(info_found))"], [40, "                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")"], [41, "            else:"], [42, "               print(\"[-]\"+domain+payload+\" - \"+str(info_found))"], [43, ""], [44, "        else:"], [45, "            print(\"[-]Domain is invalid\")"], [46, ""], [47, "if is_closed == False:"], [48, "    file.close()"], [49, ""], [50, "print(\"\\n-- Done --\")"]]}, "added_lines": 0, "deleted_lines": 50, "source_code": null, "source_code_before": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\ninfo = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/error_pages_info.txt')]\n\nprint(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n\npayloads = [\"/\",\n            \"/NotFound123\",\n            \"/.htaccess\",\n            \"/<>\"]\n\nfor payload in payloads:\n    print \"\\n - Trying payload \"+payload+\" - \"\n    for domain in domains:\n        info_found = \"\"\n        if domain != \"\":\n            found = False\n            try:\n                response = requests.get(\"http://\"+domain+payload)\n            except:\n                print(\"[-]Error on http://\"+domain+payload)\n            for i in info:\n                if i.lower() in response.content.lower():\n                    found = True\n                    #Search and get the line in the response that contains i.lower()\n                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n\n            if found:\n                if is_closed:\n                    file = open(output_file,\"w+\")\n                is_closed = False\n                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n            else:\n               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n                \n        else:\n            print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "755882af23dd464234528c3ec3b4a6fae6c5927c", "msg": "Delete header_scan.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 17:49:08+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 17:49:08+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["b9099eba0350d31455c72d8b6ec8bbac02875049"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 38, "insertions": 0, "lines": 38, "files": 1, "modified_files": [{"old_path": "header_scan.py", "new_path": null, "filename": "header_scan.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,38 +0,0 @@\n-import requests, sys\r\n-\r\n-input_file = sys.argv[1]\r\n-output_file = sys.argv[2]\r\n-\r\n-print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n-\r\n-is_closed = True\r\n-domains = open(input_file,'r').read().split('\\n')\r\n-headers = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/headers.txt')]\r\n-\r\n-for domain in domains:\r\n-\tif domain != \"\":\r\n-\t\ttry:\r\n-\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n-\t\texcept:\r\n-\t\t\tprint(\"[-]Error on https://\"+domain)\r\n-\t\theaders_found = []\r\n-\r\n-\t\tfor header in headers:\r\n-\t\t\tcurrent_header = r.headers.get(header.lower())\r\n-\t\t\tif current_header != None:\r\n-\t\t\t\theaders_found.append(str(current_header))\r\n-\t\tif headers_found != []:\r\n-\t\t\tif is_closed:\r\n-        \t\t\tfile = open(output_file,\"w+\")\r\n-\t\t\t\tis_closed = False\r\n-\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n-\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n-\t\telse:\r\n-\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n-\telse:\r\n-            print \"[-]Domain is invalid\"\r\n-\r\n-if is_closed == False:\r\n-\tfile.close()\r\n-\r\n-print(\"\\n-- Done --\")\r\n", "diff_parsed": {"added": [], "deleted": [[1, "import requests, sys"], [2, ""], [3, "input_file = sys.argv[1]"], [4, "output_file = sys.argv[2]"], [5, ""], [6, "print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [7, ""], [8, "is_closed = True"], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, "headers = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/headers.txt')]"], [11, ""], [12, "for domain in domains:"], [13, "\tif domain != \"\":"], [14, "\t\ttry:"], [15, "\t\t\tr = requests.head(\"https://\"+domain, timeout=5)"], [16, "\t\texcept:"], [17, "\t\t\tprint(\"[-]Error on https://\"+domain)"], [18, "\t\theaders_found = []"], [19, ""], [20, "\t\tfor header in headers:"], [21, "\t\t\tcurrent_header = r.headers.get(header.lower())"], [22, "\t\t\tif current_header != None:"], [23, "\t\t\t\theaders_found.append(str(current_header))"], [24, "\t\tif headers_found != []:"], [25, "\t\t\tif is_closed:"], [26, "        \t\t\tfile = open(output_file,\"w+\")"], [27, "\t\t\t\tis_closed = False"], [28, "\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))"], [29, "\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")"], [30, "\t\telse:"], [31, "\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))"], [32, "\telse:"], [33, "            print \"[-]Domain is invalid\""], [34, ""], [35, "if is_closed == False:"], [36, "\tfile.close()"], [37, ""], [38, "print(\"\\n-- Done --\")"]]}, "added_lines": 0, "deleted_lines": 38, "source_code": null, "source_code_before": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open('/home/YOUR_USER/Documents/Wordlists/headers.txt')]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None:\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "e0e9ec821d4836e0d688de1e0cb4998dfe1b4645", "msg": "Delete javascript_files_extractor.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:48:41+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:48:41+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["755882af23dd464234528c3ec3b4a6fae6c5927c"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 72, "insertions": 0, "lines": 72, "files": 1, "modified_files": [{"old_path": "javascript_files_extractor.py", "new_path": null, "filename": "javascript_files_extractor.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,72 +0,0 @@\n-#!/usr/bin/python\n-\n-import re, requests, sys\n-\n-input_file = sys.argv[1]\n-output_file = sys.argv[2]\n-\n-print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n-\n-domains = open(input_file,'r').read().split('\\n')\n-\n-file = open(output_file,\"w+\")\n-\n-black_listed_domains = [\"ajax.googleapis.com\",\n-                        \"cdn.optimizely.com\",\n-                        \"googletagmanager.com\",\n-                        \"fontawesome.com\"]\n-\n-for domain in domains:\n-\tdomain_written = False\n-\ti = 0\n-\tb_amount = 0\n-\tfull_domain = \"\"\n-\tif domain != \"\":\n-\t\tmatches = \"\"\n-\t\tr = \"\"\n-\t\tregex = r'script src=\"(.*?)\"'\n-\t\ttry:\n-\t\t\tr = requests.get(\"http://\"+domain).content\n-\t\texcept:\n-\t\t\tprint \"[-]Error in http://\"+domain\n-\n-\t\tmatches = re.findall(regex, r, re.MULTILINE)\n-\t\tif matches == []:\n-\t\t\tregex = r\"script src='(.*?)'\"\n-\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n-\n-\t\tfor m in matches:\n-\t\t\tif domain_written != True:\n-\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n-\t\t\t\tdomain_written = True\n-\t\n-\t\t\tblack_listed = False\n-\t\t\tfor b in black_listed_domains:\n-\t\t\t\tif b in m:\n-\t\t\t\t\tblack_listed = True\n-\n-\t\t\tif black_listed != True:\n-\t\t\t\tif m.startswith(\"/\"):\n-\t\t\t\t\tif m.startswith(\"//\"):\n-\t\t\t\t\t\tfull_domain = \"https:\"+m\n-\t\t\t\t\telse:\n-\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n-\t\t\t\telif m.startswith(\"http\"):\n-\t\t\t\t\tfull_domain = m\n-\t\t\t\telse:\n-\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n-\t\t\telse:\n-\t\t\t\tb_amount += 1\n-\n-\t\t\tif black_listed != True:\n-\t\t\t\ti += 1\n-\t\t\t\tfile.write(full_domain+\"\\n\")\n-\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n-\telse:\n-\t\tprint \"[-]Domain is invalid \" + domain\n-\n-\n-\n-file.close()\n-\n-print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, ""], [3, "import re, requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, ""], [8, "print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [9, ""], [10, "domains = open(input_file,'r').read().split('\\n')"], [11, ""], [12, "file = open(output_file,\"w+\")"], [13, ""], [14, "black_listed_domains = [\"ajax.googleapis.com\","], [15, "                        \"cdn.optimizely.com\","], [16, "                        \"googletagmanager.com\","], [17, "                        \"fontawesome.com\"]"], [18, ""], [19, "for domain in domains:"], [20, "\tdomain_written = False"], [21, "\ti = 0"], [22, "\tb_amount = 0"], [23, "\tfull_domain = \"\""], [24, "\tif domain != \"\":"], [25, "\t\tmatches = \"\""], [26, "\t\tr = \"\""], [27, "\t\tregex = r'script src=\"(.*?)\"'"], [28, "\t\ttry:"], [29, "\t\t\tr = requests.get(\"http://\"+domain).content"], [30, "\t\texcept:"], [31, "\t\t\tprint \"[-]Error in http://\"+domain"], [32, ""], [33, "\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [34, "\t\tif matches == []:"], [35, "\t\t\tregex = r\"script src='(.*?)'\""], [36, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [37, ""], [38, "\t\tfor m in matches:"], [39, "\t\t\tif domain_written != True:"], [40, "\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")"], [41, "\t\t\t\tdomain_written = True"], [42, ""], [43, "\t\t\tblack_listed = False"], [44, "\t\t\tfor b in black_listed_domains:"], [45, "\t\t\t\tif b in m:"], [46, "\t\t\t\t\tblack_listed = True"], [47, ""], [48, "\t\t\tif black_listed != True:"], [49, "\t\t\t\tif m.startswith(\"/\"):"], [50, "\t\t\t\t\tif m.startswith(\"//\"):"], [51, "\t\t\t\t\t\tfull_domain = \"https:\"+m"], [52, "\t\t\t\t\telse:"], [53, "\t\t\t\t\t\tfull_domain = \"https://\"+domain+m"], [54, "\t\t\t\telif m.startswith(\"http\"):"], [55, "\t\t\t\t\tfull_domain = m"], [56, "\t\t\t\telse:"], [57, "\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m"], [58, "\t\t\telse:"], [59, "\t\t\t\tb_amount += 1"], [60, ""], [61, "\t\t\tif black_listed != True:"], [62, "\t\t\t\ti += 1"], [63, "\t\t\t\tfile.write(full_domain+\"\\n\")"], [64, "\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain"], [65, "\telse:"], [66, "\t\tprint \"[-]Domain is invalid \" + domain"], [67, ""], [68, ""], [69, ""], [70, "file.close()"], [71, ""], [72, "print(\"\\n-- Done --\")"]]}, "added_lines": 0, "deleted_lines": 72, "source_code": null, "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains = open(input_file,'r').read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "5a963995525410243e1156a608e2ba9166b04a73", "msg": "Delete javascript_files_link_extractor.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:48:48+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:48:48+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["e0e9ec821d4836e0d688de1e0cb4998dfe1b4645"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 23, "insertions": 0, "lines": 23, "files": 1, "modified_files": [{"old_path": "javascript_files_link_extractor.sh", "new_path": null, "filename": "javascript_files_link_extractor.sh", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,23 +0,0 @@\n-#!/bin/bash\n-\n-printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n-\n-if [ ! -f $1 ]; then\n-    printf  \"\\n[-]File not found!\"\n-else\n-    while read domain; do \n-        if [[ $domain == \"-\"* ]]; then\n-            printf  \"\\n\\n\\n[+]-$domain--\"\n-        else\n-            if [ -z \"$domain\" ]; then\n-                printf \"\\n[-]Invalid domain $domain\"\n-            else\n-                printf \"\\n[+]$domain \\n\"\n-\t\techo \"----------------------\"\n-                command=\"ruby ~/Documents/Tools/relative-url-extractor/extract.rb $domain\"\n-                eval $command\n-            fi\n-        fi\n-    done < $1 >> $2\n-printf \"\\n -- Done -- \\n\"\n-fi\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    printf  \"\\n[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        if [[ $domain == \"-\"* ]]; then"], [10, "            printf  \"\\n\\n\\n[+]-$domain--\""], [11, "        else"], [12, "            if [ -z \"$domain\" ]; then"], [13, "                printf \"\\n[-]Invalid domain $domain\""], [14, "            else"], [15, "                printf \"\\n[+]$domain \\n\""], [16, "\t\techo \"----------------------\""], [17, "                command=\"ruby ~/Documents/Tools/relative-url-extractor/extract.rb $domain\""], [18, "                eval $command"], [19, "            fi"], [20, "        fi"], [21, "    done < $1 >> $2"], [22, "printf \"\\n -- Done -- \\n\""], [23, "fi"]]}, "added_lines": 0, "deleted_lines": 23, "source_code": null, "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n\nif [ ! -f $1 ]; then\n    printf  \"\\n[-]File not found!\"\nelse\n    while read domain; do \n        if [[ $domain == \"-\"* ]]; then\n            printf  \"\\n\\n\\n[+]-$domain--\"\n        else\n            if [ -z \"$domain\" ]; then\n                printf \"\\n[-]Invalid domain $domain\"\n            else\n                printf \"\\n[+]$domain \\n\"\n\t\techo \"----------------------\"\n                command=\"ruby ~/Documents/Tools/relative-url-extractor/extract.rb $domain\"\n                eval $command\n            fi\n        fi\n    done < $1 >> $2\nprintf \"\\n -- Done -- \\n\"\nfi\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "208a703f1613123d6880c9df27c932740f1dda82", "msg": "Delete nmap_scan.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:48:53+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:48:53+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["5a963995525410243e1156a608e2ba9166b04a73"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 13, "insertions": 0, "lines": 13, "files": 1, "modified_files": [{"old_path": "nmap_scan.sh", "new_path": null, "filename": "nmap_scan.sh", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,13 +0,0 @@\n-#!/bin/bash\n-\n-printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\n-echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n-\n-while read domain; do \n-    echo \"-- $domain --\"\n-    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\n-done < $1  >> $2\n-\n-echo \"-- Done --\"\n-\n-\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\""], [4, "echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\""], [5, ""], [6, "while read domain; do"], [7, "    echo \"-- $domain --\""], [8, "    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'"], [9, "done < $1  >> $2"], [10, ""], [11, "echo \"-- Done --\""], [12, ""], [13, ""]]}, "added_lines": 0, "deleted_lines": 13, "source_code": null, "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\necho \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n\nwhile read domain; do \n    echo \"-- $domain --\"\n    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\ndone < $1  >> $2\n\necho \"-- Done --\"\n\n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "8717e3ad3f300f3011248a09d12c024129498f2a", "msg": "Delete online.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:49:01+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:49:01+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["208a703f1613123d6880c9df27c932740f1dda82"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 45, "insertions": 0, "lines": 45, "files": 1, "modified_files": [{"old_path": "online.py", "new_path": null, "filename": "online.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,45 +0,0 @@\n-#!/usr/bin/python\n-\n-import httplib\n-import socket\n-import re\n-import sys\n-\n-def online(host):\n-    try:\n-        socket.gethostbyname(host)\n-    except socket.gaierror:\n-        return False\n-    else:\n-        return True\n-\n-def available(host, path=\"/\"):\n-    try:\n-        conn = httplib.HTTPConnection(host, timeout=5)\n-        conn.request(\"HEAD\", path)\n-        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n-            return True\n-    except StandardError:\n-        return False\n-\n-input_file = sys.argv[1]\n-output_file = sys.argv[2]\n-\n-input_file_open = open(input_file, 'r')\n-output_file_open = open(output_file, 'w+')\n-\n-domains = input_file_open.readlines()\n-\n-print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n-\n-for domain in domains:\n-    domain = domain.strip()\n-    if online(domain) == True and available(domain) == True:\n-        print(\"[+]\"+domain.strip())\n-        output_file_open.write(domain+\"\\n\")\n-    else:\n-        print(\"[-]\"+domain)\n-\n-input_file_open.close()\n-output_file_open.close()\n-print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, ""], [3, "import httplib"], [4, "import socket"], [5, "import re"], [6, "import sys"], [7, ""], [8, "def online(host):"], [9, "    try:"], [10, "        socket.gethostbyname(host)"], [11, "    except socket.gaierror:"], [12, "        return False"], [13, "    else:"], [14, "        return True"], [15, ""], [16, "def available(host, path=\"/\"):"], [17, "    try:"], [18, "        conn = httplib.HTTPConnection(host, timeout=5)"], [19, "        conn.request(\"HEAD\", path)"], [20, "        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):"], [21, "            return True"], [22, "    except StandardError:"], [23, "        return False"], [24, ""], [25, "input_file = sys.argv[1]"], [26, "output_file = sys.argv[2]"], [27, ""], [28, "input_file_open = open(input_file, 'r')"], [29, "output_file_open = open(output_file, 'w+')"], [30, ""], [31, "domains = input_file_open.readlines()"], [32, ""], [33, "print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")"], [34, ""], [35, "for domain in domains:"], [36, "    domain = domain.strip()"], [37, "    if online(domain) == True and available(domain) == True:"], [38, "        print(\"[+]\"+domain.strip())"], [39, "        output_file_open.write(domain+\"\\n\")"], [40, "    else:"], [41, "        print(\"[-]\"+domain)"], [42, ""], [43, "input_file_open.close()"], [44, "output_file_open.close()"], [45, "print(\"\\n-- Done --\")"]]}, "added_lines": 0, "deleted_lines": 45, "source_code": null, "source_code_before": "#!/usr/bin/python\n\nimport httplib\nimport socket\nimport re\nimport sys\n\ndef online(host):\n    try:\n        socket.gethostbyname(host)\n    except socket.gaierror:\n        return False\n    else:\n        return True\n\ndef available(host, path=\"/\"):\n    try:\n        conn = httplib.HTTPConnection(host, timeout=5)\n        conn.request(\"HEAD\", path)\n        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n            return True\n    except StandardError:\n        return False\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\ninput_file_open = open(input_file, 'r')\noutput_file_open = open(output_file, 'w+')\n\ndomains = input_file_open.readlines()\n\nprint(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    domain = domain.strip()\n    if online(domain) == True and available(domain) == True:\n        print(\"[+]\"+domain.strip())\n        output_file_open.write(domain+\"\\n\")\n    else:\n        print(\"[-]\"+domain)\n\ninput_file_open.close()\noutput_file_open.close()\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "changed_methods": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": 0.0, "dmm_unit_complexity": 0.0, "dmm_unit_interfacing": 0.0},
    {"hash": "7da1931baef3717fb44c713c5288842f4c3bbae7", "msg": "Delete open_redirect.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:49:08+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:49:08+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["8717e3ad3f300f3011248a09d12c024129498f2a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 48, "insertions": 0, "lines": 48, "files": 1, "modified_files": [{"old_path": "open_redirect.py", "new_path": null, "filename": "open_redirect.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,48 +0,0 @@\n-#!/usr/bin/python\r\n-import requests,sys\r\n-\r\n-def start():\r\n-    input_file = sys.argv[1]\r\n-    output_file = sys.argv[2]\r\n-\r\n-    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n-\r\n-    is_closed = True\r\n-\r\n-    payloads = open(\"/home/YOUR_USER/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n-\r\n-    #First loop trough the payloads to prevent 429 (rate limit)\r\n-    for payload in payloads: \r\n-        domains = open(input_file,'r').read().split('\\n')   \r\n-        print \"\\n - Trying payload \"+payload+\" - \"\r\n-        for domain in domains:\r\n-            if domain != \"\":\r\n-\r\n-                url = \"https://\" + domain + payload\r\n-                url = url.strip()\r\n-            \r\n-                try:\r\n-                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n-                except:\r\n-                    print \"[-]Error on \" + url\r\n-\r\n-                if r.history:  \r\n-                    if r.url == \"https://example.com\":\r\n-                        print \"[+]\"+url\r\n-                        if is_closed:\r\n-                            file = open(output_file,\"w+\")\r\n-                        is_closed = False\r\n-                        file.write(url + \"\\n\")\r\n-                    else:\r\n-                        print \"[-]\"+url\r\n-                else:\r\n-                    print \"[-]\"+url\r\n-        else:\r\n-            print \"[-]Domain is invalid\"\r\n-\r\n-    if is_closed == False:\r\n-        file.close()\r\n-    print(\"\\n-- Done --\")\r\n-\r\n-start()\r\n-\r\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, "import requests,sys"], [3, ""], [4, "def start():"], [5, "    input_file = sys.argv[1]"], [6, "    output_file = sys.argv[2]"], [7, ""], [8, "    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")"], [9, ""], [10, "    is_closed = True"], [11, ""], [12, "    payloads = open(\"/home/YOUR_USER/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')"], [13, ""], [14, "    #First loop trough the payloads to prevent 429 (rate limit)"], [15, "    for payload in payloads:"], [16, "        domains = open(input_file,'r').read().split('\\n')"], [17, "        print \"\\n - Trying payload \"+payload+\" - \""], [18, "        for domain in domains:"], [19, "            if domain != \"\":"], [20, ""], [21, "                url = \"https://\" + domain + payload"], [22, "                url = url.strip()"], [23, ""], [24, "                try:"], [25, "                    r = requests.head(url, allow_redirects=True, timeout=5)"], [26, "                except:"], [27, "                    print \"[-]Error on \" + url"], [28, ""], [29, "                if r.history:"], [30, "                    if r.url == \"https://example.com\":"], [31, "                        print \"[+]\"+url"], [32, "                        if is_closed:"], [33, "                            file = open(output_file,\"w+\")"], [34, "                        is_closed = False"], [35, "                        file.write(url + \"\\n\")"], [36, "                    else:"], [37, "                        print \"[-]\"+url"], [38, "                else:"], [39, "                    print \"[-]\"+url"], [40, "        else:"], [41, "            print \"[-]Domain is invalid\""], [42, ""], [43, "    if is_closed == False:"], [44, "        file.close()"], [45, "    print(\"\\n-- Done --\")"], [46, ""], [47, "start()"], [48, ""]]}, "added_lines": 0, "deleted_lines": 48, "source_code": null, "source_code_before": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(\"/home/YOUR_USER/Documents/Wordlists/redirects.txt\",'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n\r\n                if r.history:  \r\n                    if r.url == \"https://example.com\":\r\n                        print \"[+]\"+url\r\n                        if is_closed:\r\n                            file = open(output_file,\"w+\")\r\n                        is_closed = False\r\n                        file.write(url + \"\\n\")\r\n                    else:\r\n                        print \"[-]\"+url\r\n                else:\r\n                    print \"[-]\"+url\r\n        else:\r\n            print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "methods": [], "methods_before": [{"name": "start", "start_line": 4, "end_line": 45}], "changed_methods": [{"name": "start", "start_line": 4, "end_line": 45}], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": 1.0, "dmm_unit_complexity": 1.0, "dmm_unit_interfacing": 0.0},
    {"hash": "1b3e7583bf5f685a57a97ba03688a610805d6fad", "msg": "Delete start_recon.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:49:14+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:49:14+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["7da1931baef3717fb44c713c5288842f4c3bbae7"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 34, "insertions": 0, "lines": 34, "files": 1, "modified_files": [{"old_path": "start_recon.sh", "new_path": null, "filename": "start_recon.sh", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,34 +0,0 @@\n-#!/bin/bash\n-    \n-all_domains_file=\"domains-all\"\n-domains_file=\"domains\"\n-crlf_file=\"crlf\"\n-open_redirects_file=\"redirects\"\n-nmap_scan_file=\"nmap_scans\"\n-screenshots_folder=\"screenshots\"\n-wordpress_file=\"wordpress_sites\"\n-headers_file=\"sensitive_headers\"\n-subdomain_take_over_file=\"sub_take_over\"\n-javascript_files_file=\"javascript_files\"\n-javascript_extracted_urls=\"extracted_urls\"\n-error_page_info_file=\"error_page_info\"\n-cors_file=\"misconfigured_cors\"\n-\n-rm -rf ~/Desktop/$@; \n-mkdir ~/Desktop/$@; \n-cd ~/Desktop/$@; \n-python ~/Documents/Tools/Sublist3r/sublist3r.py -o $all_domains_file -d $@;\n-python ~/Documents/Tools/online.py $all_domains_file $domains_file;\n-~/Documents/Tools/crlf.sh $domains_file $crlf_file;\n-~/Documents/Tools/cors_misconfiguration_scan.sh $domains_file $cors_file;\n-python ~/Documents/Tools/open_redirect.py $domains_file $open_redirects_file;\n-python ~/Documents/Tools/header_scan.py $domains_file $headers_file\n-python ~/Documents/Tools/error_page_info_check.py $domains_file $error_page_info_file;\n-python ~/Documents/Tools/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n-python ~/Documents/Tools/javascript_files_extractor.py $domains_file $javascript_files_file\n-~/Documents/Tools/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls\n-python ~/Documents/Tools/webscreenshot/webscreenshot.py -i $domains_file -o $screenshots_folder\n-python ~/Documents/Tools/wordpress_check.py $domains_file $wordpress_file\n-~/Documents/Tools/wpscan/wpscan.rb --update\n-~/Documents/Tools/wpscan_domains.sh $wordpress_file\n-~/Documents/Tools/nmap_scan.sh $domains_file $nmap_scan_file;\n\\ No newline at end of file\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/bin/bash"], [2, ""], [3, "all_domains_file=\"domains-all\""], [4, "domains_file=\"domains\""], [5, "crlf_file=\"crlf\""], [6, "open_redirects_file=\"redirects\""], [7, "nmap_scan_file=\"nmap_scans\""], [8, "screenshots_folder=\"screenshots\""], [9, "wordpress_file=\"wordpress_sites\""], [10, "headers_file=\"sensitive_headers\""], [11, "subdomain_take_over_file=\"sub_take_over\""], [12, "javascript_files_file=\"javascript_files\""], [13, "javascript_extracted_urls=\"extracted_urls\""], [14, "error_page_info_file=\"error_page_info\""], [15, "cors_file=\"misconfigured_cors\""], [16, ""], [17, "rm -rf ~/Desktop/$@;"], [18, "mkdir ~/Desktop/$@;"], [19, "cd ~/Desktop/$@;"], [20, "python ~/Documents/Tools/Sublist3r/sublist3r.py -o $all_domains_file -d $@;"], [21, "python ~/Documents/Tools/online.py $all_domains_file $domains_file;"], [22, "~/Documents/Tools/crlf.sh $domains_file $crlf_file;"], [23, "~/Documents/Tools/cors_misconfiguration_scan.sh $domains_file $cors_file;"], [24, "python ~/Documents/Tools/open_redirect.py $domains_file $open_redirects_file;"], [25, "python ~/Documents/Tools/header_scan.py $domains_file $headers_file"], [26, "python ~/Documents/Tools/error_page_info_check.py $domains_file $error_page_info_file;"], [27, "python ~/Documents/Tools/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;"], [28, "python ~/Documents/Tools/javascript_files_extractor.py $domains_file $javascript_files_file"], [29, "~/Documents/Tools/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls"], [30, "python ~/Documents/Tools/webscreenshot/webscreenshot.py -i $domains_file -o $screenshots_folder"], [31, "python ~/Documents/Tools/wordpress_check.py $domains_file $wordpress_file"], [32, "~/Documents/Tools/wpscan/wpscan.rb --update"], [33, "~/Documents/Tools/wpscan_domains.sh $wordpress_file"], [34, "~/Documents/Tools/nmap_scan.sh $domains_file $nmap_scan_file;"]]}, "added_lines": 0, "deleted_lines": 34, "source_code": null, "source_code_before": "#!/bin/bash\n    \nall_domains_file=\"domains-all\"\ndomains_file=\"domains\"\ncrlf_file=\"crlf\"\nopen_redirects_file=\"redirects\"\nnmap_scan_file=\"nmap_scans\"\nscreenshots_folder=\"screenshots\"\nwordpress_file=\"wordpress_sites\"\nheaders_file=\"sensitive_headers\"\nsubdomain_take_over_file=\"sub_take_over\"\njavascript_files_file=\"javascript_files\"\njavascript_extracted_urls=\"extracted_urls\"\nerror_page_info_file=\"error_page_info\"\ncors_file=\"misconfigured_cors\"\n\nrm -rf ~/Desktop/$@; \nmkdir ~/Desktop/$@; \ncd ~/Desktop/$@; \npython ~/Documents/Tools/Sublist3r/sublist3r.py -o $all_domains_file -d $@;\npython ~/Documents/Tools/online.py $all_domains_file $domains_file;\n~/Documents/Tools/crlf.sh $domains_file $crlf_file;\n~/Documents/Tools/cors_misconfiguration_scan.sh $domains_file $cors_file;\npython ~/Documents/Tools/open_redirect.py $domains_file $open_redirects_file;\npython ~/Documents/Tools/header_scan.py $domains_file $headers_file\npython ~/Documents/Tools/error_page_info_check.py $domains_file $error_page_info_file;\npython ~/Documents/Tools/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\npython ~/Documents/Tools/javascript_files_extractor.py $domains_file $javascript_files_file\n~/Documents/Tools/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls\npython ~/Documents/Tools/webscreenshot/webscreenshot.py -i $domains_file -o $screenshots_folder\npython ~/Documents/Tools/wordpress_check.py $domains_file $wordpress_file\n~/Documents/Tools/wpscan/wpscan.rb --update\n~/Documents/Tools/wpscan_domains.sh $wordpress_file\n~/Documents/Tools/nmap_scan.sh $domains_file $nmap_scan_file;", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "429d90406eff0aba8408aa784a67921802deea57", "msg": "Delete subdomain_takeover_scan.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:49:23+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:49:23+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["1b3e7583bf5f685a57a97ba03688a610805d6fad"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 117, "insertions": 0, "lines": 117, "files": 1, "modified_files": [{"old_path": "subdomain_takeover_scan.py", "new_path": null, "filename": "subdomain_takeover_scan.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,117 +0,0 @@\n-#!/usr/bin/python\n-\n-import requests, sys, dns.resolver\n-\n-input_file = sys.argv[1]\n-output_file = sys.argv[2]\n-is_closed = True\n-\n-domains = open(input_file,'r').read().split('\\n')\n-\n-take_over_cnames = [\"createsend\",\n-\"cargocollective\",\n-\"cloudfront\",\n-\"desk.com\",\n-\"fastly.net\",\n-\"feedpress.me\",\n-\"freshdesk.com\",\n-\"github.io\",\n-\"helpjuice.com\",\n-\"helpscoutdocs.com\",\n-\"herokudns.com\",\n-\"herokussl.com\",\n-\"herokuapp.com\",\n-\"pageserve.co\",\n-\"pingdom.com\",\n-\"amazonaws.com\",\n-\"myshopify.com\",\n-\"stspg-customer.com\",\n-\"sgizmo.com\",\n-\"surveygizmo.eu\",\n-\"sgizmoca.com\",\n-\"sgizmoca.com\",\n-\"tictail.com\",\n-\"domains.tumblr.com\",\n-\"uservoice.com\",\n-\"wpengine.com\",\n-\"squarespace.com\",\n-\"unbounce.com\",\n-\"zendesk.com\"]\n-\n-take_over_content = [\"<strong>Trying to access your account\",\n-\"Use a personal domain name\",\n-\"The request could not be satisfied\",\n-\"Sorry, We Couldn't Find That Page\",\n-\"Fastly error: unknown domain\",\n-\"The feed has not been found\",\n-\"You can claim it now at\",\n-\"Publishing platform\",                        \n-\"There isn't a GitHub Pages site here\",                       \n-\"No settings were found for this company\",\n-\"<title>No such app</title>\",                        \n-\"You've Discovered A Missing Link. Our Apologies!\",\n-\"Sorry, couldn&rsquo;t find the status page\",                        \n-\"NoSuchBucket\",\n-\"Sorry, this shop is currently unavailable\",\n-\"<title>Hosted Status Pages for Your Company</title>\",\n-\"data-html-name=\\\"Header Logo Link\\\"\",                        \n-\"<title>Oops - We didn't find your site.</title>\",\n-\"class=\\\"MarketplaceHeader__tictailLogo\\\"\",                        \n-\"Whatever you were looking for doesn't currently exist at this address\",\n-\"The requested URL was not found on this server\",\n-\"The page you have requested does not exist\",\n-\"This UserVoice subdomain is currently available!\",\n-\"but is not configured for an account on our platform\",\n-\"<title>Help Center Closed | Zendesk</title>\"]\n-\n-print(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n-\n-\n-for domain in domains:\n-\t#Skip first row, lol\n-\tif domain != domains[0]:\n-\t\tfound_content = False\n-\t\tfound_cname = False\n-\t\ttry:\n-\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text\n-\t\texcept:\n-\t\t\tprint(\"[-]Error in http://\"+domain)\n-\n-\t\tfor content in take_over_content:\n-\t\t\tif str(content) in r:\n-\t\t\t\tfound_content = True\n-\n-\t\ttry:\n-\t\t\tcnames = dns.resolver.query(domain, 'CNAME')\n-\t\t\tfor cname in cnames:\n-    \t\t\t\tfor cname_url in take_over_cnames:\n-\t\t\t\t\tif str(cname_url) in str(cname.target):\n-\t\t\t\t\t\tfound_cname = True\n-\n-\t\t\tif found_cname and found_content:\n-\t\t\t\tprint(\"[+]\"+domain)\n-\t\t\t\tif is_closed:\n-\t\t\t\t\tfile = open(output_file,\"w+\")\n-\t\t\t\t\tis_closed = False\n-\t\t\t\tfile.write(domain+\"\\n\")\n-\t\t\telse:\n-\t\t\t\tprint(\"[-]\"+domain)\n-\t\texcept:\n-\t\t\tprint \"[-]No cnames for \"+domain\n-\n-if is_closed == False:\n-\tfile.close()\n-\n-print(\"\\n-- Done --\")\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys, dns.resolver"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, ""], [11, "take_over_cnames = [\"createsend\","], [12, "\"cargocollective\","], [13, "\"cloudfront\","], [14, "\"desk.com\","], [15, "\"fastly.net\","], [16, "\"feedpress.me\","], [17, "\"freshdesk.com\","], [18, "\"github.io\","], [19, "\"helpjuice.com\","], [20, "\"helpscoutdocs.com\","], [21, "\"herokudns.com\","], [22, "\"herokussl.com\","], [23, "\"herokuapp.com\","], [24, "\"pageserve.co\","], [25, "\"pingdom.com\","], [26, "\"amazonaws.com\","], [27, "\"myshopify.com\","], [28, "\"stspg-customer.com\","], [29, "\"sgizmo.com\","], [30, "\"surveygizmo.eu\","], [31, "\"sgizmoca.com\","], [32, "\"sgizmoca.com\","], [33, "\"tictail.com\","], [34, "\"domains.tumblr.com\","], [35, "\"uservoice.com\","], [36, "\"wpengine.com\","], [37, "\"squarespace.com\","], [38, "\"unbounce.com\","], [39, "\"zendesk.com\"]"], [40, ""], [41, "take_over_content = [\"<strong>Trying to access your account\","], [42, "\"Use a personal domain name\","], [43, "\"The request could not be satisfied\","], [44, "\"Sorry, We Couldn't Find That Page\","], [45, "\"Fastly error: unknown domain\","], [46, "\"The feed has not been found\","], [47, "\"You can claim it now at\","], [48, "\"Publishing platform\","], [49, "\"There isn't a GitHub Pages site here\","], [50, "\"No settings were found for this company\","], [51, "\"<title>No such app</title>\","], [52, "\"You've Discovered A Missing Link. Our Apologies!\","], [53, "\"Sorry, couldn&rsquo;t find the status page\","], [54, "\"NoSuchBucket\","], [55, "\"Sorry, this shop is currently unavailable\","], [56, "\"<title>Hosted Status Pages for Your Company</title>\","], [57, "\"data-html-name=\\\"Header Logo Link\\\"\","], [58, "\"<title>Oops - We didn't find your site.</title>\","], [59, "\"class=\\\"MarketplaceHeader__tictailLogo\\\"\","], [60, "\"Whatever you were looking for doesn't currently exist at this address\","], [61, "\"The requested URL was not found on this server\","], [62, "\"The page you have requested does not exist\","], [63, "\"This UserVoice subdomain is currently available!\","], [64, "\"but is not configured for an account on our platform\","], [65, "\"<title>Help Center Closed | Zendesk</title>\"]"], [66, ""], [67, "print(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [68, ""], [69, ""], [70, "for domain in domains:"], [71, "\t#Skip first row, lol"], [72, "\tif domain != domains[0]:"], [73, "\t\tfound_content = False"], [74, "\t\tfound_cname = False"], [75, "\t\ttry:"], [76, "\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text"], [77, "\t\texcept:"], [78, "\t\t\tprint(\"[-]Error in http://\"+domain)"], [79, ""], [80, "\t\tfor content in take_over_content:"], [81, "\t\t\tif str(content) in r:"], [82, "\t\t\t\tfound_content = True"], [83, ""], [84, "\t\ttry:"], [85, "\t\t\tcnames = dns.resolver.query(domain, 'CNAME')"], [86, "\t\t\tfor cname in cnames:"], [87, "    \t\t\t\tfor cname_url in take_over_cnames:"], [88, "\t\t\t\t\tif str(cname_url) in str(cname.target):"], [89, "\t\t\t\t\t\tfound_cname = True"], [90, ""], [91, "\t\t\tif found_cname and found_content:"], [92, "\t\t\t\tprint(\"[+]\"+domain)"], [93, "\t\t\t\tif is_closed:"], [94, "\t\t\t\t\tfile = open(output_file,\"w+\")"], [95, "\t\t\t\t\tis_closed = False"], [96, "\t\t\t\tfile.write(domain+\"\\n\")"], [97, "\t\t\telse:"], [98, "\t\t\t\tprint(\"[-]\"+domain)"], [99, "\t\texcept:"], [100, "\t\t\tprint \"[-]No cnames for \"+domain"], [101, ""], [102, "if is_closed == False:"], [103, "\tfile.close()"], [104, ""], [105, "print(\"\\n-- Done --\")"], [106, ""], [107, ""], [108, ""], [109, ""], [110, ""], [111, ""], [112, ""], [113, ""], [114, ""], [115, ""], [116, ""], [117, ""]]}, "added_lines": 0, "deleted_lines": 117, "source_code": null, "source_code_before": "#!/usr/bin/python\n\nimport requests, sys, dns.resolver\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\n\ntake_over_cnames = [\"createsend\",\n\"cargocollective\",\n\"cloudfront\",\n\"desk.com\",\n\"fastly.net\",\n\"feedpress.me\",\n\"freshdesk.com\",\n\"github.io\",\n\"helpjuice.com\",\n\"helpscoutdocs.com\",\n\"herokudns.com\",\n\"herokussl.com\",\n\"herokuapp.com\",\n\"pageserve.co\",\n\"pingdom.com\",\n\"amazonaws.com\",\n\"myshopify.com\",\n\"stspg-customer.com\",\n\"sgizmo.com\",\n\"surveygizmo.eu\",\n\"sgizmoca.com\",\n\"sgizmoca.com\",\n\"tictail.com\",\n\"domains.tumblr.com\",\n\"uservoice.com\",\n\"wpengine.com\",\n\"squarespace.com\",\n\"unbounce.com\",\n\"zendesk.com\"]\n\ntake_over_content = [\"<strong>Trying to access your account\",\n\"Use a personal domain name\",\n\"The request could not be satisfied\",\n\"Sorry, We Couldn't Find That Page\",\n\"Fastly error: unknown domain\",\n\"The feed has not been found\",\n\"You can claim it now at\",\n\"Publishing platform\",                        \n\"There isn't a GitHub Pages site here\",                       \n\"No settings were found for this company\",\n\"<title>No such app</title>\",                        \n\"You've Discovered A Missing Link. Our Apologies!\",\n\"Sorry, couldn&rsquo;t find the status page\",                        \n\"NoSuchBucket\",\n\"Sorry, this shop is currently unavailable\",\n\"<title>Hosted Status Pages for Your Company</title>\",\n\"data-html-name=\\\"Header Logo Link\\\"\",                        \n\"<title>Oops - We didn't find your site.</title>\",\n\"class=\\\"MarketplaceHeader__tictailLogo\\\"\",                        \n\"Whatever you were looking for doesn't currently exist at this address\",\n\"The requested URL was not found on this server\",\n\"The page you have requested does not exist\",\n\"This UserVoice subdomain is currently available!\",\n\"but is not configured for an account on our platform\",\n\"<title>Help Center Closed | Zendesk</title>\"]\n\nprint(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\n\nfor domain in domains:\n\t#Skip first row, lol\n\tif domain != domains[0]:\n\t\tfound_content = False\n\t\tfound_cname = False\n\t\ttry:\n\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text\n\t\texcept:\n\t\t\tprint(\"[-]Error in http://\"+domain)\n\n\t\tfor content in take_over_content:\n\t\t\tif str(content) in r:\n\t\t\t\tfound_content = True\n\n\t\ttry:\n\t\t\tcnames = dns.resolver.query(domain, 'CNAME')\n\t\t\tfor cname in cnames:\n    \t\t\t\tfor cname_url in take_over_cnames:\n\t\t\t\t\tif str(cname_url) in str(cname.target):\n\t\t\t\t\t\tfound_cname = True\n\n\t\t\tif found_cname and found_content:\n\t\t\t\tprint(\"[+]\"+domain)\n\t\t\t\tif is_closed:\n\t\t\t\t\tfile = open(output_file,\"w+\")\n\t\t\t\t\tis_closed = False\n\t\t\t\tfile.write(domain+\"\\n\")\n\t\t\telse:\n\t\t\t\tprint(\"[-]\"+domain)\n\t\texcept:\n\t\t\tprint \"[-]No cnames for \"+domain\n\nif is_closed == False:\n\tfile.close()\n\nprint(\"\\n-- Done --\")\n\n\n\n\n\n\n\n\n\n\n\n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "c0bcf0d3e78fb95c82cb8638dfe2cf8fddbf2234", "msg": "Delete wordpress_check.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:49:29+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:49:29+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["429d90406eff0aba8408aa784a67921802deea57"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 34, "insertions": 0, "lines": 34, "files": 1, "modified_files": [{"old_path": "wordpress_check.py", "new_path": null, "filename": "wordpress_check.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,34 +0,0 @@\n-#!/usr/bin/python\n-\n-import requests, sys\n-\n-input_file = sys.argv[1]\n-output_file = sys.argv[2]\n-is_closed = True\n-\n-domains = open(input_file,'r').read().split('\\n')\n-\n-print(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n-\n-for domain in domains:\n-    if domain != \"\":\n-        try:\n-            response = requests.get(\"https://\"+domain)\n-        except:\n-            print(\"[-]Error on https://\"+domain)\n-\n-        if \"/wp-content/\" in response.content:\n-            if is_closed:\n-                file = open(output_file,\"w+\")\n-            is_closed = False\n-            print(\"[+]\"+domain)\n-            file.write(domain + \"\\n\")\n-        else:\n-            print(\"[-]\"+domain)\n-    else:\n-        print(\"[-]Domain is invalid\")\n-        \n-if is_closed == False:\n-    file.close()\n-\n-print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, ""], [11, "print(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [12, ""], [13, "for domain in domains:"], [14, "    if domain != \"\":"], [15, "        try:"], [16, "            response = requests.get(\"https://\"+domain)"], [17, "        except:"], [18, "            print(\"[-]Error on https://\"+domain)"], [19, ""], [20, "        if \"/wp-content/\" in response.content:"], [21, "            if is_closed:"], [22, "                file = open(output_file,\"w+\")"], [23, "            is_closed = False"], [24, "            print(\"[+]\"+domain)"], [25, "            file.write(domain + \"\\n\")"], [26, "        else:"], [27, "            print(\"[-]\"+domain)"], [28, "    else:"], [29, "        print(\"[-]Domain is invalid\")"], [30, ""], [31, "if is_closed == False:"], [32, "    file.close()"], [33, ""], [34, "print(\"\\n-- Done --\")"]]}, "added_lines": 0, "deleted_lines": 34, "source_code": null, "source_code_before": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\n\nprint(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    if domain != \"\":\n        try:\n            response = requests.get(\"https://\"+domain)\n        except:\n            print(\"[-]Error on https://\"+domain)\n\n        if \"/wp-content/\" in response.content:\n            if is_closed:\n                file = open(output_file,\"w+\")\n            is_closed = False\n            print(\"[+]\"+domain)\n            file.write(domain + \"\\n\")\n        else:\n            print(\"[-]\"+domain)\n    else:\n        print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "05fe11025eec60ba0d773c9ae18cfeb6eab106f2", "msg": "Delete wpscan_domains.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:49:35+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:49:35+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["c0bcf0d3e78fb95c82cb8638dfe2cf8fddbf2234"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 13, "insertions": 0, "lines": 13, "files": 1, "modified_files": [{"old_path": "wpscan_domains.sh", "new_path": null, "filename": "wpscan_domains.sh", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,13 +0,0 @@\n-#!/bin/bash\n-\n-printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n-\n-if [ ! -f $1 ]; then\n-    echo \"[-]File not found!\"\n-else\n-    while read domain; do \n-        echo \"[+]Opening $domain\"\n-        xterm -hold -e \"/home/YOUR_USER/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n-    done < $1\n-fi\n-printf \"\\n -- Done -- \\n\"\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    echo \"[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        echo \"[+]Opening $domain\""], [10, "        xterm -hold -e \"/home/YOUR_USER/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &"], [11, "    done < $1"], [12, "fi"], [13, "printf \"\\n -- Done -- \\n\""]]}, "added_lines": 0, "deleted_lines": 13, "source_code": null, "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"/home/YOUR_USER/Documents/Tools/wpscan/wpscan.rb --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "390869eb3918243933864cec4b93fb35f5f45aaf", "msg": "Init", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 21:52:56+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 21:52:56+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["05fe11025eec60ba0d773c9ae18cfeb6eab106f2"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 596, "lines": 596, "files": 17, "modified_files": [{"old_path": null, "new_path": "payloads/crlf.txt", "filename": "crlf.txt", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,9 @@\n+/%0aSet-Cookie:crlf=injection\n+/%250aSet-Cookie:crlf=injection\n+/%25250aSet-Cookie:crlf=injection\n+/%%0a0aSet-Cookie:crlf=injection\n+/%3f%0dSet-Cookie:crlf=injection\n+/%23%0dSet-Cookie:crlf=injection\n+/%25%30aSet-Cookie:crlf=injection\n+/%25%30%61Set-Cookie:crlf=injection\n+/%u000aSet-Cookie:crlf=injection\n", "diff_parsed": {"added": [[1, "/%0aSet-Cookie:crlf=injection"], [2, "/%250aSet-Cookie:crlf=injection"], [3, "/%25250aSet-Cookie:crlf=injection"], [4, "/%%0a0aSet-Cookie:crlf=injection"], [5, "/%3f%0dSet-Cookie:crlf=injection"], [6, "/%23%0dSet-Cookie:crlf=injection"], [7, "/%25%30aSet-Cookie:crlf=injection"], [8, "/%25%30%61Set-Cookie:crlf=injection"], [9, "/%u000aSet-Cookie:crlf=injection"]], "deleted": []}, "added_lines": 9, "deleted_lines": 0, "source_code": "/%0aSet-Cookie:crlf=injection\n/%250aSet-Cookie:crlf=injection\n/%25250aSet-Cookie:crlf=injection\n/%%0a0aSet-Cookie:crlf=injection\n/%3f%0dSet-Cookie:crlf=injection\n/%23%0dSet-Cookie:crlf=injection\n/%25%30aSet-Cookie:crlf=injection\n/%25%30%61Set-Cookie:crlf=injection\n/%u000aSet-Cookie:crlf=injection\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "payloads/error_pages.txt", "filename": "error_pages.txt", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,6 @@\n+Apache/\n+Version Information:\n+Tomcat/\n+This is the default welcome page\n+PHP Version\n+This page is used to test the proper operation of the\n", "diff_parsed": {"added": [[1, "Apache/"], [2, "Version Information:"], [3, "Tomcat/"], [4, "This is the default welcome page"], [5, "PHP Version"], [6, "This page is used to test the proper operation of the"]], "deleted": []}, "added_lines": 6, "deleted_lines": 0, "source_code": "Apache/\nVersion Information:\nTomcat/\nThis is the default welcome page\nPHP Version\nThis page is used to test the proper operation of the\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "payloads/open_redirects.txt", "filename": "open_redirects.txt", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,22 @@\n+/https://example.com\n+/%5cexample.com\n+/%2f%2fexample.com\n+/example.com/%2f%2e%2e\n+/https:/example.com\n+/?url=http://example.com&next=http://example.com&redirect=http://example.com&redir=http://example.com&rurl=http://example.com\n+/?url=//example.com&next=//example.com&redirect=//example.com&redir=//example.com&rurl=//example.com\n+/?url=https://example.com&next=https://example.com&redirect=/\\/example.com\n+/redirect?url=http://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n+/redirect?url=//example.com&next=//example.com&redirect=//example.com&redir=//example.com&rurl=//example.com\n+/redirect?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n+/redirect.html?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n+/redirect.php?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n+/redirect.aspx?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n+/.example.com\n+///\\;@example.com\n+///example.com/\n+///example.com\n+///example.com/%2f..\n+/////example.com/\n+/%0a.example.com/\n+/\\/example.com\n", "diff_parsed": {"added": [[1, "/https://example.com"], [2, "/%5cexample.com"], [3, "/%2f%2fexample.com"], [4, "/example.com/%2f%2e%2e"], [5, "/https:/example.com"], [6, "/?url=http://example.com&next=http://example.com&redirect=http://example.com&redir=http://example.com&rurl=http://example.com"], [7, "/?url=//example.com&next=//example.com&redirect=//example.com&redir=//example.com&rurl=//example.com"], [8, "/?url=https://example.com&next=https://example.com&redirect=/\\/example.com"], [9, "/redirect?url=http://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com"], [10, "/redirect?url=//example.com&next=//example.com&redirect=//example.com&redir=//example.com&rurl=//example.com"], [11, "/redirect?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com"], [12, "/redirect.html?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com"], [13, "/redirect.php?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com"], [14, "/redirect.aspx?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com"], [15, "/.example.com"], [16, "///\\;@example.com"], [17, "///example.com/"], [18, "///example.com"], [19, "///example.com/%2f.."], [20, "/////example.com/"], [21, "/%0a.example.com/"], [22, "/\\/example.com"]], "deleted": []}, "added_lines": 22, "deleted_lines": 0, "source_code": "/https://example.com\n/%5cexample.com\n/%2f%2fexample.com\n/example.com/%2f%2e%2e\n/https:/example.com\n/?url=http://example.com&next=http://example.com&redirect=http://example.com&redir=http://example.com&rurl=http://example.com\n/?url=//example.com&next=//example.com&redirect=//example.com&redir=//example.com&rurl=//example.com\n/?url=https://example.com&next=https://example.com&redirect=/\\/example.com\n/redirect?url=http://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n/redirect?url=//example.com&next=//example.com&redirect=//example.com&redir=//example.com&rurl=//example.com\n/redirect?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n/redirect.html?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n/redirect.php?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n/redirect.aspx?url=https://example.com&next=https://example.com&redirect=https://example.com&redir=https://example.com&rurl=https://example.com\n/.example.com\n///\\;@example.com\n///example.com/\n///example.com\n///example.com/%2f..\n/////example.com/\n/%0a.example.com/\n/\\/example.com\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "payloads/sensitive_headers.txt", "filename": "sensitive_headers.txt", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,3 @@\n+X-Powered-By\n+Server\n+X-AspNet-Version\n", "diff_parsed": {"added": [[1, "X-Powered-By"], [2, "Server"], [3, "X-AspNet-Version"]], "deleted": []}, "added_lines": 3, "deleted_lines": 0, "source_code": "X-Powered-By\nServer\nX-AspNet-Version\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,56 @@\n+    home_dir=\"/home/rjp/Desktop/recon\"\n+\n+    output_dir=\"output\"\n+    tools_dir=\"tools\"\n+    payloads_dir=\"payloads\"\n+    dependencies_dir=\"dependencies\"  \n+    screenshots_dir=\"$output_dir/$@/screenshots\"  \n+\n+    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n+    domains_file=\"$output_dir/$@/domains.txt\"\n+    crlf_file=\"$output_dir/$@/crlf.txt\"\n+    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n+    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n+    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n+    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n+    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n+    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n+    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n+    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n+    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n+\n+    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n+    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n+    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n+    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n+\n+    \n+    wpscan_location=\"dependencies/wpscan\"\n+    url_extractor_location=\"dependencies/relative-url-extractor\"\n+    sublister_location=\"dependencies/Sublist3r\"\n+    webscreenshot_location=\"dependencies/webscreenshot\"\n+\n+    cd $home_dir/$output_dir;\n+    rm -rf $@; \n+    mkdir $@; \n+    cd ../\n+\n+    printf \"\\n -- $@ Started -- \\n\"\n+\n+    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n+    python $tools_dir/online.py $all_domains_file $domains_file;\n+    #$tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n+    #$tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n+    #python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n+    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n+    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n+    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n+    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n+    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n+    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n+    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n+    $wpscan_location/wpscan.rb --update;\n+    $tools_dir/wpscan_domains.sh $wordpress_file;\n+    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n+    \n+    printf \"\\n -- $@ Finished -- \\n\"\n", "diff_parsed": {"added": [[1, "    home_dir=\"/home/rjp/Desktop/recon\""], [2, ""], [3, "    output_dir=\"output\""], [4, "    tools_dir=\"tools\""], [5, "    payloads_dir=\"payloads\""], [6, "    dependencies_dir=\"dependencies\""], [7, "    screenshots_dir=\"$output_dir/$@/screenshots\""], [8, ""], [9, "    all_domains_file=\"$output_dir/$@/domains-all.txt\""], [10, "    domains_file=\"$output_dir/$@/domains.txt\""], [11, "    crlf_file=\"$output_dir/$@/crlf.txt\""], [12, "    open_redirects_file=\"$output_dir/$@/open_redirects.txt\""], [13, "    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\""], [14, "    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\""], [15, "    headers_file=\"$output_dir/$@/sensitive_headers.txt\""], [16, "    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\""], [17, "    javascript_files_file=\"$output_dir/$@/javascript_files.txt\""], [18, "    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\""], [19, "    error_page_info_file=\"$output_dir/$@/error_page_info.txt\""], [20, "    cors_file=\"$output_dir/$@/misconfigured_cors.txt\""], [21, ""], [22, "    crlf_payload_file=\"$payloads_dir/crlf.txt\""], [23, "    error_pages_payload_file=\"$payloads_dir/error_pages.txt\""], [24, "    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\""], [25, "    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\""], [26, ""], [27, ""], [28, "    wpscan_location=\"dependencies/wpscan\""], [29, "    url_extractor_location=\"dependencies/relative-url-extractor\""], [30, "    sublister_location=\"dependencies/Sublist3r\""], [31, "    webscreenshot_location=\"dependencies/webscreenshot\""], [32, ""], [33, "    cd $home_dir/$output_dir;"], [34, "    rm -rf $@;"], [35, "    mkdir $@;"], [36, "    cd ../"], [37, ""], [38, "    printf \"\\n -- $@ Started -- \\n\""], [39, ""], [40, "    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;"], [41, "    python $tools_dir/online.py $all_domains_file $domains_file;"], [42, "    #$tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;"], [43, "    #$tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;"], [44, "    #python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;"], [45, "    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;"], [46, "    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;"], [47, "    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;"], [48, "    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;"], [49, "    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;"], [50, "    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;"], [51, "    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;"], [52, "    $wpscan_location/wpscan.rb --update;"], [53, "    $tools_dir/wpscan_domains.sh $wordpress_file;"], [54, "    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;"], [55, ""], [56, "    printf \"\\n -- $@ Finished -- \\n\""]], "deleted": []}, "added_lines": 56, "deleted_lines": 0, "source_code": "    home_dir=\"/home/rjp/Desktop/recon\"\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/Sublist3r\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    #$tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    #$tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    #python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "tools/cors_misconfiguration_scan.sh", "filename": "cors_misconfiguration_scan.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,25 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n+\n+if [ ! -f $1 ]; then\n+    echo \"[-]File not found!\"\n+else\n+    while read domain; do\n+        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n+            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n+                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n+                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n+\n+                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n+                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n+                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n+                fi\n+            else\n+                echo \"[-]$domain [$origin]\"\n+            fi\n+        done\n+    done < $1 \n+fi\n+\n+printf \"\\n -- Done -- \\n\"\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    echo \"[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do"], [10, "            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then"], [11, "                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\""], [12, "                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2"], [13, ""], [14, "                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then"], [15, "                    echo \"[+](Allow-Credentials) $domain [$origin]\""], [16, "                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2"], [17, "                fi"], [18, "            else"], [19, "                echo \"[-]$domain [$origin]\""], [20, "            fi"], [21, "        done"], [22, "    done < $1"], [23, "fi"], [24, ""], [25, "printf \"\\n -- Done -- \\n\""]], "deleted": []}, "added_lines": 25, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Scanning for misconfigured cors headers in $1 with output file, $2 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do\n        for origin in https://evil.com null https://$domain.evil.com https://${domain}evil.com https://evil${domain}; do\n            if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i \"< Access-Control-Allow-Origin: $origin\" &> /dev/null; then\n                echo \"[+](Access-Control-Allow-Origin) $domain [$origin]\"\n                echo \"(Access-Control-Allow-Origin) $domain [$origin]\" >> $2\n\n                if curl -vs \"$domain\" -H\"Origin: $origin\" 2>&1 | grep -i '< Access-Control-Allow-Credentials: true' &> /dev/null; then\n                    echo \"[+](Allow-Credentials) $domain [$origin]\"\n                    echo \"(Allow-Credentials) $domain [$origin]\" >> $2\n                fi\n            else\n                echo \"[-]$domain [$origin]\"\n            fi\n        done\n    done < $1 \nfi\n\nprintf \"\\n -- Done -- \\n\"\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "tools/crlf.sh", "filename": "crlf.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,19 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\"\n+\n+#First loop trough the payloads to prevent 429 (rate limit)\n+while read payload; do \n+    while read domain; do \n+        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then \n+            echo \"[+]$domain/$payload\"\n+            echo \"$domain/$payload\" >> $2\n+        else\n+            echo \"[-]$domain/$payload\"\n+        fi\n+    done < $1\n+done < $3 \n+\n+printf \"\\n-- Done --\"\n+#Credits to Tomnomnom for the help\n+\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\""], [4, ""], [5, "#First loop trough the payloads to prevent 429 (rate limit)"], [6, "while read payload; do"], [7, "    while read domain; do"], [8, "        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then"], [9, "            echo \"[+]$domain/$payload\""], [10, "            echo \"$domain/$payload\" >> $2"], [11, "        else"], [12, "            echo \"[-]$domain/$payload\""], [13, "        fi"], [14, "    done < $1"], [15, "done < $3"], [16, ""], [17, "printf \"\\n-- Done --\""], [18, "#Credits to Tomnomnom for the help"], [19, ""]], "deleted": []}, "added_lines": 19, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Testing crlf on domains in $1 with output file, $2 -- \\n\\n\"\n\n#First loop trough the payloads to prevent 429 (rate limit)\nwhile read payload; do \n    while read domain; do \n        if curl -vs \"$domain/$payload\" 2>&1 | grep -i '^< Set-Cookie: crlf' &> /dev/null; then \n            echo \"[+]$domain/$payload\"\n            echo \"$domain/$payload\" >> $2\n        else\n            echo \"[-]$domain/$payload\"\n        fi\n    done < $1\ndone < $3 \n\nprintf \"\\n-- Done --\"\n#Credits to Tomnomnom for the help\n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "tools/error_page_info_check.py", "filename": "error_page_info_check.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,51 @@\n+#!/usr/bin/python\n+\n+import requests, sys\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+payload_file = sys.argv[3]\n+is_closed = True\n+\n+domains = open(input_file,'r').read().split('\\n')\n+info = [line.rstrip('\\n').lower() for line in open(payload_file)]\n+\n+print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n+\n+payloads = [\"/\",\n+            \"/NotFound123\",\n+            \"/.htaccess\",\n+            \"/<>\"]\n+\n+for payload in payloads:\n+    print \"\\n - Trying payload \"+payload+\" - \"\n+    for domain in domains:\n+        info_found = \"\"\n+        if domain != \"\":\n+            found = False\n+            try:\n+                response = requests.get(\"http://\"+domain+payload)\n+            except:\n+                print(\"[-]Error on http://\"+domain+payload)\n+            for i in info:\n+                if i.lower() in response.content.lower():\n+                    found = True\n+                    #Search and get the line in the response that contains i.lower()\n+                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n+\n+            if found:\n+                if is_closed:\n+                    file = open(output_file,\"w+\")\n+                is_closed = False\n+                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n+                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n+            else:\n+               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n+                \n+        else:\n+            print(\"[-]Domain is invalid\")\n+        \n+if is_closed == False:\n+    file.close()\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "payload_file = sys.argv[3]"], [8, "is_closed = True"], [9, ""], [10, "domains = open(input_file,'r').read().split('\\n')"], [11, "info = [line.rstrip('\\n').lower() for line in open(payload_file)]"], [12, ""], [13, "print(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")"], [14, ""], [15, "payloads = [\"/\","], [16, "            \"/NotFound123\","], [17, "            \"/.htaccess\","], [18, "            \"/<>\"]"], [19, ""], [20, "for payload in payloads:"], [21, "    print \"\\n - Trying payload \"+payload+\" - \""], [22, "    for domain in domains:"], [23, "        info_found = \"\""], [24, "        if domain != \"\":"], [25, "            found = False"], [26, "            try:"], [27, "                response = requests.get(\"http://\"+domain+payload)"], [28, "            except:"], [29, "                print(\"[-]Error on http://\"+domain+payload)"], [30, "            for i in info:"], [31, "                if i.lower() in response.content.lower():"], [32, "                    found = True"], [33, "                    #Search and get the line in the response that contains i.lower()"], [34, "                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]"], [35, ""], [36, "            if found:"], [37, "                if is_closed:"], [38, "                    file = open(output_file,\"w+\")"], [39, "                is_closed = False"], [40, "                print(\"[+]\"+domain+payload+\" - \"+str(info_found))"], [41, "                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")"], [42, "            else:"], [43, "               print(\"[-]\"+domain+payload+\" - \"+str(info_found))"], [44, ""], [45, "        else:"], [46, "            print(\"[-]Domain is invalid\")"], [47, ""], [48, "if is_closed == False:"], [49, "    file.close()"], [50, ""], [51, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 51, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\npayload_file = sys.argv[3]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\ninfo = [line.rstrip('\\n').lower() for line in open(payload_file)]\n\nprint(\"\\n-- Checking for sensitive info in error pages \"+input_file+\" with output file, \"+output_file+\" --\")\n\npayloads = [\"/\",\n            \"/NotFound123\",\n            \"/.htaccess\",\n            \"/<>\"]\n\nfor payload in payloads:\n    print \"\\n - Trying payload \"+payload+\" - \"\n    for domain in domains:\n        info_found = \"\"\n        if domain != \"\":\n            found = False\n            try:\n                response = requests.get(\"http://\"+domain+payload)\n            except:\n                print(\"[-]Error on http://\"+domain+payload)\n            for i in info:\n                if i.lower() in response.content.lower():\n                    found = True\n                    #Search and get the line in the response that contains i.lower()\n                    info_found = [x for x in [x.lower() for x in response.content.split(\"\\n\")] if i.lower() in x]\n\n            if found:\n                if is_closed:\n                    file = open(output_file,\"w+\")\n                is_closed = False\n                print(\"[+]\"+domain+payload+\" - \"+str(info_found))\n                file.write(domain + payload +\" - \"+str(info_found)+ \"\\n\")\n            else:\n               print(\"[-]\"+domain+payload+\" - \"+str(info_found))\n                \n        else:\n            print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 39, "complexity": 0, "token_count": 286}, {"old_path": null, "new_path": "tools/header_scan.py", "filename": "header_scan.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,39 @@\n+import requests, sys\r\n+\r\n+input_file = sys.argv[1]\r\n+output_file = sys.argv[2]\r\n+payload_file = sys.argv[3]\r\n+\r\n+print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n+\r\n+is_closed = True\r\n+domains = open(input_file,'r').read().split('\\n')\r\n+headers = [line.rstrip('\\n').lower() for line in open(payload_file)]\r\n+\r\n+for domain in domains:\r\n+\tif domain != \"\":\r\n+\t\ttry:\r\n+\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n+\t\texcept:\r\n+\t\t\tprint(\"[-]Error on https://\"+domain)\r\n+\t\theaders_found = []\r\n+\r\n+\t\tfor header in headers:\r\n+\t\t\tcurrent_header = r.headers.get(header.lower())\r\n+\t\t\tif current_header != None:\r\n+\t\t\t\theaders_found.append(str(current_header))\r\n+\t\tif headers_found != []:\r\n+\t\t\tif is_closed:\r\n+        \t\t\tfile = open(output_file,\"w+\")\r\n+\t\t\t\tis_closed = False\r\n+\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n+\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n+\t\telse:\r\n+\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n+\telse:\r\n+            print \"[-]Domain is invalid\"\r\n+\r\n+if is_closed == False:\r\n+\tfile.close()\r\n+\r\n+print(\"\\n-- Done --\")\r\n", "diff_parsed": {"added": [[1, "import requests, sys"], [2, ""], [3, "input_file = sys.argv[1]"], [4, "output_file = sys.argv[2]"], [5, "payload_file = sys.argv[3]"], [6, ""], [7, "print(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [8, ""], [9, "is_closed = True"], [10, "domains = open(input_file,'r').read().split('\\n')"], [11, "headers = [line.rstrip('\\n').lower() for line in open(payload_file)]"], [12, ""], [13, "for domain in domains:"], [14, "\tif domain != \"\":"], [15, "\t\ttry:"], [16, "\t\t\tr = requests.head(\"https://\"+domain, timeout=5)"], [17, "\t\texcept:"], [18, "\t\t\tprint(\"[-]Error on https://\"+domain)"], [19, "\t\theaders_found = []"], [20, ""], [21, "\t\tfor header in headers:"], [22, "\t\t\tcurrent_header = r.headers.get(header.lower())"], [23, "\t\t\tif current_header != None:"], [24, "\t\t\t\theaders_found.append(str(current_header))"], [25, "\t\tif headers_found != []:"], [26, "\t\t\tif is_closed:"], [27, "        \t\t\tfile = open(output_file,\"w+\")"], [28, "\t\t\t\tis_closed = False"], [29, "\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))"], [30, "\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")"], [31, "\t\telse:"], [32, "\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))"], [33, "\telse:"], [34, "            print \"[-]Domain is invalid\""], [35, ""], [36, "if is_closed == False:"], [37, "\tfile.close()"], [38, ""], [39, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 39, "deleted_lines": 0, "source_code": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\npayload_file = sys.argv[3]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open(payload_file)]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None:\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 32, "complexity": 0, "token_count": 233}, {"old_path": null, "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,72 @@\n+#!/usr/bin/python\n+\n+import re, requests, sys\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+\n+print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n+\n+domains = open(input_file,'r').read().split('\\n')\n+\n+file = open(output_file,\"w+\")\n+\n+black_listed_domains = [\"ajax.googleapis.com\",\n+                        \"cdn.optimizely.com\",\n+                        \"googletagmanager.com\",\n+                        \"fontawesome.com\"]\n+\n+for domain in domains:\n+\tdomain_written = False\n+\ti = 0\n+\tb_amount = 0\n+\tfull_domain = \"\"\n+\tif domain != \"\":\n+\t\tmatches = \"\"\n+\t\tr = \"\"\n+\t\tregex = r'script src=\"(.*?)\"'\n+\t\ttry:\n+\t\t\tr = requests.get(\"http://\"+domain).content\n+\t\texcept:\n+\t\t\tprint \"[-]Error in http://\"+domain\n+\n+\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\t\tif matches == []:\n+\t\t\tregex = r\"script src='(.*?)'\"\n+\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\n+\t\tfor m in matches:\n+\t\t\tif domain_written != True:\n+\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n+\t\t\t\tdomain_written = True\n+\t\n+\t\t\tblack_listed = False\n+\t\t\tfor b in black_listed_domains:\n+\t\t\t\tif b in m:\n+\t\t\t\t\tblack_listed = True\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\tif m.startswith(\"/\"):\n+\t\t\t\t\tif m.startswith(\"//\"):\n+\t\t\t\t\t\tfull_domain = \"https:\"+m\n+\t\t\t\t\telse:\n+\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n+\t\t\t\telif m.startswith(\"http\"):\n+\t\t\t\t\tfull_domain = m\n+\t\t\t\telse:\n+\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n+\t\t\telse:\n+\t\t\t\tb_amount += 1\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\ti += 1\n+\t\t\t\tfile.write(full_domain+\"\\n\")\n+\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n+\telse:\n+\t\tprint \"[-]Domain is invalid \" + domain\n+\n+\n+\n+file.close()\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import re, requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, ""], [8, "print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [9, ""], [10, "domains = open(input_file,'r').read().split('\\n')"], [11, ""], [12, "file = open(output_file,\"w+\")"], [13, ""], [14, "black_listed_domains = [\"ajax.googleapis.com\","], [15, "                        \"cdn.optimizely.com\","], [16, "                        \"googletagmanager.com\","], [17, "                        \"fontawesome.com\"]"], [18, ""], [19, "for domain in domains:"], [20, "\tdomain_written = False"], [21, "\ti = 0"], [22, "\tb_amount = 0"], [23, "\tfull_domain = \"\""], [24, "\tif domain != \"\":"], [25, "\t\tmatches = \"\""], [26, "\t\tr = \"\""], [27, "\t\tregex = r'script src=\"(.*?)\"'"], [28, "\t\ttry:"], [29, "\t\t\tr = requests.get(\"http://\"+domain).content"], [30, "\t\texcept:"], [31, "\t\t\tprint \"[-]Error in http://\"+domain"], [32, ""], [33, "\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [34, "\t\tif matches == []:"], [35, "\t\t\tregex = r\"script src='(.*?)'\""], [36, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [37, ""], [38, "\t\tfor m in matches:"], [39, "\t\t\tif domain_written != True:"], [40, "\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")"], [41, "\t\t\t\tdomain_written = True"], [42, ""], [43, "\t\t\tblack_listed = False"], [44, "\t\t\tfor b in black_listed_domains:"], [45, "\t\t\t\tif b in m:"], [46, "\t\t\t\t\tblack_listed = True"], [47, ""], [48, "\t\t\tif black_listed != True:"], [49, "\t\t\t\tif m.startswith(\"/\"):"], [50, "\t\t\t\t\tif m.startswith(\"//\"):"], [51, "\t\t\t\t\t\tfull_domain = \"https:\"+m"], [52, "\t\t\t\t\telse:"], [53, "\t\t\t\t\t\tfull_domain = \"https://\"+domain+m"], [54, "\t\t\t\telif m.startswith(\"http\"):"], [55, "\t\t\t\t\tfull_domain = m"], [56, "\t\t\t\telse:"], [57, "\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m"], [58, "\t\t\telse:"], [59, "\t\t\t\tb_amount += 1"], [60, ""], [61, "\t\t\tif black_listed != True:"], [62, "\t\t\t\ti += 1"], [63, "\t\t\t\tfile.write(full_domain+\"\\n\")"], [64, "\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain"], [65, "\telse:"], [66, "\t\tprint \"[-]Domain is invalid \" + domain"], [67, ""], [68, ""], [69, ""], [70, "file.close()"], [71, ""], [72, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 72, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains = open(input_file,'r').read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 55, "complexity": 0, "token_count": 310}, {"old_path": null, "new_path": "tools/javascript_files_link_extractor.sh", "filename": "javascript_files_link_extractor.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,23 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n+\n+if [ ! -f $1 ]; then\n+    printf  \"\\n[-]File not found!\"\n+else\n+    while read domain; do \n+        if [[ $domain == \"-\"* ]]; then\n+            printf  \"\\n\\n\\n[+]-$domain--\"\n+        else\n+            if [ -z \"$domain\" ]; then\n+                printf \"\\n[-]Invalid domain $domain\"\n+            else\n+                printf \"\\n[+]$domain \\n\"\n+\t\techo \"----------------------\"\n+                command=\"ruby $3 $domain\"\n+                eval $command\n+            fi\n+        fi\n+    done < $1 >> $2\n+printf \"\\n -- Done -- \\n\"\n+fi\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    printf  \"\\n[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        if [[ $domain == \"-\"* ]]; then"], [10, "            printf  \"\\n\\n\\n[+]-$domain--\""], [11, "        else"], [12, "            if [ -z \"$domain\" ]; then"], [13, "                printf \"\\n[-]Invalid domain $domain\""], [14, "            else"], [15, "                printf \"\\n[+]$domain \\n\""], [16, "\t\techo \"----------------------\""], [17, "                command=\"ruby $3 $domain\""], [18, "                eval $command"], [19, "            fi"], [20, "        fi"], [21, "    done < $1 >> $2"], [22, "printf \"\\n -- Done -- \\n\""], [23, "fi"]], "deleted": []}, "added_lines": 23, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n\nif [ ! -f $1 ]; then\n    printf  \"\\n[-]File not found!\"\nelse\n    while read domain; do \n        if [[ $domain == \"-\"* ]]; then\n            printf  \"\\n\\n\\n[+]-$domain--\"\n        else\n            if [ -z \"$domain\" ]; then\n                printf \"\\n[-]Invalid domain $domain\"\n            else\n                printf \"\\n[+]$domain \\n\"\n\t\techo \"----------------------\"\n                command=\"ruby $3 $domain\"\n                eval $command\n            fi\n        fi\n    done < $1 >> $2\nprintf \"\\n -- Done -- \\n\"\nfi\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "tools/nmap_scan.sh", "filename": "nmap_scan.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,13 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\n+echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n+\n+while read domain; do \n+    echo \"-- $domain --\"\n+    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\n+done < $1  >> $2\n+\n+echo \"-- Done --\"\n+\n+\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\""], [4, "echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\""], [5, ""], [6, "while read domain; do"], [7, "    echo \"-- $domain --\""], [8, "    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'"], [9, "done < $1  >> $2"], [10, ""], [11, "echo \"-- Done --\""], [12, ""], [13, ""]], "deleted": []}, "added_lines": 13, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\necho \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n\nwhile read domain; do \n    echo \"-- $domain --\"\n    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\ndone < $1  >> $2\n\necho \"-- Done --\"\n\n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": null, "new_path": "tools/online.py", "filename": "online.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,45 @@\n+#!/usr/bin/python\n+\n+import httplib\n+import socket\n+import re\n+import sys\n+\n+def online(host):\n+    try:\n+        socket.gethostbyname(host)\n+    except socket.gaierror:\n+        return False\n+    else:\n+        return True\n+\n+def available(host, path=\"/\"):\n+    try:\n+        conn = httplib.HTTPConnection(host, timeout=5)\n+        conn.request(\"HEAD\", path)\n+        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n+            return True\n+    except StandardError:\n+        return False\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+\n+input_file_open = open(input_file, 'r')\n+output_file_open = open(output_file, 'w+')\n+\n+domains = input_file_open.readlines()\n+\n+print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n+\n+for domain in domains:\n+    domain = domain.strip()\n+    if online(domain) == True and available(domain) == True:\n+        print(\"[+]\"+domain.strip())\n+        output_file_open.write(domain+\"\\n\")\n+    else:\n+        print(\"[-]\"+domain)\n+\n+input_file_open.close()\n+output_file_open.close()\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import httplib"], [4, "import socket"], [5, "import re"], [6, "import sys"], [7, ""], [8, "def online(host):"], [9, "    try:"], [10, "        socket.gethostbyname(host)"], [11, "    except socket.gaierror:"], [12, "        return False"], [13, "    else:"], [14, "        return True"], [15, ""], [16, "def available(host, path=\"/\"):"], [17, "    try:"], [18, "        conn = httplib.HTTPConnection(host, timeout=5)"], [19, "        conn.request(\"HEAD\", path)"], [20, "        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):"], [21, "            return True"], [22, "    except StandardError:"], [23, "        return False"], [24, ""], [25, "input_file = sys.argv[1]"], [26, "output_file = sys.argv[2]"], [27, ""], [28, "input_file_open = open(input_file, 'r')"], [29, "output_file_open = open(output_file, 'w+')"], [30, ""], [31, "domains = input_file_open.readlines()"], [32, ""], [33, "print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")"], [34, ""], [35, "for domain in domains:"], [36, "    domain = domain.strip()"], [37, "    if online(domain) == True and available(domain) == True:"], [38, "        print(\"[+]\"+domain.strip())"], [39, "        output_file_open.write(domain+\"\\n\")"], [40, "    else:"], [41, "        print(\"[-]\"+domain)"], [42, ""], [43, "input_file_open.close()"], [44, "output_file_open.close()"], [45, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 45, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport httplib\nimport socket\nimport re\nimport sys\n\ndef online(host):\n    try:\n        socket.gethostbyname(host)\n    except socket.gaierror:\n        return False\n    else:\n        return True\n\ndef available(host, path=\"/\"):\n    try:\n        conn = httplib.HTTPConnection(host, timeout=5)\n        conn.request(\"HEAD\", path)\n        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n            return True\n    except StandardError:\n        return False\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\ninput_file_open = open(input_file, 'r')\noutput_file_open = open(output_file, 'w+')\n\ndomains = input_file_open.readlines()\n\nprint(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    domain = domain.strip()\n    if online(domain) == True and available(domain) == True:\n        print(\"[+]\"+domain.strip())\n        output_file_open.write(domain+\"\\n\")\n    else:\n        print(\"[-]\"+domain)\n\ninput_file_open.close()\noutput_file_open.close()\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "methods_before": [], "changed_methods": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "nloc": 35, "complexity": 5, "token_count": 209}, {"old_path": null, "new_path": "tools/open_redirect.py", "filename": "open_redirect.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,49 @@\n+#!/usr/bin/python\r\n+import requests,sys\r\n+\r\n+def start():\r\n+    input_file = sys.argv[1]\r\n+    output_file = sys.argv[2]\r\n+    payload_file = sys.argv[3]\r\n+\r\n+    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n+\r\n+    is_closed = True\r\n+\r\n+    payloads = open(payload_file,'r').read().split('\\n')\r\n+\r\n+    #First loop trough the payloads to prevent 429 (rate limit)\r\n+    for payload in payloads: \r\n+        domains = open(input_file,'r').read().split('\\n')   \r\n+        print \"\\n - Trying payload \"+payload+\" - \"\r\n+        for domain in domains:\r\n+            if domain != \"\":\r\n+\r\n+                url = \"https://\" + domain + payload\r\n+                url = url.strip()\r\n+            \r\n+                try:\r\n+                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n+                except:\r\n+                    print \"[-]Error on \" + url\r\n+\r\n+                if r.history:  \r\n+                    if r.url == \"https://example.com\":\r\n+                        print \"[+]\"+url\r\n+                        if is_closed:\r\n+                            file = open(output_file,\"w+\")\r\n+                        is_closed = False\r\n+                        file.write(url + \"\\n\")\r\n+                    else:\r\n+                        print \"[-]\"+url\r\n+                else:\r\n+                    print \"[-]\"+url\r\n+        else:\r\n+            print \"[-]Domain is invalid\"\r\n+\r\n+    if is_closed == False:\r\n+        file.close()\r\n+    print(\"\\n-- Done --\")\r\n+\r\n+start()\r\n+\r\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, "import requests,sys"], [3, ""], [4, "def start():"], [5, "    input_file = sys.argv[1]"], [6, "    output_file = sys.argv[2]"], [7, "    payload_file = sys.argv[3]"], [8, ""], [9, "    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")"], [10, ""], [11, "    is_closed = True"], [12, ""], [13, "    payloads = open(payload_file,'r').read().split('\\n')"], [14, ""], [15, "    #First loop trough the payloads to prevent 429 (rate limit)"], [16, "    for payload in payloads:"], [17, "        domains = open(input_file,'r').read().split('\\n')"], [18, "        print \"\\n - Trying payload \"+payload+\" - \""], [19, "        for domain in domains:"], [20, "            if domain != \"\":"], [21, ""], [22, "                url = \"https://\" + domain + payload"], [23, "                url = url.strip()"], [24, ""], [25, "                try:"], [26, "                    r = requests.head(url, allow_redirects=True, timeout=5)"], [27, "                except:"], [28, "                    print \"[-]Error on \" + url"], [29, ""], [30, "                if r.history:"], [31, "                    if r.url == \"https://example.com\":"], [32, "                        print \"[+]\"+url"], [33, "                        if is_closed:"], [34, "                            file = open(output_file,\"w+\")"], [35, "                        is_closed = False"], [36, "                        file.write(url + \"\\n\")"], [37, "                    else:"], [38, "                        print \"[-]\"+url"], [39, "                else:"], [40, "                    print \"[-]\"+url"], [41, "        else:"], [42, "            print \"[-]Domain is invalid\""], [43, ""], [44, "    if is_closed == False:"], [45, "        file.close()"], [46, "    print(\"\\n-- Done --\")"], [47, ""], [48, "start()"], [49, ""]], "deleted": []}, "added_lines": 49, "deleted_lines": 0, "source_code": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n    payload_file = sys.argv[3]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(payload_file,'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n\r\n                if r.history:  \r\n                    if r.url == \"https://example.com\":\r\n                        print \"[+]\"+url\r\n                        if is_closed:\r\n                            file = open(output_file,\"w+\")\r\n                        is_closed = False\r\n                        file.write(url + \"\\n\")\r\n                    else:\r\n                        print \"[-]\"+url\r\n                else:\r\n                    print \"[-]\"+url\r\n        else:\r\n            print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "source_code_before": null, "methods": [{"name": "start", "start_line": 4, "end_line": 46}], "methods_before": [], "changed_methods": [{"name": "start", "start_line": 4, "end_line": 46}], "nloc": 36, "complexity": 9, "token_count": 212}, {"old_path": null, "new_path": "tools/subdomain_takeover_scan.py", "filename": "subdomain_takeover_scan.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,117 @@\n+#!/usr/bin/python\n+\n+import requests, sys, dns.resolver\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+is_closed = True\n+\n+domains = open(input_file,'r').read().split('\\n')\n+\n+take_over_cnames = [\"createsend\",\n+\"cargocollective\",\n+\"cloudfront\",\n+\"desk.com\",\n+\"fastly.net\",\n+\"feedpress.me\",\n+\"freshdesk.com\",\n+\"github.io\",\n+\"helpjuice.com\",\n+\"helpscoutdocs.com\",\n+\"herokudns.com\",\n+\"herokussl.com\",\n+\"herokuapp.com\",\n+\"pageserve.co\",\n+\"pingdom.com\",\n+\"amazonaws.com\",\n+\"myshopify.com\",\n+\"stspg-customer.com\",\n+\"sgizmo.com\",\n+\"surveygizmo.eu\",\n+\"sgizmoca.com\",\n+\"sgizmoca.com\",\n+\"tictail.com\",\n+\"domains.tumblr.com\",\n+\"uservoice.com\",\n+\"wpengine.com\",\n+\"squarespace.com\",\n+\"unbounce.com\",\n+\"zendesk.com\"]\n+\n+take_over_content = [\"<strong>Trying to access your account\",\n+\"Use a personal domain name\",\n+\"The request could not be satisfied\",\n+\"Sorry, We Couldn't Find That Page\",\n+\"Fastly error: unknown domain\",\n+\"The feed has not been found\",\n+\"You can claim it now at\",\n+\"Publishing platform\",                        \n+\"There isn't a GitHub Pages site here\",                       \n+\"No settings were found for this company\",\n+\"<title>No such app</title>\",                        \n+\"You've Discovered A Missing Link. Our Apologies!\",\n+\"Sorry, couldn&rsquo;t find the status page\",                        \n+\"NoSuchBucket\",\n+\"Sorry, this shop is currently unavailable\",\n+\"<title>Hosted Status Pages for Your Company</title>\",\n+\"data-html-name=\\\"Header Logo Link\\\"\",                        \n+\"<title>Oops - We didn't find your site.</title>\",\n+\"class=\\\"MarketplaceHeader__tictailLogo\\\"\",                        \n+\"Whatever you were looking for doesn't currently exist at this address\",\n+\"The requested URL was not found on this server\",\n+\"The page you have requested does not exist\",\n+\"This UserVoice subdomain is currently available!\",\n+\"but is not configured for an account on our platform\",\n+\"<title>Help Center Closed | Zendesk</title>\"]\n+\n+print(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n+\n+\n+for domain in domains:\n+\t#Skip first row, lol\n+\tif domain != domains[0]:\n+\t\tfound_content = False\n+\t\tfound_cname = False\n+\t\ttry:\n+\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text\n+\t\texcept:\n+\t\t\tprint(\"[-]Error in http://\"+domain)\n+\n+\t\tfor content in take_over_content:\n+\t\t\tif str(content) in r:\n+\t\t\t\tfound_content = True\n+\n+\t\ttry:\n+\t\t\tcnames = dns.resolver.query(domain, 'CNAME')\n+\t\t\tfor cname in cnames:\n+    \t\t\t\tfor cname_url in take_over_cnames:\n+\t\t\t\t\tif str(cname_url) in str(cname.target):\n+\t\t\t\t\t\tfound_cname = True\n+\n+\t\t\tif found_cname and found_content:\n+\t\t\t\tprint(\"[+]\"+domain)\n+\t\t\t\tif is_closed:\n+\t\t\t\t\tfile = open(output_file,\"w+\")\n+\t\t\t\t\tis_closed = False\n+\t\t\t\tfile.write(domain+\"\\n\")\n+\t\t\telse:\n+\t\t\t\tprint(\"[-]\"+domain)\n+\t\texcept:\n+\t\t\tprint \"[-]No cnames for \"+domain\n+\n+if is_closed == False:\n+\tfile.close()\n+\n+print(\"\\n-- Done --\")\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys, dns.resolver"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, ""], [11, "take_over_cnames = [\"createsend\","], [12, "\"cargocollective\","], [13, "\"cloudfront\","], [14, "\"desk.com\","], [15, "\"fastly.net\","], [16, "\"feedpress.me\","], [17, "\"freshdesk.com\","], [18, "\"github.io\","], [19, "\"helpjuice.com\","], [20, "\"helpscoutdocs.com\","], [21, "\"herokudns.com\","], [22, "\"herokussl.com\","], [23, "\"herokuapp.com\","], [24, "\"pageserve.co\","], [25, "\"pingdom.com\","], [26, "\"amazonaws.com\","], [27, "\"myshopify.com\","], [28, "\"stspg-customer.com\","], [29, "\"sgizmo.com\","], [30, "\"surveygizmo.eu\","], [31, "\"sgizmoca.com\","], [32, "\"sgizmoca.com\","], [33, "\"tictail.com\","], [34, "\"domains.tumblr.com\","], [35, "\"uservoice.com\","], [36, "\"wpengine.com\","], [37, "\"squarespace.com\","], [38, "\"unbounce.com\","], [39, "\"zendesk.com\"]"], [40, ""], [41, "take_over_content = [\"<strong>Trying to access your account\","], [42, "\"Use a personal domain name\","], [43, "\"The request could not be satisfied\","], [44, "\"Sorry, We Couldn't Find That Page\","], [45, "\"Fastly error: unknown domain\","], [46, "\"The feed has not been found\","], [47, "\"You can claim it now at\","], [48, "\"Publishing platform\","], [49, "\"There isn't a GitHub Pages site here\","], [50, "\"No settings were found for this company\","], [51, "\"<title>No such app</title>\","], [52, "\"You've Discovered A Missing Link. Our Apologies!\","], [53, "\"Sorry, couldn&rsquo;t find the status page\","], [54, "\"NoSuchBucket\","], [55, "\"Sorry, this shop is currently unavailable\","], [56, "\"<title>Hosted Status Pages for Your Company</title>\","], [57, "\"data-html-name=\\\"Header Logo Link\\\"\","], [58, "\"<title>Oops - We didn't find your site.</title>\","], [59, "\"class=\\\"MarketplaceHeader__tictailLogo\\\"\","], [60, "\"Whatever you were looking for doesn't currently exist at this address\","], [61, "\"The requested URL was not found on this server\","], [62, "\"The page you have requested does not exist\","], [63, "\"This UserVoice subdomain is currently available!\","], [64, "\"but is not configured for an account on our platform\","], [65, "\"<title>Help Center Closed | Zendesk</title>\"]"], [66, ""], [67, "print(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [68, ""], [69, ""], [70, "for domain in domains:"], [71, "\t#Skip first row, lol"], [72, "\tif domain != domains[0]:"], [73, "\t\tfound_content = False"], [74, "\t\tfound_cname = False"], [75, "\t\ttry:"], [76, "\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text"], [77, "\t\texcept:"], [78, "\t\t\tprint(\"[-]Error in http://\"+domain)"], [79, ""], [80, "\t\tfor content in take_over_content:"], [81, "\t\t\tif str(content) in r:"], [82, "\t\t\t\tfound_content = True"], [83, ""], [84, "\t\ttry:"], [85, "\t\t\tcnames = dns.resolver.query(domain, 'CNAME')"], [86, "\t\t\tfor cname in cnames:"], [87, "    \t\t\t\tfor cname_url in take_over_cnames:"], [88, "\t\t\t\t\tif str(cname_url) in str(cname.target):"], [89, "\t\t\t\t\t\tfound_cname = True"], [90, ""], [91, "\t\t\tif found_cname and found_content:"], [92, "\t\t\t\tprint(\"[+]\"+domain)"], [93, "\t\t\t\tif is_closed:"], [94, "\t\t\t\t\tfile = open(output_file,\"w+\")"], [95, "\t\t\t\t\tis_closed = False"], [96, "\t\t\t\tfile.write(domain+\"\\n\")"], [97, "\t\t\telse:"], [98, "\t\t\t\tprint(\"[-]\"+domain)"], [99, "\t\texcept:"], [100, "\t\t\tprint \"[-]No cnames for \"+domain"], [101, ""], [102, "if is_closed == False:"], [103, "\tfile.close()"], [104, ""], [105, "print(\"\\n-- Done --\")"], [106, ""], [107, ""], [108, ""], [109, ""], [110, ""], [111, ""], [112, ""], [113, ""], [114, ""], [115, ""], [116, ""], [117, ""]], "deleted": []}, "added_lines": 117, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport requests, sys, dns.resolver\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\n\ntake_over_cnames = [\"createsend\",\n\"cargocollective\",\n\"cloudfront\",\n\"desk.com\",\n\"fastly.net\",\n\"feedpress.me\",\n\"freshdesk.com\",\n\"github.io\",\n\"helpjuice.com\",\n\"helpscoutdocs.com\",\n\"herokudns.com\",\n\"herokussl.com\",\n\"herokuapp.com\",\n\"pageserve.co\",\n\"pingdom.com\",\n\"amazonaws.com\",\n\"myshopify.com\",\n\"stspg-customer.com\",\n\"sgizmo.com\",\n\"surveygizmo.eu\",\n\"sgizmoca.com\",\n\"sgizmoca.com\",\n\"tictail.com\",\n\"domains.tumblr.com\",\n\"uservoice.com\",\n\"wpengine.com\",\n\"squarespace.com\",\n\"unbounce.com\",\n\"zendesk.com\"]\n\ntake_over_content = [\"<strong>Trying to access your account\",\n\"Use a personal domain name\",\n\"The request could not be satisfied\",\n\"Sorry, We Couldn't Find That Page\",\n\"Fastly error: unknown domain\",\n\"The feed has not been found\",\n\"You can claim it now at\",\n\"Publishing platform\",                        \n\"There isn't a GitHub Pages site here\",                       \n\"No settings were found for this company\",\n\"<title>No such app</title>\",                        \n\"You've Discovered A Missing Link. Our Apologies!\",\n\"Sorry, couldn&rsquo;t find the status page\",                        \n\"NoSuchBucket\",\n\"Sorry, this shop is currently unavailable\",\n\"<title>Hosted Status Pages for Your Company</title>\",\n\"data-html-name=\\\"Header Logo Link\\\"\",                        \n\"<title>Oops - We didn't find your site.</title>\",\n\"class=\\\"MarketplaceHeader__tictailLogo\\\"\",                        \n\"Whatever you were looking for doesn't currently exist at this address\",\n\"The requested URL was not found on this server\",\n\"The page you have requested does not exist\",\n\"This UserVoice subdomain is currently available!\",\n\"but is not configured for an account on our platform\",\n\"<title>Help Center Closed | Zendesk</title>\"]\n\nprint(\"\\n-- Checking possible subdomain take overs in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\n\nfor domain in domains:\n\t#Skip first row, lol\n\tif domain != domains[0]:\n\t\tfound_content = False\n\t\tfound_cname = False\n\t\ttry:\n\t\t\tr=requests.get(\"http://\"+domain, timeout=5).text\n\t\texcept:\n\t\t\tprint(\"[-]Error in http://\"+domain)\n\n\t\tfor content in take_over_content:\n\t\t\tif str(content) in r:\n\t\t\t\tfound_content = True\n\n\t\ttry:\n\t\t\tcnames = dns.resolver.query(domain, 'CNAME')\n\t\t\tfor cname in cnames:\n    \t\t\t\tfor cname_url in take_over_cnames:\n\t\t\t\t\tif str(cname_url) in str(cname.target):\n\t\t\t\t\t\tfound_cname = True\n\n\t\t\tif found_cname and found_content:\n\t\t\t\tprint(\"[+]\"+domain)\n\t\t\t\tif is_closed:\n\t\t\t\t\tfile = open(output_file,\"w+\")\n\t\t\t\t\tis_closed = False\n\t\t\t\tfile.write(domain+\"\\n\")\n\t\t\telse:\n\t\t\t\tprint(\"[-]\"+domain)\n\t\texcept:\n\t\t\tprint \"[-]No cnames for \"+domain\n\nif is_closed == False:\n\tfile.close()\n\nprint(\"\\n-- Done --\")\n\n\n\n\n\n\n\n\n\n\n\n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 90, "complexity": 0, "token_count": 332}, {"old_path": null, "new_path": "tools/wordpress_check.py", "filename": "wordpress_check.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,34 @@\n+#!/usr/bin/python\n+\n+import requests, sys\n+\n+input_file = sys.argv[1]\n+output_file = sys.argv[2]\n+is_closed = True\n+\n+domains = open(input_file,'r').read().split('\\n')\n+\n+print(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n+\n+for domain in domains:\n+    if domain != \"\":\n+        try:\n+            response = requests.get(\"https://\"+domain)\n+        except:\n+            print(\"[-]Error on https://\"+domain)\n+\n+        if \"/wp-content/\" in response.content:\n+            if is_closed:\n+                file = open(output_file,\"w+\")\n+            is_closed = False\n+            print(\"[+]\"+domain)\n+            file.write(domain + \"\\n\")\n+        else:\n+            print(\"[-]\"+domain)\n+    else:\n+        print(\"[-]Domain is invalid\")\n+        \n+if is_closed == False:\n+    file.close()\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import requests, sys"], [4, ""], [5, "input_file = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "is_closed = True"], [8, ""], [9, "domains = open(input_file,'r').read().split('\\n')"], [10, ""], [11, "print(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")"], [12, ""], [13, "for domain in domains:"], [14, "    if domain != \"\":"], [15, "        try:"], [16, "            response = requests.get(\"https://\"+domain)"], [17, "        except:"], [18, "            print(\"[-]Error on https://\"+domain)"], [19, ""], [20, "        if \"/wp-content/\" in response.content:"], [21, "            if is_closed:"], [22, "                file = open(output_file,\"w+\")"], [23, "            is_closed = False"], [24, "            print(\"[+]\"+domain)"], [25, "            file.write(domain + \"\\n\")"], [26, "        else:"], [27, "            print(\"[-]\"+domain)"], [28, "    else:"], [29, "        print(\"[-]Domain is invalid\")"], [30, ""], [31, "if is_closed == False:"], [32, "    file.close()"], [33, ""], [34, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 34, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nis_closed = True\n\ndomains = open(input_file,'r').read().split('\\n')\n\nprint(\"\\n-- Checking for wordpress sites in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    if domain != \"\":\n        try:\n            response = requests.get(\"https://\"+domain)\n        except:\n            print(\"[-]Error on https://\"+domain)\n\n        if \"/wp-content/\" in response.content:\n            if is_closed:\n                file = open(output_file,\"w+\")\n            is_closed = False\n            print(\"[+]\"+domain)\n            file.write(domain + \"\\n\")\n        else:\n            print(\"[-]\"+domain)\n    else:\n        print(\"[-]Domain is invalid\")\n        \nif is_closed == False:\n    file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 25, "complexity": 0, "token_count": 145}, {"old_path": null, "new_path": "tools/wpscan_domains.sh", "filename": "wpscan_domains.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,13 @@\n+#!/bin/bash\n+\n+printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n+\n+if [ ! -f $1 ]; then\n+    echo \"[-]File not found!\"\n+else\n+    while read domain; do \n+        echo \"[+]Opening $domain\"\n+        xterm -hold -e \"$2 --url https://$domain\" &\n+    done < $1\n+fi\n+printf \"\\n -- Done -- \\n\"\n", "diff_parsed": {"added": [[1, "#!/bin/bash"], [2, ""], [3, "printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\""], [4, ""], [5, "if [ ! -f $1 ]; then"], [6, "    echo \"[-]File not found!\""], [7, "else"], [8, "    while read domain; do"], [9, "        echo \"[+]Opening $domain\""], [10, "        xterm -hold -e \"$2 --url https://$domain\" &"], [11, "    done < $1"], [12, "fi"], [13, "printf \"\\n -- Done -- \\n\""]], "deleted": []}, "added_lines": 13, "deleted_lines": 0, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"$2 --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": 0.30612244897959184, "dmm_unit_complexity": 0.30612244897959184, "dmm_unit_interfacing": 1.0},
    {"hash": "3f81bccdacea4b68e835be33b59a4aa20a4bc097", "msg": "Added auto dependencie downloader", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 23:11:28+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 23:11:28+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["390869eb3918243933864cec4b93fb35f5f45aaf"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 5, "insertions": 10, "lines": 15, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,4 @@\n-    home_dir=\"/home/rjp/Desktop/recon\"\n+    home_dir=\"/home/003random/Tools/recon\" # Change to your own directory!\n \n     output_dir=\"output\"\n     tools_dir=\"tools\"\n@@ -6,6 +6,11 @@\n     dependencies_dir=\"dependencies\"  \n     screenshots_dir=\"$output_dir/$@/screenshots\"  \n \n+    [ -d $output_dir ] || mkdir $output_dir;\n+    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;\n+\n+\n+\n     all_domains_file=\"$output_dir/$@/domains-all.txt\"\n     domains_file=\"$output_dir/$@/domains.txt\"\n     crlf_file=\"$output_dir/$@/crlf.txt\"\n@@ -27,7 +32,7 @@\n     \n     wpscan_location=\"dependencies/wpscan\"\n     url_extractor_location=\"dependencies/relative-url-extractor\"\n-    sublister_location=\"dependencies/Sublist3r\"\n+    sublister_location=\"dependencies/sublister\"\n     webscreenshot_location=\"dependencies/webscreenshot\"\n \n     cd $home_dir/$output_dir;\n@@ -39,9 +44,9 @@\n \n     python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n     python $tools_dir/online.py $all_domains_file $domains_file;\n-    #$tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n-    #$tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n-    #python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n+    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n+    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n+    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n     python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n     python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n     python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n", "diff_parsed": {"added": [[1, "    home_dir=\"/home/003random/Tools/recon\" # Change to your own directory!"], [9, "    [ -d $output_dir ] || mkdir $output_dir;"], [10, "    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;"], [11, ""], [12, ""], [13, ""], [35, "    sublister_location=\"dependencies/sublister\""], [47, "    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;"], [48, "    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;"], [49, "    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;"]], "deleted": [[1, "    home_dir=\"/home/rjp/Desktop/recon\""], [30, "    sublister_location=\"dependencies/Sublist3r\""], [42, "    #$tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;"], [43, "    #$tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;"], [44, "    #python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;"]]}, "added_lines": 10, "deleted_lines": 5, "source_code": "    home_dir=\"/home/003random/Tools/recon\" # Change to your own directory!\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;\n\n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    home_dir=\"/home/rjp/Desktop/recon\"\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/Sublist3r\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    #$tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    #$tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    #python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "efe14f25905e0292090f65f25f4cffaea775b048", "msg": "Removed static home dir", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 23:39:38+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 23:39:38+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["3f81bccdacea4b68e835be33b59a4aa20a4bc097"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,4 @@\n-    home_dir=\"/home/003random/Tools/recon\" # Change to your own directory!\n+    home_dir=$(pwd)\n \n     output_dir=\"output\"\n     tools_dir=\"tools\"\n", "diff_parsed": {"added": [[1, "    home_dir=$(pwd)"]], "deleted": [[1, "    home_dir=\"/home/003random/Tools/recon\" # Change to your own directory!"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;\n\n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    home_dir=\"/home/003random/Tools/recon\" # Change to your own directory!\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;\n\n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "776e4eeebabc378697aae90ed924158bbb92a6ef", "msg": "Set execute premission on tools dir", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-21 23:53:01+01:00", "author_timezone": -3600, "committer_date": "2017-11-21 23:53:01+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["efe14f25905e0292090f65f25f4cffaea775b048"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 3, "insertions": 2, "lines": 5, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -7,7 +7,7 @@\n     screenshots_dir=\"$output_dir/$@/screenshots\"  \n \n     [ -d $output_dir ] || mkdir $output_dir;\n-    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;\n+    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n \n \n \n@@ -27,8 +27,7 @@\n     crlf_payload_file=\"$payloads_dir/crlf.txt\"\n     error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n     headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n-    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n-\n+    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n     \n     wpscan_location=\"dependencies/wpscan\"\n     url_extractor_location=\"dependencies/relative-url-extractor\"\n", "diff_parsed": {"added": [[10, "    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir;"], [30, "    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\""]], "deleted": [[10, "    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;"], [30, "    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\""], [31, ""]]}, "added_lines": 2, "deleted_lines": 3, "source_code": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n\n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;\n\n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "6b4e66299ec9ba24839b19584ca509a839900715", "msg": "Update sensitive_headers.txt", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-22 13:18:06+01:00", "author_timezone": -3600, "committer_date": "2017-11-22 13:18:06+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["776e4eeebabc378697aae90ed924158bbb92a6ef"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 1, "lines": 1, "files": 1, "modified_files": [{"old_path": "payloads/sensitive_headers.txt", "new_path": "payloads/sensitive_headers.txt", "filename": "sensitive_headers.txt", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,3 +1,4 @@\n X-Powered-By\n Server\n X-AspNet-Version\n+X-Server-Instance\n", "diff_parsed": {"added": [[4, "X-Server-Instance"]], "deleted": []}, "added_lines": 1, "deleted_lines": 0, "source_code": "X-Powered-By\nServer\nX-AspNet-Version\nX-Server-Instance\n", "source_code_before": "X-Powered-By\nServer\nX-AspNet-Version\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "8f31c18917a1a429efccea34b19396f31743f611", "msg": "Added a chck if server header is nginx", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-22 13:28:58+01:00", "author_timezone": -3600, "committer_date": "2017-11-22 13:28:58+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["6b4e66299ec9ba24839b19584ca509a839900715"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "tools/header_scan.py", "new_path": "tools/header_scan.py", "filename": "header_scan.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -20,7 +20,7 @@ for domain in domains:\n \r\n \t\tfor header in headers:\r\n \t\t\tcurrent_header = r.headers.get(header.lower())\r\n-\t\t\tif current_header != None:\r\n+\t\t\tif current_header != None and \"nginx\" not in current_header.lower():\r\n \t\t\t\theaders_found.append(str(current_header))\r\n \t\tif headers_found != []:\r\n \t\t\tif is_closed:\r\n", "diff_parsed": {"added": [[23, "\t\t\tif current_header != None and \"nginx\" not in current_header.lower():"]], "deleted": [[23, "\t\t\tif current_header != None:"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\npayload_file = sys.argv[3]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open(payload_file)]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None and \"nginx\" not in current_header.lower():\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "source_code_before": "import requests, sys\r\n\r\ninput_file = sys.argv[1]\r\noutput_file = sys.argv[2]\r\npayload_file = sys.argv[3]\r\n\r\nprint(\"\\n-- Testing for sensitive info in headers on domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\r\n\r\nis_closed = True\r\ndomains = open(input_file,'r').read().split('\\n')\r\nheaders = [line.rstrip('\\n').lower() for line in open(payload_file)]\r\n\r\nfor domain in domains:\r\n\tif domain != \"\":\r\n\t\ttry:\r\n\t\t\tr = requests.head(\"https://\"+domain, timeout=5)\r\n\t\texcept:\r\n\t\t\tprint(\"[-]Error on https://\"+domain)\r\n\t\theaders_found = []\r\n\r\n\t\tfor header in headers:\r\n\t\t\tcurrent_header = r.headers.get(header.lower())\r\n\t\t\tif current_header != None:\r\n\t\t\t\theaders_found.append(str(current_header))\r\n\t\tif headers_found != []:\r\n\t\t\tif is_closed:\r\n        \t\t\tfile = open(output_file,\"w+\")\r\n\t\t\t\tis_closed = False\r\n\t\t\tprint(\"[+]\"+domain+\" - \"+str(headers_found))\r\n\t\t\tfile.write(domain+\" - \"+str(headers_found)+\"\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"[-]\"+domain+\" - \"+str(headers_found))\r\n\telse:\r\n            print \"[-]Domain is invalid\"\r\n\r\nif is_closed == False:\r\n\tfile.close()\r\n\r\nprint(\"\\n-- Done --\")\r\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 32, "complexity": 0, "token_count": 242}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "28d1819f4c8c6a3d69118047e213fe890b40c440", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-22 19:14:50+01:00", "author_timezone": -3600, "committer_date": "2017-11-22 19:14:50+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["8f31c18917a1a429efccea34b19396f31743f611"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,5 +1,5 @@\n ## recon\n \n-This repository contains all my scripts that i made to automate alot of my recon and scanning.\n+This repository contains some of my scripts that i made to automate alot of my recon and scanning.\n These files are free to take, edit and share.\n \n", "diff_parsed": {"added": [[3, "This repository contains some of my scripts that i made to automate alot of my recon and scanning."]], "deleted": [[3, "This repository contains all my scripts that i made to automate alot of my recon and scanning."]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## recon\n\nThis repository contains some of my scripts that i made to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n", "source_code_before": "## recon\n\nThis repository contains all my scripts that i made to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "d1ea21ba5a8dee057595ca60d668dc9d6044ac92", "msg": "closed the domain file, tip from @jtru", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-23 00:37:32+01:00", "author_timezone": -3600, "committer_date": "2017-11-23 00:37:32+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["28d1819f4c8c6a3d69118047e213fe890b40c440"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 1, "lines": 1, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -68,5 +68,6 @@ for domain in domains:\n \n \n file.close()\n+domains.close()\n \n print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[71, "domains.close()"]], "deleted": []}, "added_lines": 1, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains = open(input_file,'r').read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains = open(input_file,'r').read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 56, "complexity": 0, "token_count": 315}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "6b3fffa886ec5cf47e90e4f267dee516bca2fd1f", "msg": "resolved the error of closing the wrong file", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-25 01:16:44+01:00", "author_timezone": -3600, "committer_date": "2017-11-25 01:16:44+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["d1ea21ba5a8dee057595ca60d668dc9d6044ac92"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 3, "lines": 5, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -7,7 +7,8 @@ output_file = sys.argv[2]\n \n print(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n \n-domains = open(input_file,'r').read().split('\\n')\n+domains_file = open(input_file,'r')\n+domains = domains_file.read().split('\\n')\n \n file = open(output_file,\"w+\")\n \n@@ -68,6 +69,6 @@ for domain in domains:\n \n \n file.close()\n-domains.close()\n+domains_file.close()\n \n print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[10, "domains_file = open(input_file,'r')"], [11, "domains = domains_file.read().split('\\n')"], [72, "domains_file.close()"]], "deleted": [[10, "domains = open(input_file,'r').read().split('\\n')"], [71, "domains.close()"]]}, "added_lines": 3, "deleted_lines": 2, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains = open(input_file,'r').read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 57, "complexity": 0, "token_count": 318}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "2cced63f4f584ddc19d3a40c3f733cf752f78ee0", "msg": "Removed the create dir and dependencies", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-25 02:29:12+01:00", "author_timezone": -3600, "committer_date": "2017-11-25 02:29:12+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["6b3fffa886ec5cf47e90e4f267dee516bca2fd1f"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 4, "insertions": 0, "lines": 4, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -6,10 +6,6 @@\n     dependencies_dir=\"dependencies\"  \n     screenshots_dir=\"$output_dir/$@/screenshots\"  \n \n-    [ -d $output_dir ] || mkdir $output_dir;\n-    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n-\n-\n \n     all_domains_file=\"$output_dir/$@/domains-all.txt\"\n     domains_file=\"$output_dir/$@/domains.txt\"\n", "diff_parsed": {"added": [], "deleted": [[9, "    [ -d $output_dir ] || mkdir $output_dir;"], [10, "    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir;"], [11, ""], [12, ""]]}, "added_lines": 0, "deleted_lines": 4, "source_code": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n\n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "3f3323a62ee2782f66f0cc8c99f7753dafc4ca47", "msg": "Create install.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-25 02:30:31+01:00", "author_timezone": -3600, "committer_date": "2017-11-25 02:30:31+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["2cced63f4f584ddc19d3a40c3f733cf752f78ee0"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 7, "lines": 7, "files": 1, "modified_files": [{"old_path": null, "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,7 @@\n+    home_dir=$(pwd)\n+\n+    dependencies_dir=\"dependencies\"  \n+\n+    [ -d $output_dir ] || mkdir $output_dir;\n+    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n+\n", "diff_parsed": {"added": [[1, "    home_dir=$(pwd)"], [2, ""], [3, "    dependencies_dir=\"dependencies\""], [4, ""], [5, "    [ -d $output_dir ] || mkdir $output_dir;"], [6, "    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir;"], [7, ""]], "deleted": []}, "added_lines": 7, "deleted_lines": 0, "source_code": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "1505b5f63ac507cd0e9cfb89ae0748308cc6d9d6", "msg": "Update install.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-25 02:34:48+01:00", "author_timezone": -3600, "committer_date": "2017-11-25 02:34:48+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["3f3323a62ee2782f66f0cc8c99f7753dafc4ca47"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 7, "lines": 9, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -2,6 +2,11 @@\n \n     dependencies_dir=\"dependencies\"  \n \n-    [ -d $output_dir ] || mkdir $output_dir;\n-    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n+    mkdir $output_dir;\n+    mkdir $dependencies_dir; \n+    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n+    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n+    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n+    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n+    chmod -R 777 $home_dir/$tools_dir; \n \n", "diff_parsed": {"added": [[5, "    mkdir $output_dir;"], [6, "    mkdir $dependencies_dir;"], [7, "    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister;"], [8, "    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan;"], [9, "    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor;"], [10, "    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;"], [11, "    chmod -R 777 $home_dir/$tools_dir;"]], "deleted": [[5, "    [ -d $output_dir ] || mkdir $output_dir;"], [6, "    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir;"]]}, "added_lines": 7, "deleted_lines": 2, "source_code": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    chmod -R 777 $home_dir/$tools_dir; \n\n", "source_code_before": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    [ -d $output_dir ] || mkdir $output_dir;\n    [ -d $dependencies_dir ] || mkdir $dependencies_dir; git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; chmod -R 777 $home_dir/$tools_dir; \n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "c35489e7de99b38be1185ade4135b4f5e3fef23a", "msg": "Add files via upload", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-25 02:38:15+01:00", "author_timezone": -3600, "committer_date": "2017-11-25 02:38:15+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["1505b5f63ac507cd0e9cfb89ae0748308cc6d9d6"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 4, "lines": 4, "files": 1, "modified_files": [{"old_path": null, "new_path": "output/New Text Document.txt", "filename": "New Text Document.txt", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,4 @@\n+# Ignore everything in this directory\r\n+*\r\n+# Except this file\r\n+!.gitignore\n\\ No newline at end of file\n", "diff_parsed": {"added": [[1, "# Ignore everything in this directory"], [2, "*"], [3, "# Except this file"], [4, "!.gitignore"]], "deleted": []}, "added_lines": 4, "deleted_lines": 0, "source_code": "# Ignore everything in this directory\r\n*\r\n# Except this file\r\n!.gitignore", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "b9f4e4f939c6460d56177c49905c7a13fb3be8e7", "msg": "Rename New Text Document.txt to .gitignore", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-25 02:38:39+01:00", "author_timezone": -3600, "committer_date": "2017-11-25 02:38:39+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["c35489e7de99b38be1185ade4135b4f5e3fef23a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "output/New Text Document.txt", "new_path": "output/.gitignore", "filename": ".gitignore", "change_type": "ModificationType.RENAME", "change_type_name": "RENAME", "diff": "@@ -1,4 +1,4 @@\n # Ignore everything in this directory\r\n *\r\n # Except this file\r\n-!.gitignore\n\\ No newline at end of file\n+!.gitignore\r\n", "diff_parsed": {"added": [[4, "!.gitignore"]], "deleted": [[4, "!.gitignore"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "# Ignore everything in this directory\r\n*\r\n# Except this file\r\n!.gitignore\r\n", "source_code_before": "# Ignore everything in this directory\r\n*\r\n# Except this file\r\n!.gitignore", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "2fe7eaa8bc0244d4d390d53fe2bf7ce983e19b79", "msg": "added nmap in the install", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:23:48+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:23:48+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["b9f4e4f939c6460d56177c49905c7a13fb3be8e7"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 6, "lines": 6, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -8,5 +8,11 @@\n     git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n     git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n     git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n+    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+    cd $dependencies_dir/nmap;\n+    ./configure;\n+    make;\n+    make install;\n+    cd ../../;\n     chmod -R 777 $home_dir/$tools_dir; \n \n", "diff_parsed": {"added": [[11, "    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [12, "    cd $dependencies_dir/nmap;"], [13, "    ./configure;"], [14, "    make;"], [15, "    make install;"], [16, "    cd ../../;"]], "deleted": []}, "added_lines": 6, "deleted_lines": 0, "source_code": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n    cd $dependencies_dir/nmap;\n    ./configure;\n    make;\n    make install;\n    cd ../../;\n    chmod -R 777 $home_dir/$tools_dir; \n\n", "source_code_before": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    chmod -R 777 $home_dir/$tools_dir; \n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "77968757e6ae4bb3452564011c2200983092b53b", "msg": "added a dynamic path to nmap", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:40:07+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:40:07+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["2fe7eaa8bc0244d4d390d53fe2bf7ce983e19b79"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 2, "lines": 4, "files": 1, "modified_files": [{"old_path": "tools/nmap_scan.sh", "new_path": "tools/nmap_scan.sh", "filename": "nmap_scan.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -3,9 +3,9 @@\n printf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\n echo \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n \n-while read domain; do \n+while read domain; do\n     echo \"-- $domain --\"\n-    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\n+    $3/nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\n done < $1  >> $2\n \n echo \"-- Done --\"\n", "diff_parsed": {"added": [[6, "while read domain; do"], [8, "    $3/nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'"]], "deleted": [[6, "while read domain; do"], [8, "    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'"]]}, "added_lines": 2, "deleted_lines": 2, "source_code": "#!/bin/bash\n\nprintf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\necho \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n\nwhile read domain; do\n    echo \"-- $domain --\"\n    $3/nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\ndone < $1  >> $2\n\necho \"-- Done --\"\n\n\n", "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Scanning services from $1 with output file, $2 --\\n\"\necho \"-- This might take around $((`wc -l < $1` / 1)) minutes --\"\n\nwhile read domain; do \n    echo \"-- $domain --\"\n    nmap -sV $domain | sed -n '/PORT/,/report/p' | awk -F\"Service\" '{print $1}'\ndone < $1  >> $2\n\necho \"-- Done --\"\n\n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "04fac288de4a5c210e0cfa7ebc77ed2719c89ac6", "msg": "added dynamic path to nmap scan for nmap", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:41:44+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:41:44+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["77968757e6ae4bb3452564011c2200983092b53b"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 3, "lines": 4, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -29,6 +29,8 @@\n     url_extractor_location=\"dependencies/relative-url-extractor\"\n     sublister_location=\"dependencies/sublister\"\n     webscreenshot_location=\"dependencies/webscreenshot\"\n+    nmap_location=\"dependencies/nmap\"\n+\n \n     cd $home_dir/$output_dir;\n     rm -rf $@; \n@@ -51,6 +53,6 @@\n     python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n     $wpscan_location/wpscan.rb --update;\n     $tools_dir/wpscan_domains.sh $wordpress_file;\n-    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n+    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n     \n     printf \"\\n -- $@ Finished -- \\n\"\n", "diff_parsed": {"added": [[32, "    nmap_location=\"dependencies/nmap\""], [33, ""], [56, "    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;"]], "deleted": [[54, "    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;"]]}, "added_lines": 3, "deleted_lines": 1, "source_code": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "cb71a799b2d000a0b34f1665aa62b2fca29e2de7", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:44:38+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:44:38+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["04fac288de4a5c210e0cfa7ebc77ed2719c89ac6"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 10, "lines": 11, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,5 +1,14 @@\n ## recon\n \n-This repository contains some of my scripts that i made to automate alot of my recon and scanning.\n+This repository contains some of my scripts that i created to automate alot of my recon and scanning.\n These files are free to take, edit and share.\n \n+Install:\n+git clone https://github.com/003random/003Recon.git;\n+cd 003Recon;\n+chmod 777 install.sh;\n+./install.sh;\n+\n+#And then call it with:\n+\n+./recon.sh example.com\n", "diff_parsed": {"added": [[3, "This repository contains some of my scripts that i created to automate alot of my recon and scanning."], [6, "Install:"], [7, "git clone https://github.com/003random/003Recon.git;"], [8, "cd 003Recon;"], [9, "chmod 777 install.sh;"], [10, "./install.sh;"], [11, ""], [12, "#And then call it with:"], [13, ""], [14, "./recon.sh example.com"]], "deleted": [[3, "This repository contains some of my scripts that i made to automate alot of my recon and scanning."]]}, "added_lines": 10, "deleted_lines": 1, "source_code": "## recon\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\nInstall:\ngit clone https://github.com/003random/003Recon.git;\ncd 003Recon;\nchmod 777 install.sh;\n./install.sh;\n\n#And then call it with:\n\n./recon.sh example.com\n", "source_code_before": "## recon\n\nThis repository contains some of my scripts that i made to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "5d2eed0fd1a12b88f5cd382cd2075273939fbae4", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:45:00+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:45:00+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["cb71a799b2d000a0b34f1665aa62b2fca29e2de7"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 7, "insertions": 7, "lines": 14, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -3,12 +3,12 @@\n This repository contains some of my scripts that i created to automate alot of my recon and scanning.\n These files are free to take, edit and share.\n \n-Install:\n-git clone https://github.com/003random/003Recon.git;\n-cd 003Recon;\n-chmod 777 install.sh;\n-./install.sh;\n+### Install:  \n+git clone https://github.com/003random/003Recon.git;  \n+cd 003Recon;  \n+chmod 777 install.sh;  \n+./install.sh;  \n \n-#And then call it with:\n+#And then call it with:  \n \n-./recon.sh example.com\n+./recon.sh example.com  \n", "diff_parsed": {"added": [[6, "### Install:"], [7, "git clone https://github.com/003random/003Recon.git;"], [8, "cd 003Recon;"], [9, "chmod 777 install.sh;"], [10, "./install.sh;"], [12, "#And then call it with:"], [14, "./recon.sh example.com"]], "deleted": [[6, "Install:"], [7, "git clone https://github.com/003random/003Recon.git;"], [8, "cd 003Recon;"], [9, "chmod 777 install.sh;"], [10, "./install.sh;"], [12, "#And then call it with:"], [14, "./recon.sh example.com"]]}, "added_lines": 7, "deleted_lines": 7, "source_code": "## recon\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n", "source_code_before": "## recon\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\nInstall:\ngit clone https://github.com/003random/003Recon.git;\ncd 003Recon;\nchmod 777 install.sh;\n./install.sh;\n\n#And then call it with:\n\n./recon.sh example.com\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "83a8396e49f4ca7ff95c0ac7e83277252579134b", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:45:14+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:45:14+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["5d2eed0fd1a12b88f5cd382cd2075273939fbae4"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,4 @@\n-## recon\n+## Description\n \n This repository contains some of my scripts that i created to automate alot of my recon and scanning.\n These files are free to take, edit and share.\n", "diff_parsed": {"added": [[1, "## Description"]], "deleted": [[1, "## recon"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n", "source_code_before": "## recon\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "2ebbfb302b27e87b5f651e12aafe06a13f491336", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:46:17+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:46:17+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["83a8396e49f4ca7ff95c0ac7e83277252579134b"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 3, "lines": 3, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -12,3 +12,6 @@ chmod 777 install.sh;\n #And then call it with:  \n \n ./recon.sh example.com  \n+  \n+# \n+*Created by [H1 - 003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003)*\n", "diff_parsed": {"added": [[15, ""], [16, "#"], [17, "*Created by [H1 - 003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003)*"]], "deleted": []}, "added_lines": 3, "deleted_lines": 0, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n# \n*Created by [H1 - 003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "8e8f769b12e670a77da9f9e8b629d1ee0861f8ab", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-11-28 18:47:20+01:00", "author_timezone": -3600, "committer_date": "2017-11-28 18:47:20+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["2ebbfb302b27e87b5f651e12aafe06a13f491336"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -14,4 +14,4 @@ chmod 777 install.sh;\n ./recon.sh example.com  \n   \n # \n-*Created by [H1 - 003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003)*\n+*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "diff_parsed": {"added": [[17, "*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*"]], "deleted": [[17, "*Created by [H1 - 003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003)*"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n# \n*Created by [H1 - 003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "3bcba063ced9fcee2dde97b53a0d6f64df8a85dd", "msg": "Added credits to Jobert", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-04 17:49:53+01:00", "author_timezone": -3600, "committer_date": "2017-12-04 17:49:53+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["8e8f769b12e670a77da9f9e8b629d1ee0861f8ab"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_link_extractor.sh", "new_path": "tools/javascript_files_link_extractor.sh", "filename": "javascript_files_link_extractor.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,5 +1,5 @@\n #!/bin/bash\n-\n+#This tool uses https://github.com/jobertabma/relative-url-extractor. So credits to Jobert for the extractor tool. The tool is used on line 17.\n printf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n \n if [ ! -f $1 ]; then\n", "diff_parsed": {"added": [[2, "#This tool uses https://github.com/jobertabma/relative-url-extractor. So credits to Jobert for the extractor tool. The tool is used on line 17."]], "deleted": [[2, ""]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/bin/bash\n#This tool uses https://github.com/jobertabma/relative-url-extractor. So credits to Jobert for the extractor tool. The tool is used on line 17.\nprintf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n\nif [ ! -f $1 ]; then\n    printf  \"\\n[-]File not found!\"\nelse\n    while read domain; do \n        if [[ $domain == \"-\"* ]]; then\n            printf  \"\\n\\n\\n[+]-$domain--\"\n        else\n            if [ -z \"$domain\" ]; then\n                printf \"\\n[-]Invalid domain $domain\"\n            else\n                printf \"\\n[+]$domain \\n\"\n\t\techo \"----------------------\"\n                command=\"ruby $3 $domain\"\n                eval $command\n            fi\n        fi\n    done < $1 >> $2\nprintf \"\\n -- Done -- \\n\"\nfi\n", "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Extracting links out of javascript files in $1 with output file, $2  --\\n\"\n\nif [ ! -f $1 ]; then\n    printf  \"\\n[-]File not found!\"\nelse\n    while read domain; do \n        if [[ $domain == \"-\"* ]]; then\n            printf  \"\\n\\n\\n[+]-$domain--\"\n        else\n            if [ -z \"$domain\" ]; then\n                printf \"\\n[-]Invalid domain $domain\"\n            else\n                printf \"\\n[+]$domain \\n\"\n\t\techo \"----------------------\"\n                command=\"ruby $3 $domain\"\n                eval $command\n            fi\n        fi\n    done < $1 >> $2\nprintf \"\\n -- Done -- \\n\"\nfi\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "5b549978a409340df8264db1ecb4ddebc9879bc2", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:54:12+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:54:12+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["3bcba063ced9fcee2dde97b53a0d6f64df8a85dd"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 20, "lines": 20, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -13,5 +13,25 @@ chmod 777 install.sh;\n \n ./recon.sh example.com  \n   \n+  \n+This tools does the following things;  \n+1. Get subdomains of a domain    \n+2. Filter out only online domains    \n+3. Scan the domains for CRLF    \n+4. Check for a CORS misconfiguration  \n+5. Test for open redirects  \n+6. Grab sensitive headers  \n+7. Get senstive info from error pages  \n+8. Check for subdomain takeovers  \n+9. Extract javascript files  \n+10. Feed the javascript files into 'relative-url-extractor'  \n+11. Screenshot all domains  \n+12. Check if sites run wordpress  \n+13. Start a wpscan on the wordpress sites  \n+14. Do a nmap service scan  \n+\n+All out will get saved in a folder named by the domain, in the output folder.   \n+In here it will create files with the discovered content.  \n+  \n # \n *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "diff_parsed": {"added": [[16, ""], [17, "This tools does the following things;"], [18, "1. Get subdomains of a domain"], [19, "2. Filter out only online domains"], [20, "3. Scan the domains for CRLF"], [21, "4. Check for a CORS misconfiguration"], [22, "5. Test for open redirects"], [23, "6. Grab sensitive headers"], [24, "7. Get senstive info from error pages"], [25, "8. Check for subdomain takeovers"], [26, "9. Extract javascript files"], [27, "10. Feed the javascript files into 'relative-url-extractor'"], [28, "11. Screenshot all domains"], [29, "12. Check if sites run wordpress"], [30, "13. Start a wpscan on the wordpress sites"], [31, "14. Do a nmap service scan"], [32, ""], [33, "All out will get saved in a folder named by the domain, in the output folder."], [34, "In here it will create files with the discovered content."], [35, ""]], "deleted": []}, "added_lines": 20, "deleted_lines": 0, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n  \nThis tools does the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll out will get saved in a folder named by the domain, in the output folder.   \nIn here it will create files with the discovered content.  \n  \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "918edb3c41b7048ecb9a6ea0a0aa464a32ba74bd", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:54:55+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:54:55+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["5b549978a409340df8264db1ecb4ddebc9879bc2"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -31,7 +31,7 @@ This tools does the following things;\n 14. Do a nmap service scan  \n \n All out will get saved in a folder named by the domain, in the output folder.   \n-In here it will create files with the discovered content.  \n+In this folder it will create files with the discovered content.  \n   \n # \n *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "diff_parsed": {"added": [[34, "In this folder it will create files with the discovered content."]], "deleted": [[34, "In here it will create files with the discovered content."]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n  \nThis tools does the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll out will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n  \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n  \nThis tools does the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll out will get saved in a folder named by the domain, in the output folder.   \nIn here it will create files with the discovered content.  \n  \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "41a8a84d0653e436f1b448912ac87e3060231f92", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:56:12+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:56:12+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["918edb3c41b7048ecb9a6ea0a0aa464a32ba74bd"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 16, "insertions": 13, "lines": 29, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,20 +1,7 @@\n ## Description\n \n-This repository contains some of my scripts that i created to automate alot of my recon and scanning.\n-These files are free to take, edit and share.\n-\n-### Install:  \n-git clone https://github.com/003random/003Recon.git;  \n-cd 003Recon;  \n-chmod 777 install.sh;  \n-./install.sh;  \n-\n-#And then call it with:  \n-\n-./recon.sh example.com  \n-  \n-  \n-This tools does the following things;  \n+This repository contains some of my scripts that i created to automate some recon processes.  \n+It performs the following things;  \n 1. Get subdomains of a domain    \n 2. Filter out only online domains    \n 3. Scan the domains for CRLF    \n@@ -32,6 +19,16 @@ This tools does the following things;\n \n All out will get saved in a folder named by the domain, in the output folder.   \n In this folder it will create files with the discovered content.  \n-  \n+\n+### Install:  \n+git clone https://github.com/003random/003Recon.git;  \n+cd 003Recon;  \n+chmod 777 install.sh;  \n+./install.sh;  \n+\n+#And then call it with:  \n+\n+./recon.sh example.com  \n+    \n # \n *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "diff_parsed": {"added": [[3, "This repository contains some of my scripts that i created to automate some recon processes."], [4, "It performs the following things;"], [22, ""], [23, "### Install:"], [24, "git clone https://github.com/003random/003Recon.git;"], [25, "cd 003Recon;"], [26, "chmod 777 install.sh;"], [27, "./install.sh;"], [28, ""], [29, "#And then call it with:"], [30, ""], [31, "./recon.sh example.com"], [32, ""]], "deleted": [[3, "This repository contains some of my scripts that i created to automate alot of my recon and scanning."], [4, "These files are free to take, edit and share."], [5, ""], [6, "### Install:"], [7, "git clone https://github.com/003random/003Recon.git;"], [8, "cd 003Recon;"], [9, "chmod 777 install.sh;"], [10, "./install.sh;"], [11, ""], [12, "#And then call it with:"], [13, ""], [14, "./recon.sh example.com"], [15, ""], [16, ""], [17, "This tools does the following things;"], [35, ""]]}, "added_lines": 13, "deleted_lines": 16, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll out will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate alot of my recon and scanning.\nThese files are free to take, edit and share.\n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n  \nThis tools does the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll out will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n  \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "6310a6dab52ad220c89d0df8c279ae456d4fbac6", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:56:29+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:56:29+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["41a8a84d0653e436f1b448912ac87e3060231f92"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -17,7 +17,7 @@ It performs the following things;\n 13. Start a wpscan on the wordpress sites  \n 14. Do a nmap service scan  \n \n-All out will get saved in a folder named by the domain, in the output folder.   \n+All output will get saved in a folder named by the domain, in the output folder.   \n In this folder it will create files with the discovered content.  \n \n ### Install:  \n", "diff_parsed": {"added": [[20, "All output will get saved in a folder named by the domain, in the output folder."]], "deleted": [[20, "All out will get saved in a folder named by the domain, in the output folder."]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll out will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "2c36fdd9b3c541088dd550da6028b81b3ec27cf0", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:57:10+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:57:10+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["6310a6dab52ad220c89d0df8c279ae456d4fbac6"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -24,7 +24,7 @@ In this folder it will create files with the discovered content.\n git clone https://github.com/003random/003Recon.git;  \n cd 003Recon;  \n chmod 777 install.sh;  \n-./install.sh;  \n+./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n \n #And then call it with:  \n \n", "diff_parsed": {"added": [[27, "./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here."]], "deleted": [[27, "./install.sh;"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## Description\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "d8041eaca3c22c6a47ee27d2be6f701af174191d", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:57:53+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:57:53+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["2c36fdd9b3c541088dd550da6028b81b3ec27cf0"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 2, "lines": 4, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,4 @@\n-## Description\n+## Description \ud83d\udccc\n \n This repository contains some of my scripts that i created to automate some recon processes.  \n It performs the following things;  \n@@ -31,4 +31,4 @@ chmod 777 install.sh;\n ./recon.sh example.com  \n     \n # \n-*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n+*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \ud83d\udc4c\n", "diff_parsed": {"added": [[1, "## Description \ud83d\udccc"], [34, "*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \ud83d\udc4c"]], "deleted": [[1, "## Description"], [34, "*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*"]]}, "added_lines": 2, "deleted_lines": 2, "source_code": "## Description \ud83d\udccc\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \ud83d\udc4c\n", "source_code_before": "## Description\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "4a8f9f9f76732987333eee61f40f6bdaee62e28c", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 12:58:14+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 12:58:14+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["d8041eaca3c22c6a47ee27d2be6f701af174191d"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 2, "lines": 4, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,4 @@\n-## Description \ud83d\udccc\n+## \ud83d\udccc Description \n \n This repository contains some of my scripts that i created to automate some recon processes.  \n It performs the following things;  \n@@ -31,4 +31,4 @@ chmod 777 install.sh;\n ./recon.sh example.com  \n     \n # \n-*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \ud83d\udc4c\n+\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "diff_parsed": {"added": [[1, "## \ud83d\udccc Description"], [34, "\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)*"]], "deleted": [[1, "## Description \ud83d\udccc"], [34, "*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \ud83d\udc4c"]]}, "added_lines": 2, "deleted_lines": 2, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## Description \ud83d\udccc\n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n*Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \ud83d\udc4c\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "198ab90a04ec3aad9188933b6a6aecec3a936701", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 13:52:15+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 13:52:15+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["4a8f9f9f76732987333eee61f40f6bdaee62e28c"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 2, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -17,6 +17,8 @@ It performs the following things;\n 13. Start a wpscan on the wordpress sites  \n 14. Do a nmap service scan  \n \n+More tools in comming soon / in progress  :wink:  \n+\n All output will get saved in a folder named by the domain, in the output folder.   \n In this folder it will create files with the discovered content.  \n \n", "diff_parsed": {"added": [[20, "More tools in comming soon / in progress  :wink:"], [21, ""]], "deleted": []}, "added_lines": 2, "deleted_lines": 0, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "7579cf587ff7129d6b60e033c37b020ffc9dffe3", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 13:57:35+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 13:57:35+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["198ab90a04ec3aad9188933b6a6aecec3a936701"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 0, "lines": 1, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -25,7 +25,6 @@ In this folder it will create files with the discovered content.\n ### Install:  \n git clone https://github.com/003random/003Recon.git;  \n cd 003Recon;  \n-chmod 777 install.sh;  \n ./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n \n #And then call it with:  \n", "diff_parsed": {"added": [], "deleted": [[28, "chmod 777 install.sh;"]]}, "added_lines": 0, "deleted_lines": 1, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \nchmod 777 install.sh;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "66988f479a0e685999b8041969f9b666f7a67b06", "msg": "Update install.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 13:57:59+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 13:57:59+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["7579cf587ff7129d6b60e033c37b020ffc9dffe3"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 0, "lines": 2, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -14,5 +14,3 @@\n     make;\n     make install;\n     cd ../../;\n-    chmod -R 777 $home_dir/$tools_dir; \n-\n", "diff_parsed": {"added": [], "deleted": [[17, "    chmod -R 777 $home_dir/$tools_dir;"], [18, ""]]}, "added_lines": 0, "deleted_lines": 2, "source_code": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n    cd $dependencies_dir/nmap;\n    ./configure;\n    make;\n    make install;\n    cd ../../;\n", "source_code_before": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n    cd $dependencies_dir/nmap;\n    ./configure;\n    make;\n    make install;\n    cd ../../;\n    chmod -R 777 $home_dir/$tools_dir; \n\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "a6773684023484287a7fcf7ded5d809b077c96f1", "msg": "Update recon.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 14:00:37+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 14:00:37+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["66988f479a0e685999b8041969f9b666f7a67b06"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,3 +1,4 @@\n+    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n     home_dir=$(pwd)\n \n     output_dir=\"output\"\n@@ -33,7 +34,6 @@\n \n \n     cd $home_dir/$output_dir;\n-    rm -rf $@; \n     mkdir $@; \n     cd ../\n \n", "diff_parsed": {"added": [[1, "    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results."]], "deleted": [[36, "    rm -rf $@;"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    rm -rf $@; \n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "cb6b7f3417ab58383a33f9cc36d806912ec85391", "msg": "Update recon.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 14:14:37+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 14:14:37+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["a6773684023484287a7fcf7ded5d809b077c96f1"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 5, "lines": 5, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,7 @@\n     #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n+    tput setaf 3;\n+    echo \"Please read throug this script before executing, to prevent unexpected things from happening.\"\n+    \n     home_dir=$(pwd)\n \n     output_dir=\"output\"\n@@ -34,6 +37,8 @@\n \n \n     cd $home_dir/$output_dir;\n+    # Uncomment on own risk. this will first clean the old results.\n+    #rm -rf $@;\n     mkdir $@; \n     cd ../\n \n", "diff_parsed": {"added": [[2, "    tput setaf 3;"], [3, "    echo \"Please read throug this script before executing, to prevent unexpected things from happening.\""], [4, ""], [40, "    # Uncomment on own risk. this will first clean the old results."], [41, "    #rm -rf $@;"]], "deleted": []}, "added_lines": 5, "deleted_lines": 0, "source_code": "    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n    tput setaf 3;\n    echo \"Please read throug this script before executing, to prevent unexpected things from happening.\"\n    \n    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    # Uncomment on own risk. this will first clean the old results.\n    #rm -rf $@;\n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "2b6ad1fe0147e871229ec191bba78cbb346e18c4", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 15:53:04+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 15:53:04+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["cb6b7f3417ab58383a33f9cc36d806912ec85391"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 2, "lines": 3, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -30,6 +30,7 @@ cd 003Recon;\n #And then call it with:  \n \n ./recon.sh example.com  \n-    \n+  \n+#Also, youmigh        .   .\n # \n \ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "diff_parsed": {"added": [[33, ""], [34, "#Also, youmigh        .   ."]], "deleted": [[33, ""]]}, "added_lines": 2, "deleted_lines": 1, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n#Also, youmigh        .   .\n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n    \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "1f62c09e7b2b12108bf09655ab72bcbd5c03f901", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 15:54:22+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 15:54:22+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["2b6ad1fe0147e871229ec191bba78cbb346e18c4"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -31,6 +31,6 @@ cd 003Recon;\n \n ./recon.sh example.com  \n   \n-#Also, youmigh        .   .\n+#Also, you might need to install some python modules like 'requests'. \n # \n \ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "diff_parsed": {"added": [[34, "#Also, you might need to install some python modules like 'requests'."]], "deleted": [[34, "#Also, youmigh        .   ."]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n#Also, you might need to install some python modules like 'requests'. \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n#Also, youmigh        .   .\n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "e0fab4236f2ba593f4941d26b2855ca5ffdb50d5", "msg": "Update wpscan_domains.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 18:23:07+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 18:23:07+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["1f62c09e7b2b12108bf09655ab72bcbd5c03f901"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 1, "lines": 1, "files": 1, "modified_files": [{"old_path": "tools/wpscan_domains.sh", "new_path": "tools/wpscan_domains.sh", "filename": "wpscan_domains.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,5 @@\n #!/bin/bash\n+#This script uses wpscan that is installed in the dependencies folder\n \n printf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n \n", "diff_parsed": {"added": [[2, "#This script uses wpscan that is installed in the dependencies folder"]], "deleted": []}, "added_lines": 1, "deleted_lines": 0, "source_code": "#!/bin/bash\n#This script uses wpscan that is installed in the dependencies folder\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"$2 --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "source_code_before": "#!/bin/bash\n\nprintf \"\\n-- Starting a wpscan for the domains in $1 --\\n\\n\"\n\nif [ ! -f $1 ]; then\n    echo \"[-]File not found!\"\nelse\n    while read domain; do \n        echo \"[+]Opening $domain\"\n        xterm -hold -e \"$2 --url https://$domain\" &\n    done < $1\nfi\nprintf \"\\n -- Done -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "65984fefb8836374601246cf7aef800b1eeea10a", "msg": "Create javascript_files_and_links_extractor.py\n\nEdited on request for @streaak", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 18:24:49+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 18:24:49+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["e0fab4236f2ba593f4941d26b2855ca5ffdb50d5"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 62, "lines": 62, "files": 1, "modified_files": [{"old_path": null, "new_path": "edited_tools/javascript_files_and_links_extractor.py", "filename": "javascript_files_and_links_extractor.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,62 @@\n+#!/usr/bin/python\n+\n+import re, requests, sys, os\n+\n+input_string = sys.argv[1]\n+output_file = sys.argv[2]\n+extractor_file = sys.argv[3]\n+\n+print(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n+\n+\n+black_listed_domains = [\"ajax.googleapis.com\",\n+                        \"cdn.optimizely.com\",\n+                        \"googletagmanager.com\",\n+                        \"fontawesome.com\"]\n+\n+if input_string is not \"\":\n+\tdomain = input_string\n+\tdomain_written = False\n+\ti = 0\n+\tb_amount = 0\n+\tfull_domain = \"\"\n+\tif domain != \"\":\n+\t\tmatches = \"\"\n+\t\tr = \"\"\n+\t\tregex = r'script src=\"(.*?)\"'\n+\t\ttry:\n+\t\t\tr = requests.get(\"http://\"+domain).content\n+\t\texcept:\n+\t\t\tprint \"[-]Error in http://\"+domain\n+\n+\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\t\tif matches == []:\n+\t\t\tregex = r\"script src='(.*?)'\"\n+\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\n+\t\tfor m in matches:\n+\t\t\tblack_listed = False\n+\t\t\tfor b in black_listed_domains:\n+\t\t\t\tif b in m:\n+\t\t\t\t\tblack_listed = True\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\tif m.startswith(\"/\"):\n+\t\t\t\t\tif m.startswith(\"//\"):\n+\t\t\t\t\t\tfull_domain = \"https:\"+m\n+\t\t\t\t\telse:\n+\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n+\t\t\t\telif m.startswith(\"http\"):\n+\t\t\t\t\tfull_domain = m\n+\t\t\t\telse:\n+\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n+\t\t\telse:\n+\t\t\t\tb_amount += 1\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\ti += 1\n+\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n+\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n+\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, ""], [3, "import re, requests, sys, os"], [4, ""], [5, "input_string = sys.argv[1]"], [6, "output_file = sys.argv[2]"], [7, "extractor_file = sys.argv[3]"], [8, ""], [9, "print(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")"], [10, ""], [11, ""], [12, "black_listed_domains = [\"ajax.googleapis.com\","], [13, "                        \"cdn.optimizely.com\","], [14, "                        \"googletagmanager.com\","], [15, "                        \"fontawesome.com\"]"], [16, ""], [17, "if input_string is not \"\":"], [18, "\tdomain = input_string"], [19, "\tdomain_written = False"], [20, "\ti = 0"], [21, "\tb_amount = 0"], [22, "\tfull_domain = \"\""], [23, "\tif domain != \"\":"], [24, "\t\tmatches = \"\""], [25, "\t\tr = \"\""], [26, "\t\tregex = r'script src=\"(.*?)\"'"], [27, "\t\ttry:"], [28, "\t\t\tr = requests.get(\"http://\"+domain).content"], [29, "\t\texcept:"], [30, "\t\t\tprint \"[-]Error in http://\"+domain"], [31, ""], [32, "\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [33, "\t\tif matches == []:"], [34, "\t\t\tregex = r\"script src='(.*?)'\""], [35, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [36, ""], [37, "\t\tfor m in matches:"], [38, "\t\t\tblack_listed = False"], [39, "\t\t\tfor b in black_listed_domains:"], [40, "\t\t\t\tif b in m:"], [41, "\t\t\t\t\tblack_listed = True"], [42, ""], [43, "\t\t\tif black_listed != True:"], [44, "\t\t\t\tif m.startswith(\"/\"):"], [45, "\t\t\t\t\tif m.startswith(\"//\"):"], [46, "\t\t\t\t\t\tfull_domain = \"https:\"+m"], [47, "\t\t\t\t\telse:"], [48, "\t\t\t\t\t\tfull_domain = \"https://\"+domain+m"], [49, "\t\t\t\telif m.startswith(\"http\"):"], [50, "\t\t\t\t\tfull_domain = m"], [51, "\t\t\t\telse:"], [52, "\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m"], [53, "\t\t\telse:"], [54, "\t\t\t\tb_amount += 1"], [55, ""], [56, "\t\t\tif black_listed != True:"], [57, "\t\t\t\ti += 1"], [58, "\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)"], [59, "\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)"], [60, ""], [61, ""], [62, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 62, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys, os\n\ninput_string = sys.argv[1]\noutput_file = sys.argv[2]\nextractor_file = sys.argv[3]\n\nprint(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nif input_string is not \"\":\n\tdomain = input_string\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 49, "complexity": 0, "token_count": 272}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "cd7cff3c4ede579cc5138f297fb12fc6cd96408e", "msg": "Update javascript_files_and_links_extractor.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 18:26:08+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 18:26:08+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["65984fefb8836374601246cf7aef800b1eeea10a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 2, "lines": 2, "files": 1, "modified_files": [{"old_path": "edited_tools/javascript_files_and_links_extractor.py", "new_path": "edited_tools/javascript_files_and_links_extractor.py", "filename": "javascript_files_and_links_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,4 +1,6 @@\n #!/usr/bin/python\n+#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh\n+#it combines those 2 and takes 1 domain as string as argument 1. not a domain file\n \n import re, requests, sys, os\n \n", "diff_parsed": {"added": [[2, "#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh"], [3, "#it combines those 2 and takes 1 domain as string as argument 1. not a domain file"]], "deleted": []}, "added_lines": 2, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh\n#it combines those 2 and takes 1 domain as string as argument 1. not a domain file\n\nimport re, requests, sys, os\n\ninput_string = sys.argv[1]\noutput_file = sys.argv[2]\nextractor_file = sys.argv[3]\n\nprint(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nif input_string is not \"\":\n\tdomain = input_string\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys, os\n\ninput_string = sys.argv[1]\noutput_file = sys.argv[2]\nextractor_file = sys.argv[3]\n\nprint(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nif input_string is not \"\":\n\tdomain = input_string\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 49, "complexity": 0, "token_count": 272}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "73032bd601b7006503746ea3a344905e1764cd61", "msg": "Update README.md", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-05 20:00:05+01:00", "author_timezone": -3600, "committer_date": "2017-12-05 20:00:05+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["cd7cff3c4ede579cc5138f297fb12fc6cd96408e"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 2, "lines": 4, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -5,10 +5,10 @@ It performs the following things;\n 1. Get subdomains of a domain    \n 2. Filter out only online domains    \n 3. Scan the domains for CRLF    \n-4. Check for a CORS misconfiguration  \n+4. Check for a CORS misconfigurations  \n 5. Test for open redirects  \n 6. Grab sensitive headers  \n-7. Get senstive info from error pages  \n+7. Get sensitive info from error pages  \n 8. Check for subdomain takeovers  \n 9. Extract javascript files  \n 10. Feed the javascript files into 'relative-url-extractor'  \n", "diff_parsed": {"added": [[8, "4. Check for a CORS misconfigurations"], [11, "7. Get sensitive info from error pages"]], "deleted": [[8, "4. Check for a CORS misconfiguration"], [11, "7. Get senstive info from error pages"]]}, "added_lines": 2, "deleted_lines": 2, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfigurations  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get sensitive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n#Also, you might need to install some python modules like 'requests'. \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfiguration  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get senstive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n#Also, you might need to install some python modules like 'requests'. \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "fa05c3c41f03c7ec1d689e0a6329811aab272fab", "msg": "fix open_redirect 'r' error", "author": {"name": "Abhinash Jain", "email": "labor.omnia.vincit18@gmail.com"}, "committer": {"name": "Abhinash Jain", "email": "labor.omnia.vincit18@gmail.com"}, "author_date": "2017-12-06 21:13:25+05:30", "author_timezone": -19800, "committer_date": "2017-12-06 21:13:25+05:30", "committer_timezone": -19800, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["73032bd601b7006503746ea3a344905e1764cd61"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 13, "insertions": 13, "lines": 26, "files": 1, "modified_files": [{"old_path": "tools/open_redirect.py", "new_path": "tools/open_redirect.py", "filename": "open_redirect.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -24,22 +24,22 @@ def start():\n             \r\n                 try:\r\n                     r = requests.head(url, allow_redirects=True, timeout=5)\r\n-                except:\r\n-                    print \"[-]Error on \" + url\r\n \r\n-                if r.history:  \r\n-                    if r.url == \"https://example.com\":\r\n-                        print \"[+]\"+url\r\n-                        if is_closed:\r\n-                            file = open(output_file,\"w+\")\r\n-                        is_closed = False\r\n-                        file.write(url + \"\\n\")\r\n+                    if r.history:  \r\n+                        if r.url == \"https://example.com\":\r\n+                            print \"[+]\"+url\r\n+                            if is_closed:\r\n+                                file = open(output_file,\"w+\")\r\n+                            is_closed = False\r\n+                            file.write(url + \"\\n\")\r\n+                        else:\r\n+                            print \"[-]\"+url\r\n                     else:\r\n                         print \"[-]\"+url\r\n-                else:\r\n-                    print \"[-]\"+url\r\n-        else:\r\n-            print \"[-]Domain is invalid\"\r\n+                except:\r\n+                    print \"[-]Error on \" + url\r\n+            else:\r\n+                print \"[-]Domain is invalid\"\r\n \r\n     if is_closed == False:\r\n         file.close()\r\n", "diff_parsed": {"added": [[28, "                    if r.history:"], [29, "                        if r.url == \"https://example.com\":"], [30, "                            print \"[+]\"+url"], [31, "                            if is_closed:"], [32, "                                file = open(output_file,\"w+\")"], [33, "                            is_closed = False"], [34, "                            file.write(url + \"\\n\")"], [35, "                        else:"], [36, "                            print \"[-]\"+url"], [39, "                except:"], [40, "                    print \"[-]Error on \" + url"], [41, "            else:"], [42, "                print \"[-]Domain is invalid\""]], "deleted": [[27, "                except:"], [28, "                    print \"[-]Error on \" + url"], [30, "                if r.history:"], [31, "                    if r.url == \"https://example.com\":"], [32, "                        print \"[+]\"+url"], [33, "                        if is_closed:"], [34, "                            file = open(output_file,\"w+\")"], [35, "                        is_closed = False"], [36, "                        file.write(url + \"\\n\")"], [39, "                else:"], [40, "                    print \"[-]\"+url"], [41, "        else:"], [42, "            print \"[-]Domain is invalid\""]]}, "added_lines": 13, "deleted_lines": 13, "source_code": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n    payload_file = sys.argv[3]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(payload_file,'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n\r\n                    if r.history:  \r\n                        if r.url == \"https://example.com\":\r\n                            print \"[+]\"+url\r\n                            if is_closed:\r\n                                file = open(output_file,\"w+\")\r\n                            is_closed = False\r\n                            file.write(url + \"\\n\")\r\n                        else:\r\n                            print \"[-]\"+url\r\n                    else:\r\n                        print \"[-]\"+url\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n            else:\r\n                print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "source_code_before": "#!/usr/bin/python\r\nimport requests,sys\r\n\r\ndef start():\r\n    input_file = sys.argv[1]\r\n    output_file = sys.argv[2]\r\n    payload_file = sys.argv[3]\r\n\r\n    print(\"\\n-- Testing open redirects on domains in \"+input_file+\" with output file, \"+output_file+\" --\")\r\n\r\n    is_closed = True\r\n\r\n    payloads = open(payload_file,'r').read().split('\\n')\r\n\r\n    #First loop trough the payloads to prevent 429 (rate limit)\r\n    for payload in payloads: \r\n        domains = open(input_file,'r').read().split('\\n')   \r\n        print \"\\n - Trying payload \"+payload+\" - \"\r\n        for domain in domains:\r\n            if domain != \"\":\r\n\r\n                url = \"https://\" + domain + payload\r\n                url = url.strip()\r\n            \r\n                try:\r\n                    r = requests.head(url, allow_redirects=True, timeout=5)\r\n                except:\r\n                    print \"[-]Error on \" + url\r\n\r\n                if r.history:  \r\n                    if r.url == \"https://example.com\":\r\n                        print \"[+]\"+url\r\n                        if is_closed:\r\n                            file = open(output_file,\"w+\")\r\n                        is_closed = False\r\n                        file.write(url + \"\\n\")\r\n                    else:\r\n                        print \"[-]\"+url\r\n                else:\r\n                    print \"[-]\"+url\r\n        else:\r\n            print \"[-]Domain is invalid\"\r\n\r\n    if is_closed == False:\r\n        file.close()\r\n    print(\"\\n-- Done --\")\r\n\r\nstart()\r\n\r\n", "methods": [{"name": "start", "start_line": 4, "end_line": 46}], "methods_before": [{"name": "start", "start_line": 4, "end_line": 46}], "changed_methods": [{"name": "start", "start_line": 4, "end_line": 46}], "nloc": 36, "complexity": 9, "token_count": 212}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "cd1f622a0df7b1f85dcc65e00c59f74ecf275225", "msg": "Removed unused pwd var, added output dir var\n\nThansk to @ieguiguren for pointing this out to me", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-07 22:10:45+01:00", "author_timezone": -3600, "committer_date": "2017-12-07 22:10:45+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["73032bd601b7006503746ea3a344905e1764cd61"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 2, "insertions": 2, "lines": 4, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,6 +1,6 @@\n-    home_dir=$(pwd)\n-\n     dependencies_dir=\"dependencies\"  \n+    dependencies_dir=\"output\"  \n+\n \n     mkdir $output_dir;\n     mkdir $dependencies_dir; \n", "diff_parsed": {"added": [[2, "    dependencies_dir=\"output\""], [3, ""]], "deleted": [[1, "    home_dir=$(pwd)"], [2, ""]]}, "added_lines": 2, "deleted_lines": 2, "source_code": "    dependencies_dir=\"dependencies\"  \n    dependencies_dir=\"output\"  \n\n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n    cd $dependencies_dir/nmap;\n    ./configure;\n    make;\n    make install;\n    cd ../../;\n", "source_code_before": "    home_dir=$(pwd)\n\n    dependencies_dir=\"dependencies\"  \n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n    cd $dependencies_dir/nmap;\n    ./configure;\n    make;\n    make install;\n    cd ../../;\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "14d630d04423684ddd90a68890f719564cad6b77", "msg": "Delete .gitignore", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-07 22:11:47+01:00", "author_timezone": -3600, "committer_date": "2017-12-07 22:11:47+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["cd1f622a0df7b1f85dcc65e00c59f74ecf275225"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 4, "insertions": 0, "lines": 4, "files": 1, "modified_files": [{"old_path": "output/.gitignore", "new_path": null, "filename": ".gitignore", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,4 +0,0 @@\n-# Ignore everything in this directory\r\n-*\r\n-# Except this file\r\n-!.gitignore\r\n", "diff_parsed": {"added": [], "deleted": [[1, "# Ignore everything in this directory"], [2, "*"], [3, "# Except this file"], [4, "!.gitignore"]]}, "added_lines": 0, "deleted_lines": 4, "source_code": null, "source_code_before": "# Ignore everything in this directory\r\n*\r\n# Except this file\r\n!.gitignore\r\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "0896dff1a36889b5ca28e38202cb14dd825dff5f", "msg": "Merge pull request #3 from abhinashjain/master\n\nfix open_redirect 'r' error", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-07 22:19:34+01:00", "author_timezone": -3600, "committer_date": "2017-12-07 22:19:34+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": true, "parents": ["14d630d04423684ddd90a68890f719564cad6b77", "fa05c3c41f03c7ec1d689e0a6329811aab272fab"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 13, "insertions": 13, "lines": 26, "files": 1, "modified_files": [], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "2573309d82edfa70305ac3cb7f33dadd99f2aa11", "msg": "formatting, no content changes", "author": {"name": "jose nazario", "email": "jose.monkey.org@gmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-15 09:59:52-05:00", "author_timezone": 18000, "committer_date": "2017-12-15 09:59:52-05:00", "committer_timezone": 18000, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["0896dff1a36889b5ca28e38202cb14dd825dff5f"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 8, "insertions": 9, "lines": 17, "files": 1, "modified_files": [{"old_path": "README.md", "new_path": "README.md", "filename": "README.md", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -22,15 +22,16 @@ More tools in comming soon / in progress  :wink:\n All output will get saved in a folder named by the domain, in the output folder.   \n In this folder it will create files with the discovered content.  \n \n-### Install:  \n-git clone https://github.com/003random/003Recon.git;  \n-cd 003Recon;  \n-./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n+## Install:  \n \n-#And then call it with:  \n+    git clone https://github.com/003random/003Recon.git;  \n+    cd 003Recon;  \n+    ./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n \n-./recon.sh example.com  \n+And then call it with:\n+\n+    ./recon.sh example.com  \n   \n-#Also, you might need to install some python modules like 'requests'. \n-# \n+### Also, you might need to install some python modules like 'requests'.\n+\n \ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "diff_parsed": {"added": [[25, "## Install:"], [27, "    git clone https://github.com/003random/003Recon.git;"], [28, "    cd 003Recon;"], [29, "    ./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here."], [31, "And then call it with:"], [32, ""], [33, "    ./recon.sh example.com"], [35, "### Also, you might need to install some python modules like 'requests'."], [36, ""]], "deleted": [[25, "### Install:"], [26, "git clone https://github.com/003random/003Recon.git;"], [27, "cd 003Recon;"], [28, "./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here."], [30, "#And then call it with:"], [32, "./recon.sh example.com"], [34, "#Also, you might need to install some python modules like 'requests'."], [35, "#"]]}, "added_lines": 9, "deleted_lines": 8, "source_code": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfigurations  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get sensitive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n## Install:  \n\n    git clone https://github.com/003random/003Recon.git;  \n    cd 003Recon;  \n    ./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\nAnd then call it with:\n\n    ./recon.sh example.com  \n  \n### Also, you might need to install some python modules like 'requests'.\n\n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "source_code_before": "## \ud83d\udccc Description \n\nThis repository contains some of my scripts that i created to automate some recon processes.  \nIt performs the following things;  \n1. Get subdomains of a domain    \n2. Filter out only online domains    \n3. Scan the domains for CRLF    \n4. Check for a CORS misconfigurations  \n5. Test for open redirects  \n6. Grab sensitive headers  \n7. Get sensitive info from error pages  \n8. Check for subdomain takeovers  \n9. Extract javascript files  \n10. Feed the javascript files into 'relative-url-extractor'  \n11. Screenshot all domains  \n12. Check if sites run wordpress  \n13. Start a wpscan on the wordpress sites  \n14. Do a nmap service scan  \n\nMore tools in comming soon / in progress  :wink:  \n\nAll output will get saved in a folder named by the domain, in the output folder.   \nIn this folder it will create files with the discovered content.  \n\n### Install:  \ngit clone https://github.com/003random/003Recon.git;  \ncd 003Recon;  \n./install.sh;  #Or if you have some tools already installed, edit the paths in recon.sh and comment those tools out here.  \n\n#And then call it with:  \n\n./recon.sh example.com  \n  \n#Also, you might need to install some python modules like 'requests'. \n# \n\ud83d\udc4c *Created by [003random](http://hackerone.com/003random) - [@003random](https://twitter.com/rub003) - [003random.com](https://poc-server.com/blog/)* \n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "070c6cdb5e4fb8f83cab053f878771a2c1d048d5", "msg": "Merge pull request #6 from paralax/patch-1\n\nREADME formatting, no content changes", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2017-12-18 01:06:18+01:00", "author_timezone": -3600, "committer_date": "2017-12-18 01:06:18+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": true, "parents": ["0896dff1a36889b5ca28e38202cb14dd825dff5f", "2573309d82edfa70305ac3cb7f33dadd99f2aa11"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 8, "insertions": 9, "lines": 17, "files": 1, "modified_files": [], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "369ed7a3155b75b30c51a4ff3adc10a35d91a6d4", "msg": "Replaced current regex with a better one", "author": {"name": "Karel", "email": "karel.origin@gmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-15 12:44:29+01:00", "author_timezone": -3600, "committer_date": "2018-01-15 12:44:29+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["070c6cdb5e4fb8f83cab053f878771a2c1d048d5"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -25,7 +25,7 @@ for domain in domains:\n \tif domain != \"\":\n \t\tmatches = \"\"\n \t\tr = \"\"\n-\t\tregex = r'script src=\"(.*?)\"'\n+\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n \t\ttry:\n \t\t\tr = requests.get(\"http://\"+domain).content\n \t\texcept:\n", "diff_parsed": {"added": [[28, "\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'"]], "deleted": [[28, "\t\tregex = r'script src=\"(.*?)\"'"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 57, "complexity": 0, "token_count": 318}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "98c001c03e52de454a6181d0e3dd935a1f3b66b5", "msg": "Update javascript_files_extractor.py", "author": {"name": "Karel", "email": "karel.origin@gmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-15 13:00:51+01:00", "author_timezone": -3600, "committer_date": "2018-01-15 13:00:51+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["369ed7a3155b75b30c51a4ff3adc10a35d91a6d4"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -33,7 +33,7 @@ for domain in domains:\n \n \t\tmatches = re.findall(regex, r, re.MULTILINE)\n \t\tif matches == []:\n-\t\t\tregex = r\"script src='(.*?)'\"\n+\t\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n \t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n \n \t\tfor m in matches:\n", "diff_parsed": {"added": [[36, "\t\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'"]], "deleted": [[36, "\t\t\tregex = r\"script src='(.*?)'\""]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 57, "complexity": 0, "token_count": 318}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "4db608cf46b0b27ae9e14cda57c1874bc4c5afd5", "msg": "Update javascript_files_extractor.py", "author": {"name": "Karel", "email": "karel.origin@gmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-15 13:02:16+01:00", "author_timezone": -3600, "committer_date": "2018-01-15 13:02:16+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["98c001c03e52de454a6181d0e3dd935a1f3b66b5"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 3, "insertions": 0, "lines": 3, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -32,9 +32,6 @@ for domain in domains:\n \t\t\tprint \"[-]Error in http://\"+domain\n \n \t\tmatches = re.findall(regex, r, re.MULTILINE)\n-\t\tif matches == []:\n-\t\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n-\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n \n \t\tfor m in matches:\n \t\t\tif domain_written != True:\n", "diff_parsed": {"added": [], "deleted": [[35, "\t\tif matches == []:"], [36, "\t\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'"], [37, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"]]}, "added_lines": 0, "deleted_lines": 3, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 54, "complexity": 0, "token_count": 294}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "8f6994372648ed5e05793455a0d2f9a75d2c8cb7", "msg": "Merge pull request #8 from karelorigin/master\n\nChange regex for more results in javascript_file_extractor.py. Added by Karel Origin. (https://twitter.com/Karel_Origin)", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-15 13:33:47+01:00", "author_timezone": -3600, "committer_date": "2018-01-15 13:33:47+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": true, "parents": ["070c6cdb5e4fb8f83cab053f878771a2c1d048d5", "4db608cf46b0b27ae9e14cda57c1874bc4c5afd5"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 4, "insertions": 1, "lines": 5, "files": 1, "modified_files": [], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "d63747996caf3cc05b12d0fce610af0c8455e799", "msg": "Update install.sh and tools directory to resolve #7 and #9", "author": {"name": "Fauxsys", "email": "fibercipher@gmail.com"}, "committer": {"name": "Fauxsys", "email": "fibercipher@gmail.com"}, "author_date": "2018-01-18 02:35:39-05:00", "author_timezone": 18000, "committer_date": "2018-01-18 02:39:14-05:00", "committer_timezone": 18000, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["8f6994372648ed5e05793455a0d2f9a75d2c8cb7"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 14, "insertions": 30, "lines": 44, "files": 13, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,16 +1,32 @@\n-    dependencies_dir=\"dependencies\"  \n-    dependencies_dir=\"output\"  \n+#!/usr/bin/env bash\n \n+dependencies_dir=\"dependencies\"  \n+output_dir=\"output\"\n \n-    mkdir $output_dir;\n-    mkdir $dependencies_dir; \n-    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n-    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n-    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n-    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n-    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-    cd $dependencies_dir/nmap;\n-    ./configure;\n-    make;\n-    make install;\n-    cd ../../;\n+mkdir $output_dir;\n+mkdir -p $dependencies_dir/phantomjs; \n+   \n+# Install sublis3r   \n+git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n+\n+# Install wpscan\n+git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n+\n+# Install Relative-URL-Extractor\n+git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n+\n+# Install WebScreenShot\n+git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n+\n+# Install PhantomJS\n+wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\n+tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n+ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n+    \n+# Install Nmap\n+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+cd $dependencies_dir/nmap;\n+./configure;\n+make;\n+make install;\n+cd ../../;\n", "diff_parsed": {"added": [[1, "#!/usr/bin/env bash"], [3, "dependencies_dir=\"dependencies\""], [4, "output_dir=\"output\""], [6, "mkdir $output_dir;"], [7, "mkdir -p $dependencies_dir/phantomjs;"], [8, ""], [9, "# Install sublis3r"], [10, "git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister;"], [11, ""], [12, "# Install wpscan"], [13, "git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan;"], [14, ""], [15, "# Install Relative-URL-Extractor"], [16, "git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor;"], [17, ""], [18, "# Install WebScreenShot"], [19, "git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;"], [20, ""], [21, "# Install PhantomJS"], [22, "wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2"], [23, "tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/"], [24, "ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/"], [25, ""], [26, "# Install Nmap"], [27, "git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [28, "cd $dependencies_dir/nmap;"], [29, "./configure;"], [30, "make;"], [31, "make install;"], [32, "cd ../../;"]], "deleted": [[1, "    dependencies_dir=\"dependencies\""], [2, "    dependencies_dir=\"output\""], [5, "    mkdir $output_dir;"], [6, "    mkdir $dependencies_dir;"], [7, "    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister;"], [8, "    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan;"], [9, "    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor;"], [10, "    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot;"], [11, "    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [12, "    cd $dependencies_dir/nmap;"], [13, "    ./configure;"], [14, "    make;"], [15, "    make install;"], [16, "    cd ../../;"]]}, "added_lines": 30, "deleted_lines": 14, "source_code": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n# Install Nmap\ngit clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\ncd $dependencies_dir/nmap;\n./configure;\nmake;\nmake install;\ncd ../../;\n", "source_code_before": "    dependencies_dir=\"dependencies\"  \n    dependencies_dir=\"output\"  \n\n\n    mkdir $output_dir;\n    mkdir $dependencies_dir; \n    git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n    git clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n    git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n    git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n    git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n    cd $dependencies_dir/nmap;\n    ./configure;\n    make;\n    make install;\n    cd ../../;\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/cors_misconfiguration_scan.sh", "new_path": "tools/cors_misconfiguration_scan.sh", "filename": "cors_misconfiguration_scan.sh", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/crlf.sh", "new_path": "tools/crlf.sh", "filename": "crlf.sh", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/error_page_info_check.py", "new_path": "tools/error_page_info_check.py", "filename": "error_page_info_check.py", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/header_scan.py", "new_path": "tools/header_scan.py", "filename": "header_scan.py", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/javascript_files_link_extractor.sh", "new_path": "tools/javascript_files_link_extractor.sh", "filename": "javascript_files_link_extractor.sh", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/nmap_scan.sh", "new_path": "tools/nmap_scan.sh", "filename": "nmap_scan.sh", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/online.py", "new_path": "tools/online.py", "filename": "online.py", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/subdomain_takeover_scan.py", "new_path": "tools/subdomain_takeover_scan.py", "filename": "subdomain_takeover_scan.py", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/wordpress_check.py", "new_path": "tools/wordpress_check.py", "filename": "wordpress_check.py", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}, {"old_path": "tools/wpscan_domains.sh", "new_path": "tools/wpscan_domains.sh", "filename": "wpscan_domains.sh", "change_type": "ModificationType.UNKNOWN", "change_type_name": "UNKNOWN", "diff": "", "diff_parsed": {"added": [], "deleted": []}, "added_lines": 0, "deleted_lines": 0, "source_code": null, "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "0085389cd5c8561a8b0272c8c1aa6a08019f96c1", "msg": "Merge pull request #10 from Fauxsys/master\n\nUpdate install.sh and tools directory to resolve #7 and #9", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 15:06:27+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 15:06:27+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": true, "parents": ["8f6994372648ed5e05793455a0d2f9a75d2c8cb7", "d63747996caf3cc05b12d0fce610af0c8455e799"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 14, "insertions": 30, "lines": 44, "files": 13, "modified_files": [], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "609bce0149a06dbee11353571c532f0a0768493f", "msg": "Update recon.sh\n\ntypo", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 15:07:50+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 15:07:50+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["0085389cd5c8561a8b0272c8c1aa6a08019f96c1"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "recon.sh", "new_path": "recon.sh", "filename": "recon.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,6 +1,6 @@\n     #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n     tput setaf 3;\n-    echo \"Please read throug this script before executing, to prevent unexpected things from happening.\"\n+    echo \"Please read through this script before executing, to prevent unexpected things from happening.\"\n     \n     home_dir=$(pwd)\n \n", "diff_parsed": {"added": [[3, "    echo \"Please read through this script before executing, to prevent unexpected things from happening.\""]], "deleted": [[3, "    echo \"Please read throug this script before executing, to prevent unexpected things from happening.\""]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n    tput setaf 3;\n    echo \"Please read through this script before executing, to prevent unexpected things from happening.\"\n    \n    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    # Uncomment on own risk. this will first clean the old results.\n    #rm -rf $@;\n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "source_code_before": "    #This little script chains all the tools together. please read it through before using, so there wont be any unexpected results.\n    tput setaf 3;\n    echo \"Please read throug this script before executing, to prevent unexpected things from happening.\"\n    \n    home_dir=$(pwd)\n\n    output_dir=\"output\"\n    tools_dir=\"tools\"\n    payloads_dir=\"payloads\"\n    dependencies_dir=\"dependencies\"  \n    screenshots_dir=\"$output_dir/$@/screenshots\"  \n\n\n    all_domains_file=\"$output_dir/$@/domains-all.txt\"\n    domains_file=\"$output_dir/$@/domains.txt\"\n    crlf_file=\"$output_dir/$@/crlf.txt\"\n    open_redirects_file=\"$output_dir/$@/open_redirects.txt\"\n    nmap_scan_file=\"$output_dir/$@/nmap_scans.txt\"\n    wordpress_file=\"$output_dir/$@/wordpress_sites.txt\"\n    headers_file=\"$output_dir/$@/sensitive_headers.txt\"\n    subdomain_take_over_file=\"$output_dir/$@/sub_take_over.txt\"\n    javascript_files_file=\"$output_dir/$@/javascript_files.txt\"\n    javascript_extracted_urls=\"$output_dir/$@/extracted_urls.txt\"\n    error_page_info_file=\"$output_dir/$@/error_page_info.txt\"\n    cors_file=\"$output_dir/$@/misconfigured_cors.txt\"\n\n    crlf_payload_file=\"$payloads_dir/crlf.txt\"\n    error_pages_payload_file=\"$payloads_dir/error_pages.txt\"\n    headers_payload_file=\"$payloads_dir/sensitive_headers.txt\"\n    open_redirect_payload_file=\"$payloads_dir/open_redirects.txt\"\n    \n    wpscan_location=\"dependencies/wpscan\"\n    url_extractor_location=\"dependencies/relative-url-extractor\"\n    sublister_location=\"dependencies/sublister\"\n    webscreenshot_location=\"dependencies/webscreenshot\"\n    nmap_location=\"dependencies/nmap\"\n\n\n    cd $home_dir/$output_dir;\n    # Uncomment on own risk. this will first clean the old results.\n    #rm -rf $@;\n    mkdir $@; \n    cd ../\n\n    printf \"\\n -- $@ Started -- \\n\"\n\n    python $sublister_location/sublist3r.py -o $all_domains_file -d $@;\n    python $tools_dir/online.py $all_domains_file $domains_file;\n    $tools_dir/crlf.sh $domains_file $crlf_file $crlf_payload_file;\n    $tools_dir/cors_misconfiguration_scan.sh $domains_file $cors_file;\n    python $tools_dir/open_redirect.py $domains_file $open_redirects_file $open_redirect_payload_file;\n    python $tools_dir/header_scan.py $domains_file $headers_file $headers_payload_file;\n    python $tools_dir/error_page_info_check.py $domains_file $error_page_info_file $error_pages_payload_file;\n    python $tools_dir/subdomain_takeover_scan.py $domains_file $subdomain_take_over_file;\n    python $tools_dir/javascript_files_extractor.py $domains_file $javascript_files_file;\n    $tools_dir/javascript_files_link_extractor.sh $javascript_files_file $javascript_extracted_urls $url_extractor_location/extract.rb;\n    python $webscreenshot_location/webscreenshot.py -i $domains_file -o $screenshots_dir;\n    python $tools_dir/wordpress_check.py $domains_file $wordpress_file;\n    $wpscan_location/wpscan.rb --update;\n    $tools_dir/wpscan_domains.sh $wordpress_file;\n    $tools_dir/nmap_scan.sh $domains_file $nmap_scan_file $nmap_location;\n    \n    printf \"\\n -- $@ Finished -- \\n\"\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "09b0ac4ed5b56765224aba4895b28aff2f2be2a5", "msg": "Adds additional check before installing nmap. Fixes #5.", "author": {"name": "rewanth1997", "email": "rewanth1997@gmail.com"}, "committer": {"name": "rewanth1997", "email": "rewanth1997@gmail.com"}, "author_date": "2018-01-21 01:57:16+05:30", "author_timezone": -19800, "committer_date": "2018-01-21 01:57:16+05:30", "committer_timezone": -19800, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["609bce0149a06dbee11353571c532f0a0768493f"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 6, "insertions": 15, "lines": 21, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -24,9 +24,18 @@ tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /us\n ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n     \n # Install Nmap\n-git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-cd $dependencies_dir/nmap;\n-./configure;\n-make;\n-make install;\n-cd ../../;\n+# Checks if nmap is already installed on the system,\n+# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n+# If nmap isn't installed then it will clone it from github and gets installed.\n+if [ -x \"$(command -v nmap)\" ]; then\n+  mkdir nmap;\n+  ln -s \"$(command -v nmap)\" nmap/;\n+  cd ../;\n+else\n+  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+  cd $dependencies_dir/nmap;\n+  ./configure;\n+  make;\n+  make install;\n+  cd ../../;\n+fi\n", "diff_parsed": {"added": [[27, "# Checks if nmap is already installed on the system,"], [28, "# If installed, this creates a symbolic link of existing nmap to the dependencies directory."], [29, "# If nmap isn't installed then it will clone it from github and gets installed."], [30, "if [ -x \"$(command -v nmap)\" ]; then"], [31, "  mkdir nmap;"], [32, "  ln -s \"$(command -v nmap)\" nmap/;"], [33, "  cd ../;"], [34, "else"], [35, "  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [36, "  cd $dependencies_dir/nmap;"], [37, "  ./configure;"], [38, "  make;"], [39, "  make install;"], [40, "  cd ../../;"], [41, "fi"]], "deleted": [[27, "git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [28, "cd $dependencies_dir/nmap;"], [29, "./configure;"], [30, "make;"], [31, "make install;"], [32, "cd ../../;"]]}, "added_lines": 15, "deleted_lines": 6, "source_code": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n# Install Nmap\n# Checks if nmap is already installed on the system,\n# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n# If nmap isn't installed then it will clone it from github and gets installed.\nif [ -x \"$(command -v nmap)\" ]; then\n  mkdir nmap;\n  ln -s \"$(command -v nmap)\" nmap/;\n  cd ../;\nelse\n  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n  cd $dependencies_dir/nmap;\n  ./configure;\n  make;\n  make install;\n  cd ../../;\nfi\n", "source_code_before": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n# Install Nmap\ngit clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\ncd $dependencies_dir/nmap;\n./configure;\nmake;\nmake install;\ncd ../../;\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "a35f9cf02ae4a0fb14b6bedde5767cbe6294dc56", "msg": "Merge pull request #11 from rewanth1997/fix#5\n\nAdds additional check before installing nmap. Fixes #5.", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 21:36:07+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 21:36:07+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": true, "parents": ["609bce0149a06dbee11353571c532f0a0768493f", "09b0ac4ed5b56765224aba4895b28aff2f2be2a5"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 6, "insertions": 15, "lines": 21, "files": 1, "modified_files": [], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "119e374313003d9e56da5024f937d1be9d96e794", "msg": "Update install.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 21:41:56+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 21:41:56+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["a35f9cf02ae4a0fb14b6bedde5767cbe6294dc56"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 16, "insertions": 6, "lines": 22, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -23,19 +23,9 @@ wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_6\n tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n     \n-# Install Nmap\n-# Checks if nmap is already installed on the system,\n-# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n-# If nmap isn't installed then it will clone it from github and gets installed.\n-if [ -x \"$(command -v nmap)\" ]; then\n-  mkdir nmap;\n-  ln -s \"$(command -v nmap)\" nmap/;\n-  cd ../;\n-else\n-  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-  cd $dependencies_dir/nmap;\n-  ./configure;\n-  make;\n-  make install;\n-  cd ../../;\n-fi\n+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+cd $dependencies_dir/nmap;\n+./configure;\n+make;\n+make install;\n+cd ../../;\n", "diff_parsed": {"added": [[26, "git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [27, "cd $dependencies_dir/nmap;"], [28, "./configure;"], [29, "make;"], [30, "make install;"], [31, "cd ../../;"]], "deleted": [[26, "# Install Nmap"], [27, "# Checks if nmap is already installed on the system,"], [28, "# If installed, this creates a symbolic link of existing nmap to the dependencies directory."], [29, "# If nmap isn't installed then it will clone it from github and gets installed."], [30, "if [ -x \"$(command -v nmap)\" ]; then"], [31, "  mkdir nmap;"], [32, "  ln -s \"$(command -v nmap)\" nmap/;"], [33, "  cd ../;"], [34, "else"], [35, "  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [36, "  cd $dependencies_dir/nmap;"], [37, "  ./configure;"], [38, "  make;"], [39, "  make install;"], [40, "  cd ../../;"], [41, "fi"]]}, "added_lines": 6, "deleted_lines": 16, "source_code": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \ngit clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\ncd $dependencies_dir/nmap;\n./configure;\nmake;\nmake install;\ncd ../../;\n", "source_code_before": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n# Install Nmap\n# Checks if nmap is already installed on the system,\n# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n# If nmap isn't installed then it will clone it from github and gets installed.\nif [ -x \"$(command -v nmap)\" ]; then\n  mkdir nmap;\n  ln -s \"$(command -v nmap)\" nmap/;\n  cd ../;\nelse\n  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n  cd $dependencies_dir/nmap;\n  ./configure;\n  make;\n  make install;\n  cd ../../;\nfi\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "7fdf1e09e16b72128166fe14696b62f05a8b0ea8", "msg": "Update install.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 21:46:00+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 21:46:00+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["119e374313003d9e56da5024f937d1be9d96e794"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 6, "insertions": 22, "lines": 28, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -23,9 +23,25 @@ wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_6\n tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n     \n-git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-cd $dependencies_dir/nmap;\n-./configure;\n-make;\n-make install;\n-cd ../../;\n+-# Install Nmap\n++git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+-# Checks if nmap is already installed on the system,\n++cd $dependencies_dir/nmap;\n+-# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n++./configure;\n+-# If nmap isn't installed then it will clone it from github and gets installed.\n++make;\n+-if [ -x \"$(command -v nmap)\" ]; then\n++make install;\n+-  mkdir nmap;\n++cd ../../;\n+-  ln -s \"$(command -v nmap)\" nmap/;\n+-  cd ../;\n+-else\n+-  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+-  cd $dependencies_dir/nmap;\n+-  ./configure;\n+-  make;\n+-  make install;\n+-  cd ../../;\n+-fi\n", "diff_parsed": {"added": [[26, "-# Install Nmap"], [27, "+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [28, "-# Checks if nmap is already installed on the system,"], [29, "+cd $dependencies_dir/nmap;"], [30, "-# If installed, this creates a symbolic link of existing nmap to the dependencies directory."], [31, "+./configure;"], [32, "-# If nmap isn't installed then it will clone it from github and gets installed."], [33, "+make;"], [34, "-if [ -x \"$(command -v nmap)\" ]; then"], [35, "+make install;"], [36, "-  mkdir nmap;"], [37, "+cd ../../;"], [38, "-  ln -s \"$(command -v nmap)\" nmap/;"], [39, "-  cd ../;"], [40, "-else"], [41, "-  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [42, "-  cd $dependencies_dir/nmap;"], [43, "-  ./configure;"], [44, "-  make;"], [45, "-  make install;"], [46, "-  cd ../../;"], [47, "-fi"]], "deleted": [[26, "git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [27, "cd $dependencies_dir/nmap;"], [28, "./configure;"], [29, "make;"], [30, "make install;"], [31, "cd ../../;"]]}, "added_lines": 22, "deleted_lines": 6, "source_code": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n-# Install Nmap\n+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-# Checks if nmap is already installed on the system,\n+cd $dependencies_dir/nmap;\n-# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n+./configure;\n-# If nmap isn't installed then it will clone it from github and gets installed.\n+make;\n-if [ -x \"$(command -v nmap)\" ]; then\n+make install;\n-  mkdir nmap;\n+cd ../../;\n-  ln -s \"$(command -v nmap)\" nmap/;\n-  cd ../;\n-else\n-  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-  cd $dependencies_dir/nmap;\n-  ./configure;\n-  make;\n-  make install;\n-  cd ../../;\n-fi\n", "source_code_before": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \ngit clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\ncd $dependencies_dir/nmap;\n./configure;\nmake;\nmake install;\ncd ../../;\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "d0d365a8528814d6395f04e60baacb6da74ac96c", "msg": "Update install.sh", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 21:49:47+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 21:49:47+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["7fdf1e09e16b72128166fe14696b62f05a8b0ea8"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 26, "insertions": 11, "lines": 37, "files": 1, "modified_files": [{"old_path": "install.sh", "new_path": "install.sh", "filename": "install.sh", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -4,7 +4,7 @@ dependencies_dir=\"dependencies\"\n output_dir=\"output\"\n \n mkdir $output_dir;\n-mkdir -p $dependencies_dir/phantomjs; \n+#mkdir -p $dependencies_dir/phantomjs; \n    \n # Install sublis3r   \n git clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n@@ -19,29 +19,14 @@ git clone https://github.com/jobertabma/relative-url-extractor.git $dependencies\n git clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n \n # Install PhantomJS\n-wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\n-tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n-ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n+#wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\n+#tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n+#ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n     \n--# Install Nmap\n-+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n--# Checks if nmap is already installed on the system,\n-+cd $dependencies_dir/nmap;\n--# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n-+./configure;\n--# If nmap isn't installed then it will clone it from github and gets installed.\n-+make;\n--if [ -x \"$(command -v nmap)\" ]; then\n-+make install;\n--  mkdir nmap;\n-+cd ../../;\n--  ln -s \"$(command -v nmap)\" nmap/;\n--  cd ../;\n--else\n--  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n--  cd $dependencies_dir/nmap;\n--  ./configure;\n--  make;\n--  make install;\n--  cd ../../;\n--fi\n+ # Install Nmap\n+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n+cd $dependencies_dir/nmap;\n+./configure;\n+make;\n+make install;\n+cd ../../;\n", "diff_parsed": {"added": [[7, "#mkdir -p $dependencies_dir/phantomjs;"], [22, "#wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2"], [23, "#tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/"], [24, "#ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/"], [26, " # Install Nmap"], [27, "git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [28, "cd $dependencies_dir/nmap;"], [29, "./configure;"], [30, "make;"], [31, "make install;"], [32, "cd ../../;"]], "deleted": [[7, "mkdir -p $dependencies_dir/phantomjs;"], [22, "wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2"], [23, "tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/"], [24, "ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/"], [26, "-# Install Nmap"], [27, "+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [28, "-# Checks if nmap is already installed on the system,"], [29, "+cd $dependencies_dir/nmap;"], [30, "-# If installed, this creates a symbolic link of existing nmap to the dependencies directory."], [31, "+./configure;"], [32, "-# If nmap isn't installed then it will clone it from github and gets installed."], [33, "+make;"], [34, "-if [ -x \"$(command -v nmap)\" ]; then"], [35, "+make install;"], [36, "-  mkdir nmap;"], [37, "+cd ../../;"], [38, "-  ln -s \"$(command -v nmap)\" nmap/;"], [39, "-  cd ../;"], [40, "-else"], [41, "-  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap"], [42, "-  cd $dependencies_dir/nmap;"], [43, "-  ./configure;"], [44, "-  make;"], [45, "-  make install;"], [46, "-  cd ../../;"], [47, "-fi"]]}, "added_lines": 11, "deleted_lines": 26, "source_code": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\n#mkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\n#wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\n#tar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n#ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n # Install Nmap\ngit clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\ncd $dependencies_dir/nmap;\n./configure;\nmake;\nmake install;\ncd ../../;\n", "source_code_before": "#!/usr/bin/env bash\n\ndependencies_dir=\"dependencies\"  \noutput_dir=\"output\"\n\nmkdir $output_dir;\nmkdir -p $dependencies_dir/phantomjs; \n   \n# Install sublis3r   \ngit clone https://github.com/aboul3la/Sublist3r.git $dependencies_dir/sublister; \n\n# Install wpscan\ngit clone https://github.com/wpscanteam/wpscan.git $dependencies_dir/wpscan; \n\n# Install Relative-URL-Extractor\ngit clone https://github.com/jobertabma/relative-url-extractor.git $dependencies_dir/relative-url-extractor; \n\n# Install WebScreenShot\ngit clone https://github.com/maaaaz/webscreenshot.git $dependencies_dir/webscreenshot; \n\n# Install PhantomJS\nwget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 -O $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2\ntar xvjf $dependencies_dir/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\nln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/\n    \n-# Install Nmap\n+git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-# Checks if nmap is already installed on the system,\n+cd $dependencies_dir/nmap;\n-# If installed, this creates a symbolic link of existing nmap to the dependencies directory.\n+./configure;\n-# If nmap isn't installed then it will clone it from github and gets installed.\n+make;\n-if [ -x \"$(command -v nmap)\" ]; then\n+make install;\n-  mkdir nmap;\n+cd ../../;\n-  ln -s \"$(command -v nmap)\" nmap/;\n-  cd ../;\n-else\n-  git clone https://github.com/nmap/nmap.git $dependencies_dir/nmap\n-  cd $dependencies_dir/nmap;\n-  ./configure;\n-  make;\n-  make install;\n-  cd ../../;\n-fi\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "aa1d008cb8bf7858ad98df96df0c94deac40b337", "msg": "fixed regex error, credits to Karel Origin", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-01-20 22:47:47+01:00", "author_timezone": -3600, "committer_date": "2018-01-20 22:47:47+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["d0d365a8528814d6395f04e60baacb6da74ac96c"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 1, "insertions": 1, "lines": 2, "files": 1, "modified_files": [{"old_path": "tools/javascript_files_extractor.py", "new_path": "tools/javascript_files_extractor.py", "filename": "javascript_files_extractor.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -25,7 +25,7 @@ for domain in domains:\n \tif domain != \"\":\n \t\tmatches = \"\"\n \t\tr = \"\"\n-\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n+\t\tregex = r'<script.*src=[\\'|\"]([^\\'\"]*)[\\'|\"]'\n \t\ttry:\n \t\t\tr = requests.get(\"http://\"+domain).content\n \t\texcept:\n", "diff_parsed": {"added": [[28, "\t\tregex = r'<script.*src=[\\'|\"]([^\\'\"]*)[\\'|\"]'"]], "deleted": [[28, "\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'"]]}, "added_lines": 1, "deleted_lines": 1, "source_code": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"]([^\\'\"]*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport re, requests, sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\nprint(\"\\n-- Extracting javascript files from domains in \"+input_file+\" with output file, \"+output_file+\" --\\n\")\n\ndomains_file = open(input_file,'r')\ndomains = domains_file.read().split('\\n')\n\nfile = open(output_file,\"w+\")\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nfor domain in domains:\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'<script.*src=[\\'|\"](.*)[\\'|\"]'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tif domain_written != True:\n\t\t\t\tfile.write(\"\\n-\"+domain+\"\\n\")\n\t\t\t\tdomain_written = True\n\t\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tfile.write(full_domain+\"\\n\")\n\t\tprint \"[+]\"+str(i)+\" scripts, \"+str(b_amount)+\" blacklisted in \"+domain\n\telse:\n\t\tprint \"[-]Domain is invalid \" + domain\n\n\n\nfile.close()\ndomains_file.close()\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": 54, "complexity": 0, "token_count": 294}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "96a3778b817d1ab2bed403b66892f54c674b90df", "msg": "Delete javascript_files_and_links_extractor.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-03-22 10:15:11+01:00", "author_timezone": -3600, "committer_date": "2018-03-22 10:15:11+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["aa1d008cb8bf7858ad98df96df0c94deac40b337"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 64, "insertions": 0, "lines": 64, "files": 1, "modified_files": [{"old_path": "edited_tools/javascript_files_and_links_extractor.py", "new_path": null, "filename": "javascript_files_and_links_extractor.py", "change_type": "ModificationType.DELETE", "change_type_name": "DELETE", "diff": "@@ -1,64 +0,0 @@\n-#!/usr/bin/python\n-#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh\n-#it combines those 2 and takes 1 domain as string as argument 1. not a domain file\n-\n-import re, requests, sys, os\n-\n-input_string = sys.argv[1]\n-output_file = sys.argv[2]\n-extractor_file = sys.argv[3]\n-\n-print(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n-\n-\n-black_listed_domains = [\"ajax.googleapis.com\",\n-                        \"cdn.optimizely.com\",\n-                        \"googletagmanager.com\",\n-                        \"fontawesome.com\"]\n-\n-if input_string is not \"\":\n-\tdomain = input_string\n-\tdomain_written = False\n-\ti = 0\n-\tb_amount = 0\n-\tfull_domain = \"\"\n-\tif domain != \"\":\n-\t\tmatches = \"\"\n-\t\tr = \"\"\n-\t\tregex = r'script src=\"(.*?)\"'\n-\t\ttry:\n-\t\t\tr = requests.get(\"http://\"+domain).content\n-\t\texcept:\n-\t\t\tprint \"[-]Error in http://\"+domain\n-\n-\t\tmatches = re.findall(regex, r, re.MULTILINE)\n-\t\tif matches == []:\n-\t\t\tregex = r\"script src='(.*?)'\"\n-\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n-\n-\t\tfor m in matches:\n-\t\t\tblack_listed = False\n-\t\t\tfor b in black_listed_domains:\n-\t\t\t\tif b in m:\n-\t\t\t\t\tblack_listed = True\n-\n-\t\t\tif black_listed != True:\n-\t\t\t\tif m.startswith(\"/\"):\n-\t\t\t\t\tif m.startswith(\"//\"):\n-\t\t\t\t\t\tfull_domain = \"https:\"+m\n-\t\t\t\t\telse:\n-\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n-\t\t\t\telif m.startswith(\"http\"):\n-\t\t\t\t\tfull_domain = m\n-\t\t\t\telse:\n-\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n-\t\t\telse:\n-\t\t\t\tb_amount += 1\n-\n-\t\t\tif black_listed != True:\n-\t\t\t\ti += 1\n-\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n-\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n-\n-\n-print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [], "deleted": [[1, "#!/usr/bin/python"], [2, "#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh"], [3, "#it combines those 2 and takes 1 domain as string as argument 1. not a domain file"], [4, ""], [5, "import re, requests, sys, os"], [6, ""], [7, "input_string = sys.argv[1]"], [8, "output_file = sys.argv[2]"], [9, "extractor_file = sys.argv[3]"], [10, ""], [11, "print(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")"], [12, ""], [13, ""], [14, "black_listed_domains = [\"ajax.googleapis.com\","], [15, "                        \"cdn.optimizely.com\","], [16, "                        \"googletagmanager.com\","], [17, "                        \"fontawesome.com\"]"], [18, ""], [19, "if input_string is not \"\":"], [20, "\tdomain = input_string"], [21, "\tdomain_written = False"], [22, "\ti = 0"], [23, "\tb_amount = 0"], [24, "\tfull_domain = \"\""], [25, "\tif domain != \"\":"], [26, "\t\tmatches = \"\""], [27, "\t\tr = \"\""], [28, "\t\tregex = r'script src=\"(.*?)\"'"], [29, "\t\ttry:"], [30, "\t\t\tr = requests.get(\"http://\"+domain).content"], [31, "\t\texcept:"], [32, "\t\t\tprint \"[-]Error in http://\"+domain"], [33, ""], [34, "\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [35, "\t\tif matches == []:"], [36, "\t\t\tregex = r\"script src='(.*?)'\""], [37, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [38, ""], [39, "\t\tfor m in matches:"], [40, "\t\t\tblack_listed = False"], [41, "\t\t\tfor b in black_listed_domains:"], [42, "\t\t\t\tif b in m:"], [43, "\t\t\t\t\tblack_listed = True"], [44, ""], [45, "\t\t\tif black_listed != True:"], [46, "\t\t\t\tif m.startswith(\"/\"):"], [47, "\t\t\t\t\tif m.startswith(\"//\"):"], [48, "\t\t\t\t\t\tfull_domain = \"https:\"+m"], [49, "\t\t\t\t\telse:"], [50, "\t\t\t\t\t\tfull_domain = \"https://\"+domain+m"], [51, "\t\t\t\telif m.startswith(\"http\"):"], [52, "\t\t\t\t\tfull_domain = m"], [53, "\t\t\t\telse:"], [54, "\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m"], [55, "\t\t\telse:"], [56, "\t\t\t\tb_amount += 1"], [57, ""], [58, "\t\t\tif black_listed != True:"], [59, "\t\t\t\ti += 1"], [60, "\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)"], [61, "\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)"], [62, ""], [63, ""], [64, "print(\"\\n-- Done --\")"]]}, "added_lines": 0, "deleted_lines": 64, "source_code": null, "source_code_before": "#!/usr/bin/python\n#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh\n#it combines those 2 and takes 1 domain as string as argument 1. not a domain file\n\nimport re, requests, sys, os\n\ninput_string = sys.argv[1]\noutput_file = sys.argv[2]\nextractor_file = sys.argv[3]\n\nprint(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nif input_string is not \"\":\n\tdomain = input_string\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n\n\nprint(\"\\n-- Done --\")\n", "methods": [], "methods_before": [], "changed_methods": [], "nloc": null, "complexity": null, "token_count": null}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "29425cab36a865a16f342081e11742e29076058a", "msg": "Create files_and_links_extractor.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-03-22 10:16:26+01:00", "author_timezone": -3600, "committer_date": "2018-03-22 10:16:26+01:00", "committer_timezone": -3600, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["96a3778b817d1ab2bed403b66892f54c674b90df"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 0, "insertions": 64, "lines": 64, "files": 1, "modified_files": [{"old_path": null, "new_path": "tools/edited_tools/files_and_links_extractor.py", "filename": "files_and_links_extractor.py", "change_type": "ModificationType.ADD", "change_type_name": "ADD", "diff": "@@ -0,0 +1,64 @@\n+#!/usr/bin/python\n+#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh\n+#it combines those 2 and takes 1 domain as string as argument 1. not a domain file\n+\n+import re, requests, sys, os\n+\n+input_string = sys.argv[1]\n+output_file = sys.argv[2]\n+extractor_file = sys.argv[3]\n+\n+print(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n+\n+\n+black_listed_domains = [\"ajax.googleapis.com\",\n+                        \"cdn.optimizely.com\",\n+                        \"googletagmanager.com\",\n+                        \"fontawesome.com\"]\n+\n+if input_string is not \"\":\n+\tdomain = input_string\n+\tdomain_written = False\n+\ti = 0\n+\tb_amount = 0\n+\tfull_domain = \"\"\n+\tif domain != \"\":\n+\t\tmatches = \"\"\n+\t\tr = \"\"\n+\t\tregex = r'script src=\"(.*?)\"'\n+\t\ttry:\n+\t\t\tr = requests.get(\"http://\"+domain).content\n+\t\texcept:\n+\t\t\tprint \"[-]Error in http://\"+domain\n+\n+\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\t\tif matches == []:\n+\t\t\tregex = r\"script src='(.*?)'\"\n+\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n+\n+\t\tfor m in matches:\n+\t\t\tblack_listed = False\n+\t\t\tfor b in black_listed_domains:\n+\t\t\t\tif b in m:\n+\t\t\t\t\tblack_listed = True\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\tif m.startswith(\"/\"):\n+\t\t\t\t\tif m.startswith(\"//\"):\n+\t\t\t\t\t\tfull_domain = \"https:\"+m\n+\t\t\t\t\telse:\n+\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n+\t\t\t\telif m.startswith(\"http\"):\n+\t\t\t\t\tfull_domain = m\n+\t\t\t\telse:\n+\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n+\t\t\telse:\n+\t\t\t\tb_amount += 1\n+\n+\t\t\tif black_listed != True:\n+\t\t\t\ti += 1\n+\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n+\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n+\n+\n+print(\"\\n-- Done --\")\n", "diff_parsed": {"added": [[1, "#!/usr/bin/python"], [2, "#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh"], [3, "#it combines those 2 and takes 1 domain as string as argument 1. not a domain file"], [4, ""], [5, "import re, requests, sys, os"], [6, ""], [7, "input_string = sys.argv[1]"], [8, "output_file = sys.argv[2]"], [9, "extractor_file = sys.argv[3]"], [10, ""], [11, "print(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")"], [12, ""], [13, ""], [14, "black_listed_domains = [\"ajax.googleapis.com\","], [15, "                        \"cdn.optimizely.com\","], [16, "                        \"googletagmanager.com\","], [17, "                        \"fontawesome.com\"]"], [18, ""], [19, "if input_string is not \"\":"], [20, "\tdomain = input_string"], [21, "\tdomain_written = False"], [22, "\ti = 0"], [23, "\tb_amount = 0"], [24, "\tfull_domain = \"\""], [25, "\tif domain != \"\":"], [26, "\t\tmatches = \"\""], [27, "\t\tr = \"\""], [28, "\t\tregex = r'script src=\"(.*?)\"'"], [29, "\t\ttry:"], [30, "\t\t\tr = requests.get(\"http://\"+domain).content"], [31, "\t\texcept:"], [32, "\t\t\tprint \"[-]Error in http://\"+domain"], [33, ""], [34, "\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [35, "\t\tif matches == []:"], [36, "\t\t\tregex = r\"script src='(.*?)'\""], [37, "\t\t\tmatches = re.findall(regex, r, re.MULTILINE)"], [38, ""], [39, "\t\tfor m in matches:"], [40, "\t\t\tblack_listed = False"], [41, "\t\t\tfor b in black_listed_domains:"], [42, "\t\t\t\tif b in m:"], [43, "\t\t\t\t\tblack_listed = True"], [44, ""], [45, "\t\t\tif black_listed != True:"], [46, "\t\t\t\tif m.startswith(\"/\"):"], [47, "\t\t\t\t\tif m.startswith(\"//\"):"], [48, "\t\t\t\t\t\tfull_domain = \"https:\"+m"], [49, "\t\t\t\t\telse:"], [50, "\t\t\t\t\t\tfull_domain = \"https://\"+domain+m"], [51, "\t\t\t\telif m.startswith(\"http\"):"], [52, "\t\t\t\t\tfull_domain = m"], [53, "\t\t\t\telse:"], [54, "\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m"], [55, "\t\t\telse:"], [56, "\t\t\t\tb_amount += 1"], [57, ""], [58, "\t\t\tif black_listed != True:"], [59, "\t\t\t\ti += 1"], [60, "\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)"], [61, "\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)"], [62, ""], [63, ""], [64, "print(\"\\n-- Done --\")"]], "deleted": []}, "added_lines": 64, "deleted_lines": 0, "source_code": "#!/usr/bin/python\n#This is an edit of /tools/javascript_files_extractor.py and /tools/javascript_files_link_extractor.sh\n#it combines those 2 and takes 1 domain as string as argument 1. not a domain file\n\nimport re, requests, sys, os\n\ninput_string = sys.argv[1]\noutput_file = sys.argv[2]\nextractor_file = sys.argv[3]\n\nprint(\"\\n-- Extracting javascript links from \"+input_string+\" with output file, \"+output_file+\" --\\n\")\n\n\nblack_listed_domains = [\"ajax.googleapis.com\",\n                        \"cdn.optimizely.com\",\n                        \"googletagmanager.com\",\n                        \"fontawesome.com\"]\n\nif input_string is not \"\":\n\tdomain = input_string\n\tdomain_written = False\n\ti = 0\n\tb_amount = 0\n\tfull_domain = \"\"\n\tif domain != \"\":\n\t\tmatches = \"\"\n\t\tr = \"\"\n\t\tregex = r'script src=\"(.*?)\"'\n\t\ttry:\n\t\t\tr = requests.get(\"http://\"+domain).content\n\t\texcept:\n\t\t\tprint \"[-]Error in http://\"+domain\n\n\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\t\tif matches == []:\n\t\t\tregex = r\"script src='(.*?)'\"\n\t\t\tmatches = re.findall(regex, r, re.MULTILINE)\n\n\t\tfor m in matches:\n\t\t\tblack_listed = False\n\t\t\tfor b in black_listed_domains:\n\t\t\t\tif b in m:\n\t\t\t\t\tblack_listed = True\n\n\t\t\tif black_listed != True:\n\t\t\t\tif m.startswith(\"/\"):\n\t\t\t\t\tif m.startswith(\"//\"):\n\t\t\t\t\t\tfull_domain = \"https:\"+m\n\t\t\t\t\telse:\n\t\t\t\t\t\tfull_domain = \"https://\"+domain+m\n\t\t\t\telif m.startswith(\"http\"):\n\t\t\t\t\tfull_domain = m\n\t\t\t\telse:\n\t\t\t\t\tfull_domain = \"https://\"+domain+\"/\"+m\n\t\t\telse:\n\t\t\t\tb_amount += 1\n\n\t\t\tif black_listed != True:\n\t\t\t\ti += 1\n\t\t\t\tos.system(\"echo '\"+full_domain + \": \\n\\r ' >> \" + output_file)\n\t\t\t\tos.system(\"ruby \" + extractor_file + \" \" + full_domain + \" >> \" + output_file)\n\n\nprint(\"\\n-- Done --\")\n", "source_code_before": null, "methods": [], "methods_before": [], "changed_methods": [], "nloc": 49, "complexity": 0, "token_count": 272}], "dmm_unit_size": null, "dmm_unit_complexity": null, "dmm_unit_interfacing": null},
    {"hash": "e3f811730e31731743675272b3228715e7dd79ae", "msg": "Update online.py", "author": {"name": "003random", "email": "003random@protonmail.com"}, "committer": {"name": "GitHub", "email": "noreply@github.com"}, "author_date": "2018-06-05 18:24:33+00:00", "author_timezone": 0, "committer_date": "2018-06-05 18:24:33+00:00", "committer_timezone": 0, "branches": "{'master'}", "in_main_branch": true, "merge": false, "parents": ["29425cab36a865a16f342081e11742e29076058a"], "project_name": "003Recon", "project_path": "/Volumes/nVME1T/Py/GitIn/output/temp/003random_all_20250523_172558/35395dd7/35395dd7/35395dd7/003Recon", "deletions": 27, "insertions": 20, "lines": 47, "files": 1, "modified_files": [{"old_path": "tools/online.py", "new_path": "tools/online.py", "filename": "online.py", "change_type": "ModificationType.MODIFY", "change_type_name": "MODIFY", "diff": "@@ -1,26 +1,6 @@\n #!/usr/bin/python\n \n-import httplib\n-import socket\n-import re\n-import sys\n-\n-def online(host):\n-    try:\n-        socket.gethostbyname(host)\n-    except socket.gaierror:\n-        return False\n-    else:\n-        return True\n-\n-def available(host, path=\"/\"):\n-    try:\n-        conn = httplib.HTTPConnection(host, timeout=5)\n-        conn.request(\"HEAD\", path)\n-        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n-            return True\n-    except StandardError:\n-        return False\n+import sys, requests\n \n input_file = sys.argv[1]\n output_file = sys.argv[2]\n@@ -32,13 +12,26 @@ domains = input_file_open.readlines()\n \n print(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n \n+\n+def available(domain):\n+        try:\n+                r = requests.get(domain, timeout=3)\n+                return True\n+        except:\n+                return False\n+\n for domain in domains:\n-    domain = domain.strip()\n-    if online(domain) == True and available(domain) == True:\n-        print(\"[+]\"+domain.strip())\n-        output_file_open.write(domain+\"\\n\")\n-    else:\n-        print(\"[-]\"+domain)\n+        domain = domain.strip()\n+\n+        http = available(\"http://\" + domain)\n+        https = available(\"https://\" + domain)\n+\n+        if http == True or https == True:\n+                print(\"[+]\" + domain.strip())\n+                output_file_open.write(domain+\"\\n\")\n+        else:\n+                print(\"[-]\" + domain.strip())\n+\n \n input_file_open.close()\n output_file_open.close()\n", "diff_parsed": {"added": [[3, "import sys, requests"], [15, ""], [16, "def available(domain):"], [17, "        try:"], [18, "                r = requests.get(domain, timeout=3)"], [19, "                return True"], [20, "        except:"], [21, "                return False"], [22, ""], [24, "        domain = domain.strip()"], [25, ""], [26, "        http = available(\"http://\" + domain)"], [27, "        https = available(\"https://\" + domain)"], [28, ""], [29, "        if http == True or https == True:"], [30, "                print(\"[+]\" + domain.strip())"], [31, "                output_file_open.write(domain+\"\\n\")"], [32, "        else:"], [33, "                print(\"[-]\" + domain.strip())"], [34, ""]], "deleted": [[3, "import httplib"], [4, "import socket"], [5, "import re"], [6, "import sys"], [7, ""], [8, "def online(host):"], [9, "    try:"], [10, "        socket.gethostbyname(host)"], [11, "    except socket.gaierror:"], [12, "        return False"], [13, "    else:"], [14, "        return True"], [15, ""], [16, "def available(host, path=\"/\"):"], [17, "    try:"], [18, "        conn = httplib.HTTPConnection(host, timeout=5)"], [19, "        conn.request(\"HEAD\", path)"], [20, "        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):"], [21, "            return True"], [22, "    except StandardError:"], [23, "        return False"], [36, "    domain = domain.strip()"], [37, "    if online(domain) == True and available(domain) == True:"], [38, "        print(\"[+]\"+domain.strip())"], [39, "        output_file_open.write(domain+\"\\n\")"], [40, "    else:"], [41, "        print(\"[-]\"+domain)"]]}, "added_lines": 20, "deleted_lines": 27, "source_code": "#!/usr/bin/python\n\nimport sys, requests\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\ninput_file_open = open(input_file, 'r')\noutput_file_open = open(output_file, 'w+')\n\ndomains = input_file_open.readlines()\n\nprint(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n\n\ndef available(domain):\n        try:\n                r = requests.get(domain, timeout=3)\n                return True\n        except:\n                return False\n\nfor domain in domains:\n        domain = domain.strip()\n\n        http = available(\"http://\" + domain)\n        https = available(\"https://\" + domain)\n\n        if http == True or https == True:\n                print(\"[+]\" + domain.strip())\n                output_file_open.write(domain+\"\\n\")\n        else:\n                print(\"[-]\" + domain.strip())\n\n\ninput_file_open.close()\noutput_file_open.close()\nprint(\"\\n-- Done --\")\n", "source_code_before": "#!/usr/bin/python\n\nimport httplib\nimport socket\nimport re\nimport sys\n\ndef online(host):\n    try:\n        socket.gethostbyname(host)\n    except socket.gaierror:\n        return False\n    else:\n        return True\n\ndef available(host, path=\"/\"):\n    try:\n        conn = httplib.HTTPConnection(host, timeout=5)\n        conn.request(\"HEAD\", path)\n        if re.match(\"^[23]\\d\\d$\", str(conn.getresponse().status)):\n            return True\n    except StandardError:\n        return False\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\ninput_file_open = open(input_file, 'r')\noutput_file_open = open(output_file, 'w+')\n\ndomains = input_file_open.readlines()\n\nprint(\"\\n-- Writing online hosts in \"+input_file+\" to \"+output_file+\" --\\n\")\n\nfor domain in domains:\n    domain = domain.strip()\n    if online(domain) == True and available(domain) == True:\n        print(\"[+]\"+domain.strip())\n        output_file_open.write(domain+\"\\n\")\n    else:\n        print(\"[-]\"+domain)\n\ninput_file_open.close()\noutput_file_open.close()\nprint(\"\\n-- Done --\")\n", "methods": [{"name": "available", "start_line": 16, "end_line": 21}], "methods_before": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 23}], "changed_methods": [{"name": "online", "start_line": 8, "end_line": 14}, {"name": "available", "start_line": 16, "end_line": 21}, {"name": "available", "start_line": 16, "end_line": 23}], "nloc": 25, "complexity": 2, "token_count": 162}], "dmm_unit_size": 0.0, "dmm_unit_complexity": 0.0, "dmm_unit_interfacing": 0.0}
  ],
  "process_metrics": {
  "Week_1_2017-11-20": {
    "productivity": {
      "change_set": {
        "max": 17,
        "avg": 1.5925925925925926
      },
      "commits_count": {
        "null": 13,
        "payloads/crlf.txt": 1,
        "payloads/error_pages.txt": 1,
        "payloads/open_redirects.txt": 1,
        "payloads/sensitive_headers.txt": 2,
        "recon.sh": 5,
        "tools/cors_misconfiguration_scan.sh": 1,
        "tools/crlf.sh": 1,
        "tools/error_page_info_check.py": 1,
        "tools/header_scan.py": 2,
        "tools/javascript_files_extractor.py": 3,
        "tools/javascript_files_link_extractor.sh": 1,
        "tools/nmap_scan.sh": 1,
        "tools/online.py": 1,
        "tools/open_redirect.py": 1,
        "tools/subdomain_takeover_scan.py": 1,
        "tools/wordpress_check.py": 1,
        "tools/wpscan_domains.sh": 1,
        "README.md": 1,
        "install.sh": 2,
        "output/New Text Document.txt": 1,
        "output/.gitignore": 1
      },
      "contributors_count": {
        "total": {
          "null": 1,
          "payloads/crlf.txt": 1,
          "payloads/error_pages.txt": 1,
          "payloads/open_redirects.txt": 1,
          "payloads/sensitive_headers.txt": 1,
          "recon.sh": 1,
          "tools/cors_misconfiguration_scan.sh": 1,
          "tools/crlf.sh": 1,
          "tools/error_page_info_check.py": 1,
          "tools/header_scan.py": 1,
          "tools/javascript_files_extractor.py": 1,
          "tools/javascript_files_link_extractor.sh": 1,
          "tools/nmap_scan.sh": 1,
          "tools/online.py": 1,
          "tools/open_redirect.py": 1,
          "tools/subdomain_takeover_scan.py": 1,
          "tools/wordpress_check.py": 1,
          "tools/wpscan_domains.sh": 1,
          "README.md": 1,
          "install.sh": 1,
          "output/New Text Document.txt": 1,
          "output/.gitignore": 1
        },
        "minor": {
          "null": 0,
          "payloads/crlf.txt": 0,
          "payloads/error_pages.txt": 0,
          "payloads/open_redirects.txt": 0,
          "payloads/sensitive_headers.txt": 0,
          "recon.sh": 0,
          "tools/cors_misconfiguration_scan.sh": 0,
          "tools/crlf.sh": 0,
          "tools/error_page_info_check.py": 0,
          "tools/header_scan.py": 0,
          "tools/javascript_files_extractor.py": 0,
          "tools/javascript_files_link_extractor.sh": 0,
          "tools/nmap_scan.sh": 0,
          "tools/online.py": 0,
          "tools/open_redirect.py": 0,
          "tools/subdomain_takeover_scan.py": 0,
          "tools/wordpress_check.py": 0,
          "tools/wpscan_domains.sh": 0,
          "README.md": 0,
          "install.sh": 0,
          "output/New Text Document.txt": 0,
          "output/.gitignore": 0
        }
      },
      "contributors_experience": {
        "null": 1,
        "payloads/crlf.txt": 1,
        "payloads/error_pages.txt": 1,
        "payloads/open_redirects.txt": 1,
        "payloads/sensitive_headers.txt": 1,
        "recon.sh": 1,
        "tools/cors_misconfiguration_scan.sh": 1,
        "tools/crlf.sh": 1,
        "tools/error_page_info_check.py": 1,
        "tools/header_scan.py": 1,
        "tools/javascript_files_extractor.py": 1,
        "tools/javascript_files_link_extractor.sh": 1,
        "tools/nmap_scan.sh": 1,
        "tools/online.py": 1,
        "tools/open_redirect.py": 1,
        "tools/subdomain_takeover_scan.py": 1,
        "tools/wordpress_check.py": 1,
        "tools/wpscan_domains.sh": 1,
        "README.md": 1,
        "install.sh": 1,
        "output/New Text Document.txt": 1,
        "output/.gitignore": 1
      },
      "hunks_count": {
        "null": 1,
        "payloads/crlf.txt": 1,
        "payloads/error_pages.txt": 1,
        "payloads/open_redirects.txt": 1,
        "payloads/sensitive_headers.txt": 1.0,
        "recon.sh": 1,
        "tools/cors_misconfiguration_scan.sh": 1,
        "tools/crlf.sh": 1,
        "tools/error_page_info_check.py": 1,
        "tools/header_scan.py": 1.0,
        "tools/javascript_files_extractor.py": 1,
        "tools/javascript_files_link_extractor.sh": 1,
        "tools/nmap_scan.sh": 1,
        "tools/online.py": 1,
        "tools/open_redirect.py": 1,
        "tools/subdomain_takeover_scan.py": 1,
        "tools/wordpress_check.py": 1,
        "tools/wpscan_domains.sh": 1,
        "README.md": 1,
        "install.sh": 1.0,
        "output/New Text Document.txt": 1,
        "output/.gitignore": 2
      },
      "lines_count": {
        "added": {
          "total": {
            "null": 0,
            "payloads/crlf.txt": 9,
            "payloads/error_pages.txt": 6,
            "payloads/open_redirects.txt": 22,
            "payloads/sensitive_headers.txt": 4,
            "recon.sh": 69,
            "tools/cors_misconfiguration_scan.sh": 25,
            "tools/crlf.sh": 19,
            "tools/error_page_info_check.py": 51,
            "tools/header_scan.py": 40,
            "tools/javascript_files_extractor.py": 76,
            "tools/javascript_files_link_extractor.sh": 23,
            "tools/nmap_scan.sh": 13,
            "tools/online.py": 45,
            "tools/open_redirect.py": 49,
            "tools/subdomain_takeover_scan.py": 117,
            "tools/wordpress_check.py": 34,
            "tools/wpscan_domains.sh": 13,
            "README.md": 1,
            "install.sh": 14,
            "output/New Text Document.txt": 4,
            "output/.gitignore": 1
          },
          "max": {
            "null": 0,
            "payloads/crlf.txt": 9,
            "payloads/error_pages.txt": 6,
            "payloads/open_redirects.txt": 22,
            "payloads/sensitive_headers.txt": 3,
            "recon.sh": 56,
            "tools/cors_misconfiguration_scan.sh": 25,
            "tools/crlf.sh": 19,
            "tools/error_page_info_check.py": 51,
            "tools/header_scan.py": 39,
            "tools/javascript_files_extractor.py": 72,
            "tools/javascript_files_link_extractor.sh": 23,
            "tools/nmap_scan.sh": 13,
            "tools/online.py": 45,
            "tools/open_redirect.py": 49,
            "tools/subdomain_takeover_scan.py": 117,
            "tools/wordpress_check.py": 34,
            "tools/wpscan_domains.sh": 13,
            "README.md": 1,
            "install.sh": 7,
            "output/New Text Document.txt": 4,
            "output/.gitignore": 1
          },
          "avg": {
            "null": 0,
            "payloads/crlf.txt": 9,
            "payloads/error_pages.txt": 6,
            "payloads/open_redirects.txt": 22,
            "payloads/sensitive_headers.txt": 2,
            "recon.sh": 14,
            "tools/cors_misconfiguration_scan.sh": 25,
            "tools/crlf.sh": 19,
            "tools/error_page_info_check.py": 51,
            "tools/header_scan.py": 20,
            "tools/javascript_files_extractor.py": 25,
            "tools/javascript_files_link_extractor.sh": 23,
            "tools/nmap_scan.sh": 13,
            "tools/online.py": 45,
            "tools/open_redirect.py": 49,
            "tools/subdomain_takeover_scan.py": 117,
            "tools/wordpress_check.py": 34,
            "tools/wpscan_domains.sh": 13,
            "README.md": 1,
            "install.sh": 7,
            "output/New Text Document.txt": 4,
            "output/.gitignore": 1
          }
        },
        "removed": {
          "total": {
            "null": 534,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 13,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 1,
            "tools/javascript_files_extractor.py": 2,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/open_redirect.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0,
            "README.md": 1,
            "install.sh": 2,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 1
          },
          "max": {
            "null": 117,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 5,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 1,
            "tools/javascript_files_extractor.py": 2,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/open_redirect.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0,
            "README.md": 1,
            "install.sh": 2,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 1
          },
          "avg": {
            "null": 41,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 3,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 1,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/open_redirect.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0,
            "README.md": 1,
            "install.sh": 1,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 1
          }
        },
        "noop_added": {
          "total": {
            "null": 0,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 12,
            "tools/cors_misconfiguration_scan.sh": 4,
            "tools/crlf.sh": 4,
            "tools/error_page_info_check.py": 10,
            "tools/header_scan.py": 7,
            "tools/javascript_files_extractor.py": 16,
            "tools/javascript_files_link_extractor.sh": 2,
            "tools/nmap_scan.sh": 5,
            "tools/online.py": 9,
            "tools/open_redirect.py": 11,
            "tools/subdomain_takeover_scan.py": 25,
            "tools/wordpress_check.py": 8,
            "tools/wpscan_domains.sh": 2,
            "README.md": 0,
            "install.sh": 3,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 0
          },
          "max": {
            "null": 0,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 9,
            "tools/cors_misconfiguration_scan.sh": 4,
            "tools/crlf.sh": 4,
            "tools/error_page_info_check.py": 10,
            "tools/header_scan.py": 7,
            "tools/javascript_files_extractor.py": 16,
            "tools/javascript_files_link_extractor.sh": 2,
            "tools/nmap_scan.sh": 5,
            "tools/online.py": 9,
            "tools/open_redirect.py": 11,
            "tools/subdomain_takeover_scan.py": 25,
            "tools/wordpress_check.py": 8,
            "tools/wpscan_domains.sh": 2,
            "README.md": 0,
            "install.sh": 3,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 0
          },
          "avg": {
            "null": 0,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 2,
            "tools/cors_misconfiguration_scan.sh": 4,
            "tools/crlf.sh": 4,
            "tools/error_page_info_check.py": 10,
            "tools/header_scan.py": 4,
            "tools/javascript_files_extractor.py": 5,
            "tools/javascript_files_link_extractor.sh": 2,
            "tools/nmap_scan.sh": 5,
            "tools/online.py": 9,
            "tools/open_redirect.py": 11,
            "tools/subdomain_takeover_scan.py": 25,
            "tools/wordpress_check.py": 8,
            "tools/wpscan_domains.sh": 2,
            "README.md": 0,
            "install.sh": 2,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 0
          }
        },
        "noop_removed": {
          "total": {
            "null": 106,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 3,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/open_redirect.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0,
            "README.md": 0,
            "install.sh": 0,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 0
          },
          "max": {
            "null": 25,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 2,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/open_redirect.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0,
            "README.md": 0,
            "install.sh": 0,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 0
          },
          "avg": {
            "null": 8,
            "payloads/crlf.txt": 0,
            "payloads/error_pages.txt": 0,
            "payloads/open_redirects.txt": 0,
            "payloads/sensitive_headers.txt": 0,
            "recon.sh": 1,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/open_redirect.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0,
            "README.md": 0,
            "install.sh": 0,
            "output/New Text Document.txt": 0,
            "output/.gitignore": 0
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 1189,
          "max": 117,
          "avg": 35
        },
        "net_churn": {
          "count": 81,
          "max": 117,
          "avg": 2
        },
        "added_removed": {
          "added": 635,
          "removed": 554
        },
        "true_churn": {
          "overall": {
            "contribution": 635,
            "churn": 17
          },
          "per_author": {
            "003random": {
              "contribution": 635,
              "churn": 17
            }
          },
          "per_file": {
            "cors_misconfiguration_scan.sh": {
              "contribution": 0,
              "churn": 0
            },
            "crlf.sh": {
              "contribution": 0,
              "churn": 0
            },
            "error_page_info_check.py": {
              "contribution": 0,
              "churn": 0
            },
            "header_scan.py": {
              "contribution": 0,
              "churn": 0
            },
            "javascript_files_extractor.py": {
              "contribution": 0,
              "churn": 0
            },
            "javascript_files_link_extractor.sh": {
              "contribution": 0,
              "churn": 0
            },
            "nmap_scan.sh": {
              "contribution": 0,
              "churn": 0
            },
            "online.py": {
              "contribution": 0,
              "churn": 0
            },
            "open_redirect.py": {
              "contribution": 0,
              "churn": 0
            },
            "start_recon.sh": {
              "contribution": 0,
              "churn": 0
            },
            "subdomain_takeover_scan.py": {
              "contribution": 0,
              "churn": 0
            },
            "wordpress_check.py": {
              "contribution": 0,
              "churn": 0
            },
            "wpscan_domains.sh": {
              "contribution": 0,
              "churn": 0
            },
            "payloads/crlf.txt": {
              "contribution": 9,
              "churn": 0
            },
            "payloads/error_pages.txt": {
              "contribution": 6,
              "churn": 0
            },
            "payloads/open_redirects.txt": {
              "contribution": 22,
              "churn": 0
            },
            "payloads/sensitive_headers.txt": {
              "contribution": 4,
              "churn": 0
            },
            "recon.sh": {
              "contribution": 69,
              "churn": 12
            },
            "tools/cors_misconfiguration_scan.sh": {
              "contribution": 25,
              "churn": 0
            },
            "tools/crlf.sh": {
              "contribution": 19,
              "churn": 0
            },
            "tools/error_page_info_check.py": {
              "contribution": 51,
              "churn": 0
            },
            "tools/header_scan.py": {
              "contribution": 40,
              "churn": 1
            },
            "tools/javascript_files_extractor.py": {
              "contribution": 76,
              "churn": 2
            },
            "tools/javascript_files_link_extractor.sh": {
              "contribution": 23,
              "churn": 0
            },
            "tools/nmap_scan.sh": {
              "contribution": 13,
              "churn": 0
            },
            "tools/online.py": {
              "contribution": 45,
              "churn": 0
            },
            "tools/open_redirect.py": {
              "contribution": 49,
              "churn": 0
            },
            "tools/subdomain_takeover_scan.py": {
              "contribution": 117,
              "churn": 0
            },
            "tools/wordpress_check.py": {
              "contribution": 34,
              "churn": 0
            },
            "tools/wpscan_domains.sh": {
              "contribution": 13,
              "churn": 0
            },
            "README.md": {
              "contribution": 1,
              "churn": 0
            },
            "install.sh": {
              "contribution": 14,
              "churn": 2
            },
            "output/New Text Document.txt": {
              "contribution": 4,
              "churn": 0
            },
            "output/.gitignore": {
              "contribution": 1,
              "churn": 0
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "null": 0.0,
          "payloads/crlf.txt": 0.0,
          "payloads/error_pages.txt": 0.0,
          "payloads/open_redirects.txt": 0.0,
          "payloads/sensitive_headers.txt": 0.0,
          "recon.sh": 0.0,
          "tools/cors_misconfiguration_scan.sh": 0.0,
          "tools/crlf.sh": 0.0,
          "tools/error_page_info_check.py": 0.0,
          "tools/header_scan.py": 0.0,
          "tools/javascript_files_extractor.py": 0.0,
          "tools/javascript_files_link_extractor.sh": 0.0,
          "tools/nmap_scan.sh": 0.0,
          "tools/online.py": 0.0,
          "tools/open_redirect.py": 0.0,
          "tools/subdomain_takeover_scan.py": 0.0,
          "tools/wordpress_check.py": 0.0,
          "tools/wpscan_domains.sh": 0.0,
          "README.md": 0.0,
          "install.sh": 0.0,
          "output/New Text Document.txt": 0.0,
          "output/.gitignore": 0.0
        },
        "overall_bug_work_percent": 0.0,
        "total_bug_lines": 0,
        "total_lines": 1189
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 104,
        "total_changed_lines": 875,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 11.885714285714286
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 6,
          "lines": 80,
          "percent": 7.373271889400922
        },
        "total": {
          "files": 22,
          "lines": 1085
        },
        "quality_score": 3.686635944700461
      },
      "meaningful_code": {
        "total": {
          "files": 22,
          "lines": 1085,
          "meaningful_lines": 785,
          "meaningful_percent": 72.35023041474655
        },
        "quality_score": 3.686635944700461,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 1189
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 1,
          "repetitive_patterns": 0,
          "total": 1,
          "percent": 0.09216589861751152
        }
      }
    },
    "timings": {
      "diff_delta": {
        "003random@protonmail.com": {
          "total_diff_delta": 587.85,
          "total_commits": 27,
          "weekly_velocity": {
            "2017-11-20": {
              "diff_delta": 587.85,
              "lines_added": 479,
              "lines_updated": 7,
              "lines_deleted": 414,
              "lines_moved": 1,
              "commits": 27,
              "files_changed": 22,
              "active_days": 4,
              "velocity_per_day": 146.9625
            }
          }
        }
      },
      "code_provenance": {
        "003random@protonmail.com": {
          "weekly_provenance": {
            "2017-11-20": {
              "new_code_lines": 316,
              "recent_code_lines": 1,
              "old_code_lines": 0,
              "legacy_code_lines": 0,
              "total_lines": 317,
              "new_code_percent": 99.6845425867508,
              "recent_code_percent": 0.31545741324921134,
              "old_code_percent": 0.0,
              "legacy_code_percent": 0.0
            }
          }
        }
      },
      "developer_hours": {
        "003random@protonmail.com": {
          "total_estimated_hours": 7.887416666666668,
          "total_sessions": 6,
          "weekly_hours": {
            "Week_47_2017-11-21": {
              "estimated_hours": 3.3,
              "sessions": 2,
              "commits": 17,
              "hours_per_day": 3.3
            },
            "Week_47_2017-11-22": {
              "estimated_hours": 1.64,
              "sessions": 2,
              "commits": 3,
              "hours_per_day": 1.64
            },
            "Week_47_2017-11-23": {
              "estimated_hours": 0.62,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.62
            },
            "Week_47_2017-11-25": {
              "estimated_hours": 2.33,
              "sessions": 1,
              "commits": 6,
              "hours_per_day": 2.33
            }
          }
        }
      },
      "code_domain": {
        "003random@protonmail.com": {
          "total_by_domain": {
            "devops": 321,
            "backend": 819,
            "docs": 47,
            "config": 2
          },
          "weekly_domains": {
            "2017-11-20": {
              "domains": {
                "devops": 321,
                "backend": 819,
                "docs": 47,
                "config": 2
              },
              "total_changes": 1189,
              "percentages": {
                "devops": 26.997476871320437,
                "backend": 68.88141295206054,
                "docs": 3.9529015979814974,
                "config": 0.16820857863751051
              }
            }
          },
          "domain_percentages": {
            "devops": 26.997476871320437,
            "backend": 68.88141295206054,
            "docs": 3.9529015979814974,
            "config": 0.16820857863751051
          }
        }
      },
      "comprehensive_time_analysis": {
        "003random@protonmail.com": {
          "basic_stats": {
            "total_commits": 27,
            "total_repos": 1,
            "first_commit_date": "2017-11-21T17:48:36+01:00",
            "last_commit_date": "2017-11-25T02:38:39+01:00",
            "total_span_days": 3.37,
            "commits_per_day": 8.016,
            "total_lines_changed": 1189,
            "total_files_changed": 43
          },
          "timing_patterns": {
            "mean_interval_hours": 3.11,
            "median_interval_hours": 0.04,
            "min_interval_minutes": 0.08,
            "max_interval_days": 2.03
          },
          "work_sessions": {
            "session_count": 5,
            "avg_session_length_hours": 1.52,
            "max_session_length_hours": 6.07,
            "avg_commits_per_session": 5.4,
            "max_commits_per_session": 17,
            "sessions": [
              {
                "start": "2017-11-21T17:48:36+01:00",
                "end": "2017-11-21T23:53:01+01:00",
                "length_hours": 6.07,
                "commits": 17
              },
              {
                "start": "2017-11-22T13:18:06+01:00",
                "end": "2017-11-22T13:28:58+01:00",
                "length_hours": 0.18,
                "commits": 2
              },
              {
                "start": "2017-11-22T19:14:50+01:00",
                "end": "2017-11-22T19:14:50+01:00",
                "length_hours": 0.0,
                "commits": 1
              },
              {
                "start": "2017-11-23T00:37:32+01:00",
                "end": "2017-11-23T00:37:32+01:00",
                "length_hours": 0.0,
                "commits": 1
              },
              {
                "start": "2017-11-25T01:16:44+01:00",
                "end": "2017-11-25T02:38:39+01:00",
                "length_hours": 1.37,
                "commits": 6
              }
            ]
          },
          "daily_patterns": {
            "peak_day": 1,
            "peak_day_count": 17,
            "day_distribution": {
              "1": 17,
              "2": 3,
              "3": 1,
              "5": 6
            }
          },
          "weekly_patterns": {
            "total_weeks": 1,
            "avg_activities_per_week": 27,
            "max_activities_per_week": 27,
            "min_activities_per_week": 27
          },
          "downtime_analysis": {
            "short_breaks_count": 22,
            "long_breaks_count": 1,
            "avg_break_hours": 3.1090064102564106
          },
          "rhythm_analysis": {
            "activities_per_day": 6.75,
            "consistency_score": 0.0
          },
          "sustained_activity": {
            "total_active_days": 4,
            "max_consecutive_days": 3,
            "avg_activities_per_active_day": 6.75
          }
        }
      }
    }
  },
  "Week_2_2017-11-27": {
    "productivity": {
      "change_set": {
        "max": 1,
        "avg": 1.0
      },
      "commits_count": {
        "install.sh": 1,
        "tools/nmap_scan.sh": 1,
        "recon.sh": 1,
        "README.md": 5
      },
      "contributors_count": {
        "total": {
          "install.sh": 1,
          "tools/nmap_scan.sh": 1,
          "recon.sh": 1,
          "README.md": 1
        },
        "minor": {
          "install.sh": 0,
          "tools/nmap_scan.sh": 0,
          "recon.sh": 0,
          "README.md": 0
        }
      },
      "contributors_experience": {
        "install.sh": 1,
        "tools/nmap_scan.sh": 1,
        "recon.sh": 1,
        "README.md": 1
      },
      "hunks_count": {
        "install.sh": 1,
        "tools/nmap_scan.sh": 2,
        "recon.sh": 2,
        "README.md": 1
      },
      "lines_count": {
        "added": {
          "total": {
            "install.sh": 6,
            "tools/nmap_scan.sh": 2,
            "recon.sh": 3,
            "README.md": 22
          },
          "max": {
            "install.sh": 6,
            "tools/nmap_scan.sh": 2,
            "recon.sh": 3,
            "README.md": 10
          },
          "avg": {
            "install.sh": 6,
            "tools/nmap_scan.sh": 2,
            "recon.sh": 3,
            "README.md": 4
          }
        },
        "removed": {
          "total": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 2,
            "recon.sh": 1,
            "README.md": 10
          },
          "max": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 2,
            "recon.sh": 1,
            "README.md": 7
          },
          "avg": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 2,
            "recon.sh": 1,
            "README.md": 2
          }
        },
        "noop_added": {
          "total": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 0,
            "recon.sh": 1,
            "README.md": 3
          },
          "max": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 0,
            "recon.sh": 1,
            "README.md": 2
          },
          "avg": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 0,
            "recon.sh": 1,
            "README.md": 1
          }
        },
        "noop_removed": {
          "total": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 0,
            "recon.sh": 0,
            "README.md": 0
          },
          "max": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 0,
            "recon.sh": 0,
            "README.md": 0
          },
          "avg": {
            "install.sh": 0,
            "tools/nmap_scan.sh": 0,
            "recon.sh": 0,
            "README.md": 0
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 46,
          "max": 32,
          "avg": 12
        },
        "net_churn": {
          "count": 20,
          "max": 12,
          "avg": 5
        },
        "added_removed": {
          "added": 33,
          "removed": 13
        },
        "true_churn": {
          "overall": {
            "contribution": 33,
            "churn": 8
          },
          "per_author": {
            "003random": {
              "contribution": 33,
              "churn": 8
            }
          },
          "per_file": {
            "install.sh": {
              "contribution": 6,
              "churn": 0
            },
            "tools/nmap_scan.sh": {
              "contribution": 2,
              "churn": 0
            },
            "recon.sh": {
              "contribution": 3,
              "churn": 0
            },
            "README.md": {
              "contribution": 22,
              "churn": 8
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "install.sh": 0.0,
          "tools/nmap_scan.sh": 0.0,
          "recon.sh": 0.0,
          "README.md": 0.0
        },
        "overall_bug_work_percent": 0.0,
        "total_bug_lines": 0,
        "total_lines": 46
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 40,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 0.0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 1,
          "lines": 82,
          "percent": 47.398843930635834
        },
        "total": {
          "files": 4,
          "lines": 173
        },
        "quality_score": 23.699421965317917
      },
      "meaningful_code": {
        "total": {
          "files": 4,
          "lines": 173,
          "meaningful_lines": 71,
          "meaningful_percent": 41.040462427745666
        },
        "quality_score": 23.699421965317917,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 46
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0.0
        }
      }
    },
    "timings": {
      "diff_delta": {
        "003random@protonmail.com": {
          "total_diff_delta": 17.700000000000003,
          "total_commits": 8,
          "weekly_velocity": {
            "2017-11-27": {
              "diff_delta": 17.700000000000003,
              "lines_added": 16,
              "lines_updated": 0,
              "lines_deleted": 4,
              "lines_moved": 7,
              "commits": 8,
              "files_changed": 4,
              "active_days": 1,
              "velocity_per_day": 17.700000000000003
            }
          }
        }
      },
      "code_provenance": {},
      "developer_hours": {
        "003random@protonmail.com": {
          "total_estimated_hours": 1.2564444444444445,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_48_2017-11-28": {
              "estimated_hours": 1.26,
              "sessions": 1,
              "commits": 8,
              "hours_per_day": 1.26
            }
          }
        }
      },
      "code_domain": {
        "003random@protonmail.com": {
          "total_by_domain": {
            "devops": 14,
            "docs": 32
          },
          "weekly_domains": {
            "2017-11-27": {
              "domains": {
                "devops": 14,
                "docs": 32
              },
              "total_changes": 46,
              "percentages": {
                "devops": 30.434782608695656,
                "docs": 69.56521739130434
              }
            }
          },
          "domain_percentages": {
            "devops": 30.434782608695656,
            "docs": 69.56521739130434
          }
        }
      },
      "comprehensive_time_analysis": {
        "003random@protonmail.com": {
          "basic_stats": {
            "total_commits": 8,
            "total_repos": 1,
            "first_commit_date": "2017-11-28T18:23:48+01:00",
            "last_commit_date": "2017-11-28T18:47:20+01:00",
            "total_span_days": 0.02,
            "commits_per_day": 8.0,
            "total_lines_changed": 46,
            "total_files_changed": 8
          },
          "timing_patterns": {
            "mean_interval_hours": 0.06,
            "median_interval_hours": 0.02,
            "min_interval_minutes": 0.23,
            "max_interval_days": 0.01
          },
          "work_sessions": {
            "session_count": 1,
            "avg_session_length_hours": 0.39,
            "max_session_length_hours": 0.39,
            "avg_commits_per_session": 8,
            "max_commits_per_session": 8,
            "sessions": [
              {
                "start": "2017-11-28T18:23:48+01:00",
                "end": "2017-11-28T18:47:20+01:00",
                "length_hours": 0.39,
                "commits": 8
              }
            ]
          },
          "daily_patterns": {
            "peak_day": 1,
            "peak_day_count": 8,
            "day_distribution": {
              "1": 8
            }
          },
          "weekly_patterns": {
            "total_weeks": 1,
            "avg_activities_per_week": 8,
            "max_activities_per_week": 8,
            "min_activities_per_week": 8
          },
          "downtime_analysis": {
            "short_breaks_count": 7,
            "long_breaks_count": 0,
            "avg_break_hours": 0.05603174603174604
          },
          "rhythm_analysis": {
            "activities_per_day": 8.0,
            "consistency_score": 0.0
          },
          "sustained_activity": {
            "total_active_days": 1,
            "max_consecutive_days": 1,
            "avg_activities_per_active_day": 8.0
          }
        }
      }
    }
  },
  "Week_3_2017-12-04": {
    "productivity": {
      "change_set": {
        "max": 1,
        "avg": 0.9565217391304348
      },
      "commits_count": {
        "tools/javascript_files_link_extractor.sh": 1,
        "README.md": 12,
        "install.sh": 2,
        "recon.sh": 2,
        "tools/wpscan_domains.sh": 1,
        "edited_tools/javascript_files_and_links_extractor.py": 2,
        "tools/open_redirect.py": 1,
        "null": 1
      },
      "contributors_count": {
        "total": {
          "tools/javascript_files_link_extractor.sh": 1,
          "README.md": 1,
          "install.sh": 1,
          "recon.sh": 1,
          "tools/wpscan_domains.sh": 1,
          "edited_tools/javascript_files_and_links_extractor.py": 1,
          "tools/open_redirect.py": 1,
          "null": 1
        },
        "minor": {
          "tools/javascript_files_link_extractor.sh": 0,
          "README.md": 0,
          "install.sh": 0,
          "recon.sh": 0,
          "tools/wpscan_domains.sh": 0,
          "edited_tools/javascript_files_and_links_extractor.py": 0,
          "tools/open_redirect.py": 0,
          "null": 0
        }
      },
      "contributors_experience": {
        "tools/javascript_files_link_extractor.sh": 1,
        "README.md": 1,
        "install.sh": 1,
        "recon.sh": 1,
        "tools/wpscan_domains.sh": 1,
        "edited_tools/javascript_files_and_links_extractor.py": 1,
        "tools/open_redirect.py": 1,
        "null": 1
      },
      "hunks_count": {
        "tools/javascript_files_link_extractor.sh": 1,
        "README.md": 1.0,
        "install.sh": 1.5,
        "recon.sh": 2.0,
        "tools/wpscan_domains.sh": 1,
        "edited_tools/javascript_files_and_links_extractor.py": 1.0,
        "tools/open_redirect.py": 3,
        "null": 1
      },
      "lines_count": {
        "added": {
          "total": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 47,
            "install.sh": 2,
            "recon.sh": 6,
            "tools/wpscan_domains.sh": 1,
            "edited_tools/javascript_files_and_links_extractor.py": 64,
            "tools/open_redirect.py": 13,
            "null": 0
          },
          "max": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 20,
            "install.sh": 2,
            "recon.sh": 5,
            "tools/wpscan_domains.sh": 1,
            "edited_tools/javascript_files_and_links_extractor.py": 62,
            "tools/open_redirect.py": 13,
            "null": 0
          },
          "avg": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 4,
            "install.sh": 1,
            "recon.sh": 3,
            "tools/wpscan_domains.sh": 1,
            "edited_tools/javascript_files_and_links_extractor.py": 32,
            "tools/open_redirect.py": 13,
            "null": 0
          }
        },
        "removed": {
          "total": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 28,
            "install.sh": 4,
            "recon.sh": 1,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 0,
            "tools/open_redirect.py": 13,
            "null": 4
          },
          "max": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 16,
            "install.sh": 2,
            "recon.sh": 1,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 0,
            "tools/open_redirect.py": 13,
            "null": 4
          },
          "avg": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 2,
            "install.sh": 2,
            "recon.sh": 0,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 0,
            "tools/open_redirect.py": 13,
            "null": 4
          }
        },
        "noop_added": {
          "total": {
            "tools/javascript_files_link_extractor.sh": 0,
            "README.md": 9,
            "install.sh": 1,
            "recon.sh": 1,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 12,
            "tools/open_redirect.py": 0,
            "null": 0
          },
          "max": {
            "tools/javascript_files_link_extractor.sh": 0,
            "README.md": 4,
            "install.sh": 1,
            "recon.sh": 1,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 12,
            "tools/open_redirect.py": 0,
            "null": 0
          },
          "avg": {
            "tools/javascript_files_link_extractor.sh": 0,
            "README.md": 1,
            "install.sh": 0,
            "recon.sh": 0,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 6,
            "tools/open_redirect.py": 0,
            "null": 0
          }
        },
        "noop_removed": {
          "total": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 7,
            "install.sh": 2,
            "recon.sh": 0,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 0,
            "tools/open_redirect.py": 0,
            "null": 0
          },
          "max": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 6,
            "install.sh": 1,
            "recon.sh": 0,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 0,
            "tools/open_redirect.py": 0,
            "null": 0
          },
          "avg": {
            "tools/javascript_files_link_extractor.sh": 1,
            "README.md": 1,
            "install.sh": 1,
            "recon.sh": 0,
            "tools/wpscan_domains.sh": 0,
            "edited_tools/javascript_files_and_links_extractor.py": 0,
            "tools/open_redirect.py": 0,
            "null": 0
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 185,
          "max": 75,
          "avg": 23
        },
        "net_churn": {
          "count": 83,
          "max": 64,
          "avg": 10
        },
        "added_removed": {
          "added": 134,
          "removed": 51
        },
        "true_churn": {
          "overall": {
            "contribution": 134,
            "churn": 11
          },
          "per_author": {
            "003random": {
              "contribution": 121,
              "churn": 11
            },
            "Abhinash Jain": {
              "contribution": 13,
              "churn": 0
            }
          },
          "per_file": {
            "tools/javascript_files_link_extractor.sh": {
              "contribution": 1,
              "churn": 0
            },
            "README.md": {
              "contribution": 47,
              "churn": 11
            },
            "install.sh": {
              "contribution": 2,
              "churn": 0
            },
            "recon.sh": {
              "contribution": 6,
              "churn": 0
            },
            "tools/wpscan_domains.sh": {
              "contribution": 1,
              "churn": 0
            },
            "edited_tools/javascript_files_and_links_extractor.py": {
              "contribution": 64,
              "churn": 0
            },
            "tools/open_redirect.py": {
              "contribution": 13,
              "churn": 0
            },
            ".gitignore": {
              "contribution": 0,
              "churn": 0
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "tools/javascript_files_link_extractor.sh": 0.0,
          "README.md": 0.0,
          "install.sh": 0.0,
          "recon.sh": 0.0,
          "tools/wpscan_domains.sh": 0.0,
          "edited_tools/javascript_files_and_links_extractor.py": 0.0,
          "tools/open_redirect.py": 0.0,
          "null": 0.0
        },
        "overall_bug_work_percent": 0.0,
        "total_bug_lines": 0,
        "total_lines": 185
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 2,
        "total_changed_lines": 142,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 1.4084507042253522
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 1,
          "lines": 449,
          "percent": 55.500618046971574
        },
        "total": {
          "files": 8,
          "lines": 809
        },
        "quality_score": 27.750309023485787
      },
      "meaningful_code": {
        "total": {
          "files": 8,
          "lines": 809,
          "meaningful_lines": 293,
          "meaningful_percent": 36.21755253399258
        },
        "quality_score": 27.750309023485787,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 185
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 1,
          "repetitive_patterns": 0,
          "total": 1,
          "percent": 0.12360939431396785
        }
      }
    },
    "timings": {
      "diff_delta": {
        "003random@protonmail.com": {
          "total_diff_delta": 86.95,
          "total_commits": 22,
          "weekly_velocity": {
            "2017-12-04": {
              "diff_delta": 86.95,
              "lines_added": 77,
              "lines_updated": 8,
              "lines_deleted": 13,
              "lines_moved": 7,
              "commits": 22,
              "files_changed": 7,
              "active_days": 3,
              "velocity_per_day": 28.983333333333334
            }
          }
        },
        "labor.omnia.vincit18@gmail.com": {
          "total_diff_delta": 4.05,
          "total_commits": 1,
          "weekly_velocity": {
            "2017-12-04": {
              "diff_delta": 4.05,
              "lines_added": 0,
              "lines_updated": 0,
              "lines_deleted": 11,
              "lines_moved": 13,
              "commits": 1,
              "files_changed": 1,
              "active_days": 1,
              "velocity_per_day": 4.05
            }
          }
        }
      },
      "code_provenance": {
        "003random@protonmail.com": {
          "weekly_provenance": {
            "2017-12-04": {
              "new_code_lines": 49,
              "recent_code_lines": 0,
              "old_code_lines": 0,
              "legacy_code_lines": 0,
              "total_lines": 49,
              "new_code_percent": 100.0,
              "recent_code_percent": 0.0,
              "old_code_percent": 0.0,
              "legacy_code_percent": 0.0
            }
          }
        },
        "labor.omnia.vincit18@gmail.com": {
          "weekly_provenance": {
            "2017-12-04": {
              "new_code_lines": 13,
              "recent_code_lines": 0,
              "old_code_lines": 0,
              "legacy_code_lines": 0,
              "total_lines": 13,
              "new_code_percent": 100.0,
              "recent_code_percent": 0.0,
              "old_code_percent": 0.0,
              "legacy_code_percent": 0.0
            }
          }
        }
      },
      "developer_hours": {
        "003random@protonmail.com": {
          "total_estimated_hours": 7.923361111111111,
          "total_sessions": 4,
          "weekly_hours": {
            "Week_49_2017-12-04": {
              "estimated_hours": 0.62,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.62
            },
            "Week_49_2017-12-05": {
              "estimated_hours": 6.32,
              "sessions": 2,
              "commits": 18,
              "hours_per_day": 6.32
            },
            "Week_49_2017-12-07": {
              "estimated_hours": 0.99,
              "sessions": 1,
              "commits": 3,
              "hours_per_day": 0.99
            }
          }
        },
        "labor.omnia.vincit18@gmail.com": {
          "total_estimated_hours": 0.61875,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_49_2017-12-06": {
              "estimated_hours": 0.62,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.62
            }
          }
        }
      },
      "code_domain": {
        "003random@protonmail.com": {
          "total_by_domain": {
            "devops": 16,
            "docs": 75,
            "backend": 64,
            "config": 4
          },
          "weekly_domains": {
            "2017-12-04": {
              "domains": {
                "devops": 16,
                "docs": 75,
                "backend": 64,
                "config": 4
              },
              "total_changes": 159,
              "percentages": {
                "devops": 10.062893081761008,
                "docs": 47.16981132075472,
                "backend": 40.25157232704403,
                "config": 2.515723270440252
              }
            }
          },
          "domain_percentages": {
            "devops": 10.062893081761008,
            "docs": 47.16981132075472,
            "backend": 40.25157232704403,
            "config": 2.515723270440252
          }
        },
        "labor.omnia.vincit18@gmail.com": {
          "total_by_domain": {
            "backend": 26
          },
          "weekly_domains": {
            "2017-12-04": {
              "domains": {
                "backend": 26
              },
              "total_changes": 26,
              "percentages": {
                "backend": 100.0
              }
            }
          },
          "domain_percentages": {
            "backend": 100.0
          }
        }
      },
      "comprehensive_time_analysis": {
        "003random@protonmail.com": {
          "basic_stats": {
            "total_commits": 22,
            "total_repos": 1,
            "first_commit_date": "2017-12-04T17:49:53+01:00",
            "last_commit_date": "2017-12-07T22:19:34+01:00",
            "total_span_days": 3.19,
            "commits_per_day": 6.902,
            "total_lines_changed": 159,
            "total_files_changed": 21
          },
          "timing_patterns": {
            "mean_interval_hours": 3.64,
            "median_interval_hours": 0.03,
            "min_interval_minutes": 0.28,
            "max_interval_days": 2.09
          },
          "work_sessions": {
            "session_count": 3,
            "avg_session_length_hours": 2.42,
            "max_session_length_hours": 7.1,
            "avg_commits_per_session": 7.33,
            "max_commits_per_session": 18,
            "sessions": [
              {
                "start": "2017-12-04T17:49:53+01:00",
                "end": "2017-12-04T17:49:53+01:00",
                "length_hours": 0.0,
                "commits": 1
              },
              {
                "start": "2017-12-05T12:54:12+01:00",
                "end": "2017-12-05T20:00:05+01:00",
                "length_hours": 7.1,
                "commits": 18
              },
              {
                "start": "2017-12-07T22:10:45+01:00",
                "end": "2017-12-07T22:19:34+01:00",
                "length_hours": 0.15,
                "commits": 3
              }
            ]
          },
          "daily_patterns": {
            "peak_day": 1,
            "peak_day_count": 18,
            "day_distribution": {
              "0": 1,
              "1": 18,
              "3": 3
            }
          },
          "weekly_patterns": {
            "total_weeks": 1,
            "avg_activities_per_week": 22,
            "max_activities_per_week": 22,
            "min_activities_per_week": 22
          },
          "downtime_analysis": {
            "short_breaks_count": 19,
            "long_breaks_count": 1,
            "avg_break_hours": 3.6426058201058202
          },
          "rhythm_analysis": {
            "activities_per_day": 5.5,
            "consistency_score": 0.0
          },
          "sustained_activity": {
            "total_active_days": 3,
            "max_consecutive_days": 2,
            "avg_activities_per_active_day": 7.333333333333333
          }
        }
      }
    }
  },
  "Week_4_2017-12-11": {
    "productivity": {
      "change_set": {
        "max": 1,
        "avg": 1.0
      },
      "commits_count": {
        "README.md": 1
      },
      "contributors_count": {
        "total": {
          "README.md": 1
        },
        "minor": {
          "README.md": 0
        }
      },
      "contributors_experience": {
        "README.md": 1
      },
      "hunks_count": {
        "README.md": 4
      },
      "lines_count": {
        "added": {
          "total": {
            "README.md": 9
          },
          "max": {
            "README.md": 9
          },
          "avg": {
            "README.md": 9
          }
        },
        "removed": {
          "total": {
            "README.md": 8
          },
          "max": {
            "README.md": 8
          },
          "avg": {
            "README.md": 8
          }
        },
        "noop_added": {
          "total": {
            "README.md": 2
          },
          "max": {
            "README.md": 2
          },
          "avg": {
            "README.md": 2
          }
        },
        "noop_removed": {
          "total": {
            "README.md": 0
          },
          "max": {
            "README.md": 0
          },
          "avg": {
            "README.md": 0
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 17,
          "max": 17,
          "avg": 17
        },
        "net_churn": {
          "count": 1,
          "max": 1,
          "avg": 1
        },
        "added_removed": {
          "added": 9,
          "removed": 8
        },
        "true_churn": {
          "overall": {
            "contribution": 9,
            "churn": 0
          },
          "per_author": {
            "jose nazario": {
              "contribution": 9,
              "churn": 0
            }
          },
          "per_file": {
            "README.md": {
              "contribution": 9,
              "churn": 0
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "README.md": 0.0
        },
        "overall_bug_work_percent": 0.0,
        "total_bug_lines": 0,
        "total_lines": 17
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 14,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 0.0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 1,
          "lines": 38,
          "percent": 100.0
        },
        "total": {
          "files": 1,
          "lines": 38
        },
        "quality_score": 50.0
      },
      "meaningful_code": {
        "total": {
          "files": 1,
          "lines": 38,
          "meaningful_lines": 0,
          "meaningful_percent": 0.0
        },
        "quality_score": 50.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 17
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0.0
        }
      }
    },
    "timings": {
      "diff_delta": {
        "jose.monkey.org@gmail.com": {
          "total_diff_delta": 4.65,
          "total_commits": 1,
          "weekly_velocity": {
            "2017-12-11": {
              "diff_delta": 4.65,
              "lines_added": 1,
              "lines_updated": 3,
              "lines_deleted": 4,
              "lines_moved": 4,
              "commits": 1,
              "files_changed": 1,
              "active_days": 1,
              "velocity_per_day": 4.65
            }
          }
        }
      },
      "code_provenance": {},
      "developer_hours": {
        "jose.monkey.org@gmail.com": {
          "total_estimated_hours": 0.61875,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_50_2017-12-15": {
              "estimated_hours": 0.62,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.62
            }
          }
        }
      },
      "code_domain": {
        "jose.monkey.org@gmail.com": {
          "total_by_domain": {
            "docs": 17
          },
          "weekly_domains": {
            "2017-12-11": {
              "domains": {
                "docs": 17
              },
              "total_changes": 17,
              "percentages": {
                "docs": 100.0
              }
            }
          },
          "domain_percentages": {
            "docs": 100.0
          }
        }
      },
      "comprehensive_time_analysis": {}
    }
  },
  "Week_5_2017-12-18": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_6_2017-12-25": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_7_2018-01-01": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_8_2018-01-08": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_9_2018-01-15": {
    "productivity": {
      "change_set": {
        "max": 13,
        "avg": 2.111111111111111
      },
      "commits_count": {
        "install.sh": 5,
        "recon.sh": 2,
        "tools/cors_misconfiguration_scan.sh": 1,
        "tools/crlf.sh": 1,
        "tools/error_page_info_check.py": 1,
        "tools/header_scan.py": 1,
        "tools/javascript_files_extractor.py": 2,
        "tools/javascript_files_link_extractor.sh": 1,
        "tools/nmap_scan.sh": 1,
        "tools/online.py": 1,
        "tools/subdomain_takeover_scan.py": 1,
        "tools/wordpress_check.py": 1,
        "tools/wpscan_domains.sh": 1
      },
      "contributors_count": {
        "total": {
          "install.sh": 3,
          "recon.sh": 2,
          "tools/javascript_files_extractor.py": 2
        },
        "minor": {
          "install.sh": 0,
          "recon.sh": 1,
          "tools/javascript_files_extractor.py": 1
        }
      },
      "contributors_experience": {
        "install.sh": 3,
        "recon.sh": 2,
        "tools/cors_misconfiguration_scan.sh": 1,
        "tools/crlf.sh": 1,
        "tools/error_page_info_check.py": 1,
        "tools/header_scan.py": 1,
        "tools/javascript_files_extractor.py": 2,
        "tools/javascript_files_link_extractor.sh": 1,
        "tools/nmap_scan.sh": 1,
        "tools/online.py": 1,
        "tools/subdomain_takeover_scan.py": 1,
        "tools/wordpress_check.py": 1,
        "tools/wpscan_domains.sh": 1
      },
      "hunks_count": {
        "install.sh": 1,
        "recon.sh": 1,
        "tools/javascript_files_extractor.py": 1
      },
      "lines_count": {
        "added": {
          "total": {
            "install.sh": 84,
            "recon.sh": 1,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 1,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "max": {
            "install.sh": 30,
            "recon.sh": 1,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 1,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "avg": {
            "install.sh": 17,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          }
        },
        "removed": {
          "total": {
            "install.sh": 68,
            "recon.sh": 1,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 1,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "max": {
            "install.sh": 26,
            "recon.sh": 1,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 1,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "avg": {
            "install.sh": 14,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          }
        },
        "noop_added": {
          "total": {
            "install.sh": 6,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "max": {
            "install.sh": 6,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "avg": {
            "install.sh": 1,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          }
        },
        "noop_removed": {
          "total": {
            "install.sh": 0,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "max": {
            "install.sh": 0,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          },
          "avg": {
            "install.sh": 0,
            "recon.sh": 0,
            "tools/cors_misconfiguration_scan.sh": 0,
            "tools/crlf.sh": 0,
            "tools/error_page_info_check.py": 0,
            "tools/header_scan.py": 0,
            "tools/javascript_files_extractor.py": 0,
            "tools/javascript_files_link_extractor.sh": 0,
            "tools/nmap_scan.sh": 0,
            "tools/online.py": 0,
            "tools/subdomain_takeover_scan.py": 0,
            "tools/wordpress_check.py": 0,
            "tools/wpscan_domains.sh": 0
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 156,
          "max": 152,
          "avg": 12
        },
        "net_churn": {
          "count": 16,
          "max": 16,
          "avg": 1
        },
        "added_removed": {
          "added": 86,
          "removed": 70
        },
        "true_churn": {
          "overall": {
            "contribution": 86,
            "churn": 28
          },
          "per_author": {
            "Fauxsys": {
              "contribution": 30,
              "churn": 0
            },
            "003random": {
              "contribution": 41,
              "churn": 28
            },
            "rewanth1997": {
              "contribution": 15,
              "churn": 0
            }
          },
          "per_file": {
            "install.sh": {
              "contribution": 84,
              "churn": 28
            },
            "recon.sh": {
              "contribution": 1,
              "churn": 0
            },
            "tools/javascript_files_extractor.py": {
              "contribution": 1,
              "churn": 0
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "install.sh": 42.76315789473684,
          "recon.sh": 0.0,
          "tools/cors_misconfiguration_scan.sh": 0,
          "tools/crlf.sh": 0,
          "tools/error_page_info_check.py": 0,
          "tools/header_scan.py": 0,
          "tools/javascript_files_extractor.py": 0.0,
          "tools/javascript_files_link_extractor.sh": 0,
          "tools/nmap_scan.sh": 0,
          "tools/online.py": 0,
          "tools/subdomain_takeover_scan.py": 0,
          "tools/wordpress_check.py": 0,
          "tools/wpscan_domains.sh": 0
        },
        "overall_bug_work_percent": 41.66666666666667,
        "total_bug_lines": 65,
        "total_lines": 156
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 134,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 0.0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 44,
          "percent": 13.580246913580247
        },
        "total": {
          "files": 13,
          "lines": 324
        },
        "quality_score": 6.790123456790123
      },
      "meaningful_code": {
        "total": {
          "files": 13,
          "lines": 324,
          "meaningful_lines": 248,
          "meaningful_percent": 76.5432098765432
        },
        "quality_score": 6.790123456790123,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 156
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0.0
        }
      }
    },
    "timings": {
      "diff_delta": {
        "fibercipher@gmail.com": {
          "total_diff_delta": 10.45,
          "total_commits": 1,
          "weekly_velocity": {
            "2018-01-15": {
              "diff_delta": 10.45,
              "lines_added": 5,
              "lines_updated": 1,
              "lines_deleted": 14,
              "lines_moved": 12,
              "commits": 1,
              "files_changed": 13,
              "active_days": 1,
              "velocity_per_day": 10.45
            }
          }
        },
        "003random@protonmail.com": {
          "total_diff_delta": 51.1,
          "total_commits": 7,
          "weekly_velocity": {
            "2018-01-15": {
              "diff_delta": 51.1,
              "lines_added": 30,
              "lines_updated": 12,
              "lines_deleted": 46,
              "lines_moved": 6,
              "commits": 7,
              "files_changed": 3,
              "active_days": 1,
              "velocity_per_day": 51.1
            }
          }
        },
        "rewanth1997@gmail.com": {
          "total_diff_delta": 8.1,
          "total_commits": 1,
          "weekly_velocity": {
            "2018-01-15": {
              "diff_delta": 8.1,
              "lines_added": 6,
              "lines_updated": 0,
              "lines_deleted": 6,
              "lines_moved": 6,
              "commits": 1,
              "files_changed": 1,
              "active_days": 1,
              "velocity_per_day": 8.1
            }
          }
        }
      },
      "code_provenance": {
        "003random@protonmail.com": {
          "weekly_provenance": {
            "2018-01-15": {
              "new_code_lines": 1,
              "recent_code_lines": 0,
              "old_code_lines": 0,
              "legacy_code_lines": 0,
              "total_lines": 1,
              "new_code_percent": 100.0,
              "recent_code_percent": 0.0,
              "old_code_percent": 0.0,
              "legacy_code_percent": 0.0
            }
          }
        }
      },
      "developer_hours": {
        "fibercipher@gmail.com": {
          "total_estimated_hours": 0.5625,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_3_2018-01-18": {
              "estimated_hours": 0.56,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.56
            }
          }
        },
        "003random@protonmail.com": {
          "total_estimated_hours": 2.7175000000000002,
          "total_sessions": 2,
          "weekly_hours": {
            "Week_3_2018-01-20": {
              "estimated_hours": 2.72,
              "sessions": 2,
              "commits": 7,
              "hours_per_day": 2.72
            }
          }
        },
        "rewanth1997@gmail.com": {
          "total_estimated_hours": 0.61875,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_3_2018-01-21": {
              "estimated_hours": 0.62,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.62
            }
          }
        }
      },
      "code_domain": {
        "fibercipher@gmail.com": {
          "total_by_domain": {
            "devops": 44
          },
          "weekly_domains": {
            "2018-01-15": {
              "domains": {
                "devops": 44
              },
              "total_changes": 44,
              "percentages": {
                "devops": 100.0
              }
            }
          },
          "domain_percentages": {
            "devops": 100.0
          }
        },
        "003random@protonmail.com": {
          "total_by_domain": {
            "devops": 89,
            "backend": 2
          },
          "weekly_domains": {
            "2018-01-15": {
              "domains": {
                "devops": 89,
                "backend": 2
              },
              "total_changes": 91,
              "percentages": {
                "devops": 97.8021978021978,
                "backend": 2.197802197802198
              }
            }
          },
          "domain_percentages": {
            "devops": 97.8021978021978,
            "backend": 2.197802197802198
          }
        },
        "rewanth1997@gmail.com": {
          "total_by_domain": {
            "devops": 21
          },
          "weekly_domains": {
            "2018-01-15": {
              "domains": {
                "devops": 21
              },
              "total_changes": 21,
              "percentages": {
                "devops": 100.0
              }
            }
          },
          "domain_percentages": {
            "devops": 100.0
          }
        }
      },
      "comprehensive_time_analysis": {
        "003random@protonmail.com": {
          "basic_stats": {
            "total_commits": 7,
            "total_repos": 1,
            "first_commit_date": "2018-01-20T15:06:27+01:00",
            "last_commit_date": "2018-01-20T22:47:47+01:00",
            "total_span_days": 0.32,
            "commits_per_day": 7.0,
            "total_lines_changed": 91,
            "total_files_changed": 5
          },
          "timing_patterns": {
            "mean_interval_hours": 1.28,
            "median_interval_hours": 0.08,
            "min_interval_minutes": 1.38,
            "max_interval_days": 0.27
          },
          "work_sessions": {
            "session_count": 2,
            "avg_session_length_hours": 0.61,
            "max_session_length_hours": 1.19,
            "avg_commits_per_session": 3.5,
            "max_commits_per_session": 5,
            "sessions": [
              {
                "start": "2018-01-20T15:06:27+01:00",
                "end": "2018-01-20T15:07:50+01:00",
                "length_hours": 0.02,
                "commits": 2
              },
              {
                "start": "2018-01-20T21:36:07+01:00",
                "end": "2018-01-20T22:47:47+01:00",
                "length_hours": 1.19,
                "commits": 5
              }
            ]
          },
          "daily_patterns": {
            "peak_day": 5,
            "peak_day_count": 7,
            "day_distribution": {
              "5": 7
            }
          },
          "weekly_patterns": {
            "total_weeks": 1,
            "avg_activities_per_week": 7,
            "max_activities_per_week": 7,
            "min_activities_per_week": 7
          },
          "downtime_analysis": {
            "short_breaks_count": 5,
            "long_breaks_count": 0,
            "avg_break_hours": 1.2814814814814814
          },
          "rhythm_analysis": {
            "activities_per_day": 7.0,
            "consistency_score": 0.0
          },
          "sustained_activity": {
            "total_active_days": 1,
            "max_consecutive_days": 1,
            "avg_activities_per_active_day": 7.0
          }
        }
      }
    }
  },
  "Week_10_2018-01-22": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_11_2018-01-29": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_12_2018-02-05": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_13_2018-02-12": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_14_2018-02-19": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_15_2018-02-26": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_16_2018-03-05": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_17_2018-03-12": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_18_2018-03-19": {
    "productivity": {
      "change_set": {
        "max": 1,
        "avg": 1.0
      },
      "commits_count": {
        "null": 1,
        "tools/edited_tools/files_and_links_extractor.py": 1
      },
      "contributors_count": {
        "total": {
          "null": 1,
          "tools/edited_tools/files_and_links_extractor.py": 1
        },
        "minor": {
          "null": 0,
          "tools/edited_tools/files_and_links_extractor.py": 0
        }
      },
      "contributors_experience": {
        "null": 1,
        "tools/edited_tools/files_and_links_extractor.py": 1
      },
      "hunks_count": {
        "null": 1,
        "tools/edited_tools/files_and_links_extractor.py": 1
      },
      "lines_count": {
        "added": {
          "total": {
            "null": 0,
            "tools/edited_tools/files_and_links_extractor.py": 64
          },
          "max": {
            "null": 0,
            "tools/edited_tools/files_and_links_extractor.py": 64
          },
          "avg": {
            "null": 0,
            "tools/edited_tools/files_and_links_extractor.py": 64
          }
        },
        "removed": {
          "total": {
            "null": 64,
            "tools/edited_tools/files_and_links_extractor.py": 0
          },
          "max": {
            "null": 64,
            "tools/edited_tools/files_and_links_extractor.py": 0
          },
          "avg": {
            "null": 64,
            "tools/edited_tools/files_and_links_extractor.py": 0
          }
        },
        "noop_added": {
          "total": {
            "null": 0,
            "tools/edited_tools/files_and_links_extractor.py": 12
          },
          "max": {
            "null": 0,
            "tools/edited_tools/files_and_links_extractor.py": 12
          },
          "avg": {
            "null": 0,
            "tools/edited_tools/files_and_links_extractor.py": 12
          }
        },
        "noop_removed": {
          "total": {
            "null": 12,
            "tools/edited_tools/files_and_links_extractor.py": 0
          },
          "max": {
            "null": 12,
            "tools/edited_tools/files_and_links_extractor.py": 0
          },
          "avg": {
            "null": 12,
            "tools/edited_tools/files_and_links_extractor.py": 0
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 128,
          "max": 64,
          "avg": 64
        },
        "net_churn": {
          "count": 0,
          "max": 64,
          "avg": 0
        },
        "added_removed": {
          "added": 64,
          "removed": 64
        },
        "true_churn": {
          "overall": {
            "contribution": 64,
            "churn": 0
          },
          "per_author": {
            "003random": {
              "contribution": 64,
              "churn": 0
            }
          },
          "per_file": {
            "javascript_files_and_links_extractor.py": {
              "contribution": 0,
              "churn": 0
            },
            "tools/edited_tools/files_and_links_extractor.py": {
              "contribution": 64,
              "churn": 0
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "null": 0.0,
          "tools/edited_tools/files_and_links_extractor.py": 0.0
        },
        "overall_bug_work_percent": 0.0,
        "total_bug_lines": 0,
        "total_lines": 128
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 2,
        "total_changed_lines": 94,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 2.127659574468085
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 3,
          "percent": 4.615384615384616
        },
        "total": {
          "files": 2,
          "lines": 65
        },
        "quality_score": 2.307692307692308
      },
      "meaningful_code": {
        "total": {
          "files": 2,
          "lines": 65,
          "meaningful_lines": 52,
          "meaningful_percent": 80.0
        },
        "quality_score": 2.307692307692308,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 128
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0.0
        }
      }
    },
    "timings": {
      "diff_delta": {
        "003random@protonmail.com": {
          "total_diff_delta": 60.0,
          "total_commits": 2,
          "weekly_velocity": {
            "2018-03-19": {
              "diff_delta": 60.0,
              "lines_added": 48,
              "lines_updated": 0,
              "lines_deleted": 48,
              "lines_moved": 0,
              "commits": 2,
              "files_changed": 2,
              "active_days": 1,
              "velocity_per_day": 60.0
            }
          }
        }
      },
      "code_provenance": {
        "003random@protonmail.com": {
          "weekly_provenance": {
            "2018-03-19": {
              "new_code_lines": 49,
              "recent_code_lines": 0,
              "old_code_lines": 0,
              "legacy_code_lines": 0,
              "total_lines": 49,
              "new_code_percent": 100.0,
              "recent_code_percent": 0.0,
              "old_code_percent": 0.0,
              "legacy_code_percent": 0.0
            }
          }
        }
      },
      "developer_hours": {
        "003random@protonmail.com": {
          "total_estimated_hours": 0.7708333333333334,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_12_2018-03-22": {
              "estimated_hours": 0.77,
              "sessions": 1,
              "commits": 2,
              "hours_per_day": 0.77
            }
          }
        }
      },
      "code_domain": {
        "003random@protonmail.com": {
          "total_by_domain": {
            "backend": 128
          },
          "weekly_domains": {
            "2018-03-19": {
              "domains": {
                "backend": 128
              },
              "total_changes": 128,
              "percentages": {
                "backend": 100.0
              }
            }
          },
          "domain_percentages": {
            "backend": 100.0
          }
        }
      },
      "comprehensive_time_analysis": {
        "003random@protonmail.com": {
          "basic_stats": {
            "total_commits": 2,
            "total_repos": 1,
            "first_commit_date": "2018-03-22T10:15:11+01:00",
            "last_commit_date": "2018-03-22T10:16:26+01:00",
            "total_span_days": 0.0,
            "commits_per_day": 2.0,
            "total_lines_changed": 128,
            "total_files_changed": 2
          },
          "timing_patterns": {
            "mean_interval_hours": 0.02,
            "median_interval_hours": 0.02,
            "min_interval_minutes": 1.25,
            "max_interval_days": 0.0
          },
          "work_sessions": {
            "session_count": 1,
            "avg_session_length_hours": 0.02,
            "max_session_length_hours": 0.02,
            "avg_commits_per_session": 2,
            "max_commits_per_session": 2,
            "sessions": [
              {
                "start": "2018-03-22T10:15:11+01:00",
                "end": "2018-03-22T10:16:26+01:00",
                "length_hours": 0.02,
                "commits": 2
              }
            ]
          },
          "daily_patterns": {
            "peak_day": 3,
            "peak_day_count": 2,
            "day_distribution": {
              "3": 2
            }
          },
          "weekly_patterns": {
            "total_weeks": 1,
            "avg_activities_per_week": 2,
            "max_activities_per_week": 2,
            "min_activities_per_week": 2
          },
          "downtime_analysis": {
            "short_breaks_count": 1,
            "long_breaks_count": 0,
            "avg_break_hours": 0.020833333333333332
          },
          "rhythm_analysis": {
            "activities_per_day": 2.0,
            "consistency_score": 0.0
          },
          "sustained_activity": {
            "total_active_days": 1,
            "max_consecutive_days": 1,
            "avg_activities_per_active_day": 2.0
          }
        }
      }
    }
  },
  "Week_19_2018-03-26": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_20_2018-04-02": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_21_2018-04-09": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_22_2018-04-16": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_23_2018-04-23": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_24_2018-04-30": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_25_2018-05-07": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_26_2018-05-14": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_27_2018-05-21": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_28_2018-05-28": {
    "productivity": {
      "change_set": {
        "max": 0,
        "avg": 0
      },
      "commits_count": {},
      "contributors_count": {
        "total": {},
        "minor": {}
      },
      "contributors_experience": {},
      "hunks_count": {},
      "lines_count": {
        "added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "removed": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_added": {
          "total": {},
          "max": {},
          "avg": {}
        },
        "noop_removed": {
          "total": {},
          "max": {},
          "avg": {}
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "net_churn": {
          "count": 0,
          "max": 0,
          "avg": 0
        },
        "added_removed": {
          "added": 0,
          "removed": 0
        },
        "true_churn": {
          "overall": {
            "contribution": 0,
            "churn": 0
          },
          "per_author": {},
          "per_file": {}
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {},
        "overall_bug_work_percent": 0,
        "total_bug_lines": 0,
        "total_lines": 0
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 0,
        "moved_lines_percent": 0,
        "copy_pasted_lines_percent": 0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0
        },
        "total": {
          "files": 0,
          "lines": 0
        },
        "quality_score": 0.0
      },
      "meaningful_code": {
        "total": {
          "files": 0,
          "lines": 0,
          "meaningful_lines": 0,
          "meaningful_percent": 0
        },
        "quality_score": 0.0,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 0
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0
        }
      }
    },
    "timings": {
      "diff_delta": {},
      "code_provenance": {},
      "developer_hours": {},
      "code_domain": {},
      "comprehensive_time_analysis": {}
    }
  },
  "Week_29_2018-06-04": {
    "productivity": {
      "change_set": {
        "max": 1,
        "avg": 1.0
      },
      "commits_count": {
        "tools/online.py": 1
      },
      "contributors_count": {
        "total": {
          "tools/online.py": 1
        },
        "minor": {
          "tools/online.py": 0
        }
      },
      "contributors_experience": {
        "tools/online.py": 1
      },
      "hunks_count": {
        "tools/online.py": 3
      },
      "lines_count": {
        "added": {
          "total": {
            "tools/online.py": 20
          },
          "max": {
            "tools/online.py": 20
          },
          "avg": {
            "tools/online.py": 20
          }
        },
        "removed": {
          "total": {
            "tools/online.py": 27
          },
          "max": {
            "tools/online.py": 27
          },
          "avg": {
            "tools/online.py": 27
          }
        },
        "noop_added": {
          "total": {
            "tools/online.py": 5
          },
          "max": {
            "tools/online.py": 5
          },
          "avg": {
            "tools/online.py": 5
          }
        },
        "noop_removed": {
          "total": {
            "tools/online.py": 2
          },
          "max": {
            "tools/online.py": 2
          },
          "avg": {
            "tools/online.py": 2
          }
        }
      }
    },
    "quality": {
      "code_churn": {
        "total_churn": {
          "count": 47,
          "max": 47,
          "avg": 47
        },
        "net_churn": {
          "count": -7,
          "max": -7,
          "avg": -7
        },
        "added_removed": {
          "added": 20,
          "removed": 27
        },
        "true_churn": {
          "overall": {
            "contribution": 20,
            "churn": 0
          },
          "per_author": {
            "003random": {
              "contribution": 20,
              "churn": 0
            }
          },
          "per_file": {
            "tools/online.py": {
              "contribution": 20,
              "churn": 0
            }
          }
        }
      },
      "bugs": {
        "bug_work_percent_by_file": {
          "tools/online.py": 0.0
        },
        "overall_bug_work_percent": 0.0,
        "total_bug_lines": 0,
        "total_lines": 47
      },
      "code_movement": {
        "moved_lines_count": 0,
        "copy_pasted_lines_count": 0,
        "total_changed_lines": 34,
        "moved_lines_percent": 0.0,
        "copy_pasted_lines_percent": 0.0
      },
      "test_doc_pct": {
        "test_coverage": {
          "files": 0,
          "lines": 0,
          "percent": 0.0
        },
        "doc_coverage": {
          "files": 0,
          "lines": 1,
          "percent": 2.564102564102564
        },
        "total": {
          "files": 1,
          "lines": 39
        },
        "quality_score": 1.282051282051282
      },
      "meaningful_code": {
        "total": {
          "files": 1,
          "lines": 39,
          "meaningful_lines": 26,
          "meaningful_percent": 66.66666666666666
        },
        "quality_score": 1.282051282051282,
        "unrealistic_commits": {
          "large_commits": 0,
          "rapid_large_commits": 0,
          "total": 0,
          "skipped_lines": 47
        },
        "auto_generated": {
          "long_sequences": 0,
          "repeated_chars": 0,
          "repetitive_patterns": 0,
          "total": 0,
          "percent": 0.0
        }
      }
    },
    "timings": {
      "diff_delta": {
        "003random@protonmail.com": {
          "total_diff_delta": 22.1,
          "total_commits": 1,
          "weekly_velocity": {
            "2018-06-04": {
              "diff_delta": 22.1,
              "lines_added": 8,
              "lines_updated": 11,
              "lines_deleted": 21,
              "lines_moved": 6,
              "commits": 1,
              "files_changed": 1,
              "active_days": 1,
              "velocity_per_day": 22.1
            }
          }
        }
      },
      "code_provenance": {
        "003random@protonmail.com": {
          "weekly_provenance": {
            "2018-06-04": {
              "new_code_lines": 11,
              "recent_code_lines": 0,
              "old_code_lines": 4,
              "legacy_code_lines": 0,
              "total_lines": 15,
              "new_code_percent": 73.33333333333333,
              "recent_code_percent": 0.0,
              "old_code_percent": 26.666666666666668,
              "legacy_code_percent": 0.0
            }
          }
        }
      },
      "developer_hours": {
        "003random@protonmail.com": {
          "total_estimated_hours": 0.5625,
          "total_sessions": 1,
          "weekly_hours": {
            "Week_23_2018-06-05": {
              "estimated_hours": 0.56,
              "sessions": 1,
              "commits": 1,
              "hours_per_day": 0.56
            }
          }
        }
      },
      "code_domain": {
        "003random@protonmail.com": {
          "total_by_domain": {
            "backend": 47
          },
          "weekly_domains": {
            "2018-06-04": {
              "domains": {
                "backend": 47
              },
              "total_changes": 47,
              "percentages": {
                "backend": 100.0
              }
            }
          },
          "domain_percentages": {
            "backend": 100.0
          }
        }
      },
      "comprehensive_time_analysis": {}
    }
  },
  "developer_stats": {}
},
  "metrics_type": "weekly",
  "processing": {
    "total_commits": 86,
    "total_lines_added": 1641,
    "total_lines_removed": 888
  }
}
